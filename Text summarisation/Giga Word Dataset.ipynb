{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import _pickle as pickle\n",
    "\n",
    "## Some useful functions to ease the processings\n",
    "def save(file,name, folder = \"\"):\n",
    "    if folder != \"\":\n",
    "        outfile = open('./'+folder+'/'+name+'.pickle', 'wb')\n",
    "    else:\n",
    "        outfile = open(name+'.pickle', 'wb')\n",
    "    pickle.dump(file, outfile)\n",
    "    outfile.close\n",
    "    \n",
    "def load(name, folder = \"\"):\n",
    "    if folder != \"\":\n",
    "        outfile = open('./'+folder+'/'+name+'.pickle', 'rb')\n",
    "    else:\n",
    "        outfile = open(name+'.pickle', 'rb')\n",
    "    file = pickle.load(outfile)\n",
    "    outfile.close\n",
    "    return file\n",
    "\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, TFBertForQuestionAnswering, TFBertModel, TFBertForNextSentencePrediction\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# from tensorflow.keras.backend.tensorflow_backend import set_session\n",
    "# import tensorflow as tf\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# sess = tf.Session(config=config)\n",
    "# set_session(sess)\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.article.txt', sep = '\\n', header = None)\n",
    "df1 = pd.read_csv('train.title.txt', sep = '\\n', header = None)\n",
    "df.columns = ['article']\n",
    "df['title'] = df1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(df, 'train_gigaword')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load('train_gigaword')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:200000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "max_length = 64\n",
    "max_length_out = 16\n",
    "X = list(np.zeros(df.shape[0]))\n",
    "X_type = list(np.zeros(df.shape[0]))\n",
    "X_masks = list(np.zeros(df.shape[0]))\n",
    "Y = list(np.zeros(df.shape[0]))\n",
    "\n",
    "text_pairs = []\n",
    "for index, line in tqdm(df.iterrows(), total = df.shape[0]):\n",
    "    s1 = line['article']\n",
    "    s2 = line['title']\n",
    "    \n",
    "    tokenized = tokenizer.encode_plus(str(s1), add_special_tokens = True, max_length = max_length, pad_to_max_length = True)\n",
    "    answer = tokenizer.encode_plus(str(s2), add_special_tokens = True, max_length = max_length_out, pad_to_max_length = True)\n",
    "    \n",
    "    X[index] = tokenized['input_ids']\n",
    "    X_type[index] = tokenized['token_type_ids']\n",
    "    X_masks[index] = tokenized['attention_mask']\n",
    "    Y[index] = answer['input_ids']\n",
    "    text_pairs.append(tokenizer.decode(tokenized['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1 = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1[:,-2] = 102*(Y1[:,-2]!=0)\n",
    "Y1[:,-1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_input = Y1[:,:-1]\n",
    "Y_output = Y1[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['inputs_ids'] = X\n",
    "df['token_type_ids'] = X_type\n",
    "df['attention_masks'] = X_masks\n",
    "df['target'] = Y\n",
    "df['target_input'] = list(Y_input)\n",
    "df['target_output'] = list(Y_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target_input'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(df, 'small_data_refined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load('small_data_refined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([list(elt) for elt in df['inputs_ids'].values]).astype(int)\n",
    "X_masks = np.array([list(elt) for elt in df['attention_masks'].values]).astype(int)\n",
    "X_type = np.array([list(elt) for elt in df['token_type_ids']]).astype(int)\n",
    "\n",
    "Y = np.array([list(elt) for elt in df['target']]).astype(int)\n",
    "Y_input = np.array([list(elt) for elt in df['target_input']]).astype('float32')\n",
    "Y_output = np.array([list(elt) for elt in df['target_output']]).astype(int)\n",
    "# from keras.utils import np_utils\n",
    "# y = np_utils.to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ids, X_test_ids, y_train, y_test = train_test_split(X, Y_output, random_state=42, test_size=0.1)\n",
    "X_train_masks, X_test_masks, y_train_input, y_test_input = train_test_split(X_masks, Y_input, random_state=42, test_size=0.1)\n",
    "X_train_type, X_test_type, _ , _ = train_test_split(X_type, Y, random_state=42, test_size=0.1)\n",
    "\n",
    "X_train = [X_train_ids, X_train_masks, X_train_type, y_train_input]\n",
    "X_test = [X_test_ids, X_test_masks, X_test_type, y_test_input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(X, y, batch_size = 64):\n",
    "    \n",
    "    while True:\n",
    "        ids = X[0]\n",
    "        masks = X[1]\n",
    "        types = X[2]\n",
    "        Y_input = X[3]\n",
    "        \n",
    "        batch = np.random.randint(0, len(ids), batch_size)\n",
    "\n",
    "        Y1 = y[batch]\n",
    "        y1 = np_utils.to_categorical(Y1, num_classes = 29611)\n",
    "        \n",
    "        Y_input = Y_input[batch]\n",
    "        y_input = np_utils.to_categorical(Y_input, num_classes = 29611)\n",
    "        \n",
    "        \n",
    "        batch_x = [ids[batch], masks[batch], types[batch], y_input]\n",
    "        batch_y = y1\n",
    "        yield( batch_x, batch_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = generator(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.send(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[0][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, TimeDistributed, LSTM\n",
    "\n",
    "max_length = 64\n",
    "max_length_out = 15\n",
    "vocab_size = 29611\n",
    "\n",
    "inputs_ids = Input(shape = (max_length,), dtype = 'int32')\n",
    "inputs_mask = Input(shape = (max_length,), dtype = 'int32')\n",
    "inputs_type = Input(shape = (max_length,), dtype = 'int32')\n",
    "inputs_decoder = Input(shape = (max_length_out,vocab_size,), dtype = 'float32')\n",
    "\n",
    "inputs = [inputs_ids, inputs_mask, inputs_type, inputs_decoder]\n",
    "\n",
    "sentence_encoder = TFBertModel.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.  \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "encoded = sentence_encoder(inputs_ids, attention_mask = inputs_mask, token_type_ids = inputs_type)\n",
    "\n",
    "word_embedding = encoded[0]\n",
    "pooled_encoded = encoded[1]\n",
    "\n",
    "# encoder = LSTM(512, activation='tanh', recurrent_activation='sigmoid', dropout=0.2, \n",
    "#                 recurrent_dropout=0.2,return_sequences=True)(word_embedding)\n",
    "\n",
    "encoder = LSTM(64, activation = 'tanh', return_state=True, recurrent_dropout = 0.001)\n",
    "encoder_outputs, state_h, state_c = encoder(word_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_lstm = LSTM(64, activation = 'tanh', return_sequences=True, return_state=True, recurrent_dropout = 0.001)\n",
    "decoder_outputs, _, _ = decoder_lstm(inputs_decoder,\n",
    "                                     initial_state=encoder_states)\n",
    "# recurrent_dropout = 0\n",
    "# decoder_outputs = Dense(vocab_size, activation='softmax')(decoder_outputs)\n",
    "\n",
    "decoder_outputs = TimeDistributed(Dense(vocab_size, activation = 'sigmoid'))(decoder_outputs)\n",
    "\n",
    "\n",
    "model = Model(inputs, decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.metrics import sparse_categorical_accuracy\n",
    "import tensorflow as tf\n",
    "\n",
    "loss_classif     =  'categorical_crossentropy'# find the right loss for multi-class classification\n",
    "optimizer        =  Adam(3e-5, 1e-8) # find the right optimizer\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 4\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    history = model.fit_generator(generator(X_train, y_train, batch_size=batch_size), steps_per_epoch=len(X_train[0]) // batch_size,\n",
    "        epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('text_summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.device('XLA_GPU:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(a.send(None)[1], axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = generator(X_test, y_test, batch_size = 1)\n",
    "batch = a.send(None)\n",
    "x = batch[0]\n",
    "y = batch[1]\n",
    "pred = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(x):\n",
    "    return np_utils.to_categorical(x, num_classes = 29611)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([encode([101,0,0,0,0,0,0,0,0,0,0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(x[3], axis = 2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_pred = np.argmax(pred, axis = 2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_true = np.argmax(y, axis = 2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "print(tokenizer.decode(x[0][0]))\n",
    "print('\\n')\n",
    "print(tokenizer.decode(tok_true))\n",
    "print('\\n')\n",
    "print(tokenizer.decode(tok_pred))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
