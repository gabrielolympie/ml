{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "import pydicom\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import _pickle as pickle\n",
    "\n",
    "def save(file,name, folder = \"\"):\n",
    "    if folder != \"\":\n",
    "        outfile = open('./'+folder+'/'+name+'.pickle', 'wb')\n",
    "    else:\n",
    "        outfile = open(name+'.pickle', 'wb')\n",
    "    pickle.dump(file, outfile)\n",
    "    outfile.close\n",
    "    \n",
    "def load(name, folder = \"\"):\n",
    "    if folder != \"\":\n",
    "        outfile = open('./'+folder+'/'+name+'.pickle', 'rb')\n",
    "    else:\n",
    "        outfile = open(name+'.pickle', 'rb')\n",
    "    file = pickle.load(outfile)\n",
    "    outfile.close\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imSize = 128\n",
    "\n",
    "def find_bound(im, tres, axis):\n",
    "    inf = 0\n",
    "    sup = im.shape[axis]\n",
    "    cond = True\n",
    "    tres = tres\n",
    "    temp = 0\n",
    "    for i,elt in enumerate(im.sum(axis = axis)):\n",
    "        if abs(elt) <=tres and cond == True:\n",
    "            temp = i\n",
    "        if abs(elt) >=tres:\n",
    "            cond = False\n",
    "            inf = temp\n",
    "            \n",
    "\n",
    "        if abs(elt)  <= tres and cond == False:\n",
    "            sup = i\n",
    "            cond = True\n",
    "    return inf, sup\n",
    "\n",
    "def preprocess_image(image_path, desired_size=imSize):\n",
    "#     print(image_path)\n",
    "    im = pydicom.dcmread(image_path).pixel_array\n",
    "    \n",
    "#     im = (im - im.min())/(im.max()-im.min())\n",
    "    tres = 5\n",
    "    inf_x, sup_x = find_bound(im, tres, 1)\n",
    "    inf_y, sup_y = find_bound(im,tres,0)\n",
    "    im = im[inf_x:sup_x,inf_y: sup_y]\n",
    "    im = cv2.resize(im, (imSize,imSize))\n",
    "    return im\n",
    "\n",
    "def build_3d_image(ids, res = 256, to_keep = 'ALL'):\n",
    "    path = './train/' + ids\n",
    "\n",
    "    list_dir = [int(elt.split('.')[0]) for elt in os.listdir(path)]\n",
    "    list_dir.sort()\n",
    "    \n",
    "    if to_keep == 'ALL':\n",
    "        n_image_keep = len(list_dir)\n",
    "    else:\n",
    "        n_image_keep = to_keep\n",
    "    X = list(np.zeros(n_image_keep))  #list(np.zeros(len(list_dir)))\n",
    "\n",
    "    n_img = len(list_dir)\n",
    "    print(n_img)\n",
    "\n",
    "    for i in range(n_image_keep):\n",
    "        elt = list_dir[int(i*n_img/(n_image_keep))]\n",
    "        im = preprocess_image(path + '/'+ str(elt) + '.dcm', desired_size = res)\n",
    "        X[i] = im\n",
    "#         print(int(i*n_img/(n_image_keep)))\n",
    "\n",
    "    X = np.array(X).astype(float)\n",
    "    \n",
    "    X = X.transpose(1, 2, 0)\n",
    "    # X = cv2.resize(X, (imSize,imSize))\n",
    "    \n",
    "    X = (X - X.min())/(X.max()-X.min())\n",
    "    \n",
    "    return X\n",
    "\n",
    "def make_3d_vid(X):\n",
    "    %matplotlib notebook\n",
    "    fig1 = plt.figure(num='Lung', figsize = (5,5))\n",
    "\n",
    "    ax1 = fig1.add_subplot(111)\n",
    "    ax1.set_xlabel('x label')\n",
    "\n",
    "    end = X.shape[0]\n",
    "    for i in range(end):\n",
    "        ax1.cla()  # Clear only 2nd figure's axes, figure 1 is ADDITIVE\n",
    "        ax1.set_title('Axes title')  # Reset as removed by cla()\n",
    "\n",
    "        ax1.imshow(X[i,:,:], cmap = 'gray')\n",
    "    #     ax2.plot(range(i,end), range(i,end), 'rx')\n",
    "        fig1.canvas.draw()\n",
    "    #     plt.pause(0.001)\n",
    "\n",
    "    %matplotlib notebook\n",
    "    fig1 = plt.figure(num='Lung', figsize = (5,5))\n",
    "\n",
    "    ax1 = fig1.add_subplot(111)\n",
    "    ax1.set_xlabel('x label')\n",
    "\n",
    "    end = X.shape[1]\n",
    "    for i in range(end):\n",
    "        ax1.cla()  # Clear only 2nd figure's axes, figure 1 is ADDITIVE\n",
    "        ax1.set_title('Axes title')  # Reset as removed by cla()\n",
    "\n",
    "        ax1.imshow(X[:,i,:], cmap = 'gray')\n",
    "    #     ax2.plot(range(i,end), range(i,end), 'rx')\n",
    "        fig1.canvas.draw()\n",
    "    #     plt.pause(0.001)\n",
    "\n",
    "    %matplotlib notebook\n",
    "    fig1 = plt.figure(num='Lung', figsize = (5,5))\n",
    "\n",
    "    ax1 = fig1.add_subplot(111)\n",
    "    ax1.set_xlabel('x label')\n",
    "\n",
    "    end = X.shape[2]\n",
    "    for i in range(end):\n",
    "        ax1.cla()  # Clear only 2nd figure's axes, figure 1 is ADDITIVE\n",
    "        ax1.set_title('Axes title')  # Reset as removed by cla()\n",
    "\n",
    "        ax1.imshow(X[:,:,i], cmap = 'gray')\n",
    "    #     ax2.plot(range(i,end), range(i,end), 'rx')\n",
    "        fig1.canvas.draw()\n",
    "    #     plt.pause(0.001)\n",
    "    \n",
    "def metric(true, pred):\n",
    "    return np.mean(abs(true-pred))\n",
    "\n",
    "def comp_metric(true, pred, conf):\n",
    "    conf1 = np.maximum(conf, np.zeros(len(conf))+70)\n",
    "    \n",
    "    ab = np.minimum(abs(true-pred), np.zeros(len(conf))+1000)\n",
    "    \n",
    "    first_term = -np.sqrt(2)*ab/conf1\n",
    "    second_term = -np.log(np.sqrt(2)*conf1)\n",
    "    return np.mean(first_term + second_term)\n",
    "\n",
    "def test(model, X_train, X_test, test=True):\n",
    "    print('FVC0 testing')\n",
    "    model.fit(X_train, fvc0_train)\n",
    "    \n",
    "    if test:\n",
    "        pred = model.predict(X_test)\n",
    "        true = fvc0_test\n",
    "    else:\n",
    "        pred = model.predict(X_train)\n",
    "        true = fvc0_train\n",
    "        \n",
    "    print(metric(true, pred))\n",
    "    plt.figure(0)\n",
    "    plt.scatter(true, pred)\n",
    "    \n",
    "    print('\\n')\n",
    "    print('FVC1 testing')\n",
    "    model.fit(X_train, fvc1_train)\n",
    "    \n",
    "    if test:\n",
    "        pred = model.predict(X_test)\n",
    "        true = fvc1_test\n",
    "    else:\n",
    "        pred = model.predict(X_train)\n",
    "        true = fvc1_train\n",
    "        \n",
    "    print(metric(true, pred))\n",
    "    plt.figure(1)\n",
    "    plt.scatter(true, pred)\n",
    "    \n",
    "    std = np.zeros(len(true))+200\n",
    "    print(comp_metric(true, pred, std))\n",
    "    \n",
    "    print('\\n')\n",
    "    print('STD testing')\n",
    "    model.fit(X_train, std_train)\n",
    "    \n",
    "    if test:\n",
    "        pred = model.predict(X_test)\n",
    "        true = std_test\n",
    "    else:\n",
    "        pred = model.predict(X_train)\n",
    "        true = std_train\n",
    "        \n",
    "    print(metric(true, pred))\n",
    "    plt.figure(2)\n",
    "    plt.scatter(true, pred)\n",
    "    \n",
    "    print('evaluating metric with std prediction')\n",
    "    model.fit(X_train, fvc1_train)\n",
    "    \n",
    "    pred_train = model.predict(X_train)\n",
    "    pred = model.predict(X_test)\n",
    "    fin = np.abs(pred_train)\n",
    "    \n",
    "    model.fit(X_train, fin)\n",
    "    std = model.predict(X_test)\n",
    "    \n",
    "    print(comp_metric(fvc1_test, pred, std))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ids = 'ID00014637202177757139317'\n",
    "# ind = 4\n",
    "# ids = os.listdir('./train')[ind]\n",
    "# ids = 'ID00232637202260377586117'\n",
    "image_path = './train/'+str(ids)+'/5.dcm'\n",
    "im = pydicom.dcmread(image_path).pixel_array\n",
    "# X = build_3d_image(ids,res = imSize, to_keep = 32)\n",
    "\n",
    "# make_3d_vid(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = preprocess_image(image_path, desired_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ids = list(np.zeros(len(os.listdir('./train'))))\n",
    "# data = list(np.zeros(len(os.listdir('./train'))))\n",
    "\n",
    "ids = []\n",
    "data = []\n",
    "\n",
    "missed = 0\n",
    "for i,elt in enumerate(tqdm(os.listdir('./train'))):\n",
    "    try:\n",
    "#         ids[i] = elt\n",
    "#         data[i] = build_3d_image(elt,res = 256, to_keep = 30)\n",
    "        x= build_3d_image(elt,res = 128, to_keep = 32).astype(float)\n",
    "        data.append(x)\n",
    "        print(x.shape)\n",
    "        ids.append(elt)\n",
    "        print(elt)\n",
    "    except:\n",
    "#         ids[i] = elt\n",
    "#         data[i]= 'missing'\n",
    "        print('miss')\n",
    "        missed+=1\n",
    "        \n",
    "df = pd.read_csv('train.csv')\n",
    "df = df.sort_values(by = ['Weeks'])\n",
    "df.head()\n",
    "\n",
    "FVC0 = []\n",
    "FVC1 = []\n",
    "STD = []\n",
    "for elt in ids:\n",
    "    df1 = df[df['Patient'] == elt]\n",
    "    m = 100\n",
    "    ind = 0\n",
    "    for i, elt1 in enumerate(df1['Weeks']):\n",
    "        if abs(elt1)<m:\n",
    "            m = abs(elt1)\n",
    "            ind = i\n",
    "    FVC0.append(df1.iloc[ind]['FVC'])\n",
    "    FVC1.append(df1.iloc[-1]['FVC'])\n",
    "    STD.append(df1['FVC'].values.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "ids = np.array(ids)\n",
    "FVC0 = np.array(FVC0).astype(float)\n",
    "FVC1 = np.array(FVC1).astype(float)\n",
    "STD = np.array(STD).astype(float)\n",
    "\n",
    "\n",
    "save((ids, data, FVC0, FVC1, STD), 'dataset_64')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unet trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet3d import *\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from tensorflow.keras.layers import GlobalAveragePooling3D, Dense, Concatenate, Input\n",
    "\n",
    "import tensorflow as tf\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    numerator = 2 * tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "    denominator = tf.reduce_sum(y_true + y_pred, axis=-1)\n",
    "\n",
    "    return 1 - (numerator + 1) / (denominator + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ids, data, FVC0, FVC1, STD) = load('dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FVC0 = (FVC0-FVC0.mean())/1000\n",
    "FVC1 = (FVC1-FVC1.mean())/1000\n",
    "STD = (STD-STD.mean())/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_3d_vid(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reshape((data.shape[0],1 , data.shape[1],data.shape[2],data.shape[3]))\n",
    "X_train, X_test, fvc0_train, fvc0_test = train_test_split(data, FVC0, test_size=0.1, random_state=42)\n",
    "fvc1_train, fvc1_test, std_train, std_test = train_test_split(FVC1, STD, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train = [X_train, fvc0_train]\n",
    "X_test = [X_test, fvc0_test]\n",
    "\n",
    "y_train = [fvc0_train, fvc1_train, std_train]\n",
    "y_test = [fvc0_test, fvc1_test, std_test]\n",
    "\n",
    "data = data.reshape((data.shape[0], data.shape[2],data.shape[3],data.shape[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels, input_rows, input_cols, input_deps = 1, 128, 128, 32\n",
    "num_class = 1\n",
    "weight_dir = 'checkpoints/Genesis_Chest_CT.h5'\n",
    "# weight_dir = './checkpoints/finetuneunet_128.h5'\n",
    "models_genesis = unet_model_3d((input_channels, input_rows, input_cols, input_deps), batch_normalization=True)\n",
    "print(\"Load pre-trained Models Genesis weights from {}\".format(weight_dir))\n",
    "models_genesis.load_weights(weight_dir)\n",
    "x = models_genesis.get_layer('depth_13_relu').output\n",
    "final_convolution = Conv3D(num_class, (1, 1, 1), activation = 'linear')(x)\n",
    "output = final_convolution\n",
    "# output = keras.layers.Softmax(axis=1)(final_convolution)\n",
    "model = keras.models.Model(inputs=models_genesis.input, outputs=output)\n",
    "\n",
    "optimizer = SGD(0.1)\n",
    "loss = ['mae']\n",
    "metrics=['mse', 'mae']\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0.0001, patience=6, verbose=1, mode='auto',\n",
    "    baseline=None, restore_best_weights=True\n",
    ")\n",
    "reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, \n",
    "                                                     mode='auto', min_delta=0.0001, cooldown=0, min_lr=0.00001)\n",
    "\n",
    "batch_size = 2\n",
    "epochs = 90\n",
    "\n",
    "history = model.fit(X_train, X_train, batch_size=batch_size,\n",
    "    validation_data=(X_test, X_test),  epochs=epochs, callbacks = [stop, reduce])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_genesis.save_weights('./checkpoints/finetuneunet_128.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test[:2]).reshape(2,128,128,32)\n",
    "true = X_test.reshape(18,128,128,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 1\n",
    "make_3d_vid(true[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_3d_vid(pred[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prepare the 3D model\n",
    "\n",
    "from unet3d import *\n",
    "input_channels, input_rows, input_cols, input_deps = 1, 128, 128, 32\n",
    "num_class, activate = 2, 'linear'\n",
    "# weight_dir = 'checkpoints/Genesis_Chest_CT.h5'\n",
    "weight_dir = 'checkpoints/finetuneunet_128.h5'\n",
    "models_genesis = unet_model_3d((input_channels, input_rows, input_cols, input_deps), batch_normalization=True)\n",
    "print(\"Load pre-trained Models Genesis weights from {}\".format(weight_dir))\n",
    "models_genesis.load_weights(weight_dir)\n",
    "x = models_genesis.get_layer('depth_7_relu').output\n",
    "x = GlobalAveragePooling3D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "in2 = Input(shape = (1,))\n",
    "\n",
    "x = Concatenate(axis = -1)([x, in2])\n",
    "\n",
    "output1 = Dense(1, activation=activate)(x)\n",
    "output2 = Dense(1, activation=activate)(x)\n",
    "output3 = Dense(1, activation=activate)(x)\n",
    "outputs = [output1, output2, output3]\n",
    "model = keras.models.Model(inputs=[models_genesis.input, in2], outputs=outputs)\n",
    "\n",
    "optimizer = SGD(0.1)\n",
    "loss = ['mae', 'mae', 'mae']\n",
    "metrics=['mse']\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0.0001, patience=6, verbose=1, mode='auto',\n",
    "    baseline=None, restore_best_weights=True\n",
    ")\n",
    "reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, \n",
    "                                                     mode='auto', min_delta=0.0001, cooldown=0, min_lr=0.00001)\n",
    "# batch_size = 8 #64\n",
    "batch_size = 4 #128\n",
    "epochs = 90\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size,\n",
    "    validation_data=(X_test, y_test),  epochs=epochs, callbacks = [stop, reduce])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./checkpoints/finetuneunet_128_refined.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEEP LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv3DTranspose, Activation,Conv2DTranspose, BatchNormalization, Flatten, LeakyReLU, MaxPool3D,Conv2D, Conv3D, AveragePooling3D, Reshape, Input, Dense, Add\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ids, data, FVC0, FVC1, STD) = load('dataset')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FVC0 = np.array(FVC0).astype(float)/1000\n",
    "# FVC1 = np.array(FVC1).astype(float)/1000\n",
    "# STD = np.array(STD).astype(float)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df = df.sort_values(by = ['Weeks'])\n",
    "df.head()\n",
    "additionnals = []\n",
    "for elt in ids:\n",
    "    df1 = df[df['Patient'] == elt]\n",
    "    \n",
    "    vect = []\n",
    "    vect.append(df1['FVC'].iloc[0])\n",
    "    vect.append(df1['Percent'].iloc[0])\n",
    "    vect.append(df1['Age'].iloc[0])\n",
    "    \n",
    "    if df1['Age'].iloc[0] == 'Male':\n",
    "        vect.extend([1,0])\n",
    "    else:\n",
    "        vect.extend([0,1])\n",
    "    \n",
    "    if df1['SmokingStatus'].iloc[0] == 'Never smoked':\n",
    "        vect.extend([1,0,0])\n",
    "    elif df1['SmokingStatus'].iloc[0] == 'Ex-smoker':\n",
    "        vect.extend([0,1,0])\n",
    "    else:\n",
    "        vect.extend([0,0,1])\n",
    "    \n",
    "    additionnals.append(vect)\n",
    "\n",
    "additionnals = np.array(additionnals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder(input_shape, latent_dim = 512):\n",
    "    inputs = Input(shape = input_shape)\n",
    "    \n",
    "    x = Conv2D(32, (7,7), strides = 1, padding = 'same')(inputs)\n",
    "    \n",
    "    filters = [32,32,64,64,128]\n",
    "    for f in filters:\n",
    "        x = Conv2D(f, (3, 3), strides=2, padding=\"same\")(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        x = BatchNormalization(axis=-1)(x)\n",
    "    volumeSize = K.int_shape(x)   \n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    outputs = Dense(latent_dim)(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model, volumeSize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imSize = 128\n",
    "input_shape = (imSize,imSize,1)\n",
    "encoder, shape = build_encoder(input_shape, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.load_weights('./checkpoints/encoder/encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "\n",
    "for i in tqdm(range(len(data))):\n",
    "    x = encoder.predict(data[i:i+1,:,:,:].transpose((3,1,2,0)))\n",
    "    \n",
    "    a = np.concatenate(x, axis= -1)\n",
    "    \n",
    "    X.append(a)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet3d import *\n",
    "input_channels, input_rows, input_cols, input_deps = 1, 128, 128, 32\n",
    "num_class, activate = 2, 'linear'\n",
    "# weight_dir = 'checkpoints/Genesis_Chest_CT.h5'\n",
    "weight_dir = 'checkpoints/finetuneunet_128.h5'\n",
    "models_genesis = unet_model_3d((input_channels, input_rows, input_cols, input_deps), batch_normalization=True)\n",
    "print(\"Load pre-trained Models Genesis weights from {}\".format(weight_dir))\n",
    "models_genesis.load_weights(weight_dir)\n",
    "x = models_genesis.get_layer('depth_7_relu').output\n",
    "x = GlobalAveragePooling3D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "in2 = Input(shape = (1,))\n",
    "\n",
    "x = Concatenate(axis = -1)([x, in2])\n",
    "\n",
    "output1 = Dense(1, activation=activate)(x)\n",
    "output2 = Dense(1, activation=activate)(x)\n",
    "output3 = Dense(1, activation=activate)(x)\n",
    "outputs = [output1, output2, output3]\n",
    "model = keras.models.Model(inputs=[models_genesis.input, in2], outputs=outputs)\n",
    "\n",
    "model.load_weights('./checkpoints/finetuneunet_128_refined.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = model.get_layer('concatenate_27').output\n",
    "model1 = keras.models.Model(inputs=model.input, outputs=model.get_layer('concatenate_27').output)\n",
    "\n",
    "X = model1.predict([data.reshape((data.shape[0],1 , data.shape[1],data.shape[2],data.shape[3])), FVC0], batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.reshape((data.shape[0], data.shape[1],data.shape[2],data.shape[3],1))\n",
    "X_train, X_test, fvc0_train, fvc0_test = train_test_split(X, FVC0, test_size=0.1, random_state=42)\n",
    "fvc1_train, fvc1_test, std_train, std_test = train_test_split(FVC1, STD, test_size=0.1, random_state=42)\n",
    "add_train, add_test, _ , _ = train_test_split(additionnals, additionnals, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "y_train = [fvc0_train, fvc1_train, std_train]\n",
    "y_test = [fvc0_test, fvc1_test, std_test]\n",
    "\n",
    "del data\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "clf = LinearRegression()\n",
    "\n",
    "# import xgboost as xgb\n",
    "# clf = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "#                 max_depth = 5, alpha = 10, n_estimators = 10)\n",
    "# import lightgbm\n",
    "# clf = lightgbm.LGBMRegressor(boosting_type='gbdt', num_leaves=31, max_depth=- 1, learning_rate=0.1, \n",
    "#                        n_estimators=100, subsample_for_bin=200000, objective=None, class_weight=None, \n",
    "#                        min_split_gain=0.0, min_child_weight=0.001, min_child_samples=20, \n",
    "#                        subsample=1.0, subsample_freq=0, colsample_bytree=1.0, reg_alpha=0.0, \n",
    "#                        reg_lambda=0.0, random_state=None, n_jobs=- 1, silent=True, \n",
    "#                        importance_type='split')\n",
    "\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# clf = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# clf = RandomForestRegressor(n_estimators=10, criterion='mse', max_depth=5, min_samples_split=2, \n",
    "#                             min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', \n",
    "#                             max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, \n",
    "#                             bootstrap=True, oob_score=False, n_jobs=8)\n",
    "\n",
    "# from sklearn.svm import SVR\n",
    "\n",
    "# clf = SVR(kernel = 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(clf,X_train, X_test, test = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(clf,add_train, add_test, test = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(clf,np.concatenate([X_train, add_train], axis = -1), np.concatenate([X_test, add_test], axis = -1), test = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "reducer = umap.UMAP(n_neighbors=3, metric='cosine', n_components=2, target_metric = 'l2')\n",
    "\n",
    "y_test_unsupervised = np.zeros(fvc1_test.shape[0])-1\n",
    "y_learn = np.concatenate([fvc1_train, y_test_unsupervised])\n",
    "\n",
    "embedding = reducer.fit(np.concatenate([X_train, X_test], axis=0), y_learn)\n",
    "\n",
    "Xt = embedding.transform(X_train)\n",
    "Xv = embedding.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(clf,Xt, Xv, test = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test(clf,np.concatenate([Xt, add_train], axis = -1), np.concatenate([Xv, add_test], axis = -1), test = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([Xt, add_train], axis = -1), np.concatenate([Xv, add_test], axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(add_train, fvc1_train)\n",
    "pred = clf.predict(add_test)\n",
    "true = fvc1_test\n",
    "\n",
    "# clf.fit(Xt, fvc1_train)\n",
    "# pred = clf.predict(Xv)\n",
    "# true = fvc1_test\n",
    "\n",
    "# clf.fit(np.concatenate([Xt, add_train], axis = -1), fvc1_train)\n",
    "# pred = clf.predict(np.concatenate([Xv, add_test], axis = -1))\n",
    "# true = fvc1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "\n",
    "for i in range(80,500):\n",
    "    x.append(comp_metric(true, pred, np.zeros(len(true))+i))\n",
    "    y.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.plot(y,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
