{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "import pydicom\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import _pickle as pickle\n",
    "\n",
    "def save(file,name, folder = \"\"):\n",
    "    if folder != \"\":\n",
    "        outfile = open('./'+folder+'/'+name+'.pickle', 'wb')\n",
    "    else:\n",
    "        outfile = open(name+'.pickle', 'wb')\n",
    "    pickle.dump(file, outfile)\n",
    "    outfile.close\n",
    "    \n",
    "def load(name, folder = \"\"):\n",
    "    if folder != \"\":\n",
    "        outfile = open('./'+folder+'/'+name+'.pickle', 'rb')\n",
    "    else:\n",
    "        outfile = open(name+'.pickle', 'rb')\n",
    "    file = pickle.load(outfile)\n",
    "    outfile.close\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imSize = 128\n",
    "\n",
    "def find_bound(im, tres, axis):\n",
    "    inf = 0\n",
    "    sup = im.shape[axis]\n",
    "    cond = True\n",
    "    tres = tres\n",
    "    temp = 0\n",
    "    for i,elt in enumerate(im.sum(axis = axis)):\n",
    "        if abs(elt) <=tres and cond == True:\n",
    "            temp = i\n",
    "        if abs(elt) >=tres:\n",
    "            cond = False\n",
    "            inf = temp\n",
    "            \n",
    "\n",
    "        if abs(elt)  <= tres and cond == False:\n",
    "            sup = i\n",
    "            cond = True\n",
    "    return inf, sup\n",
    "\n",
    "def preprocess_image(image_path, desired_size=imSize):\n",
    "#     print(image_path)\n",
    "    im = pydicom.dcmread(image_path).pixel_array\n",
    "    \n",
    "#     im = (im - im.min())/(im.max()-im.min())\n",
    "    tres = 5\n",
    "    inf_x, sup_x = find_bound(im, tres, 1)\n",
    "    inf_y, sup_y = find_bound(im,tres,0)\n",
    "    im = im[inf_x:sup_x,inf_y: sup_y]\n",
    "    im = cv2.resize(im, (imSize,imSize))\n",
    "    return im\n",
    "\n",
    "def build_3d_image(ids, res = 256, to_keep = 'ALL'):\n",
    "    path = './train/' + ids\n",
    "\n",
    "    list_dir = [int(elt.split('.')[0]) for elt in os.listdir(path)]\n",
    "    list_dir.sort()\n",
    "    \n",
    "    if to_keep == 'ALL':\n",
    "        n_image_keep = len(list_dir)\n",
    "    else:\n",
    "        n_image_keep = to_keep\n",
    "    X = list(np.zeros(n_image_keep))  #list(np.zeros(len(list_dir)))\n",
    "\n",
    "    n_img = len(list_dir)\n",
    "    print(n_img)\n",
    "\n",
    "    for i in range(n_image_keep):\n",
    "        elt = list_dir[int(i*n_img/(n_image_keep))]\n",
    "        im = preprocess_image(path + '/'+ str(elt) + '.dcm', desired_size = res)\n",
    "        X[i] = im\n",
    "#         print(int(i*n_img/(n_image_keep)))\n",
    "\n",
    "    X = np.array(X).astype(float)\n",
    "    \n",
    "    X = X.transpose(1, 2, 0)\n",
    "    # X = cv2.resize(X, (imSize,imSize))\n",
    "    \n",
    "    X = (X - X.min())/(X.max()-X.min())\n",
    "    \n",
    "    return X\n",
    "\n",
    "def make_3d_vid(X):\n",
    "    %matplotlib notebook\n",
    "    fig1 = plt.figure(num='Lung', figsize = (5,5))\n",
    "\n",
    "    ax1 = fig1.add_subplot(111)\n",
    "    ax1.set_xlabel('x label')\n",
    "\n",
    "    end = X.shape[0]\n",
    "    for i in range(end):\n",
    "        ax1.cla()  # Clear only 2nd figure's axes, figure 1 is ADDITIVE\n",
    "        ax1.set_title('Axes title')  # Reset as removed by cla()\n",
    "\n",
    "        ax1.imshow(X[i,:,:], cmap = 'gray')\n",
    "    #     ax2.plot(range(i,end), range(i,end), 'rx')\n",
    "        fig1.canvas.draw()\n",
    "    #     plt.pause(0.001)\n",
    "\n",
    "    %matplotlib notebook\n",
    "    fig1 = plt.figure(num='Lung', figsize = (5,5))\n",
    "\n",
    "    ax1 = fig1.add_subplot(111)\n",
    "    ax1.set_xlabel('x label')\n",
    "\n",
    "    end = X.shape[1]\n",
    "    for i in range(end):\n",
    "        ax1.cla()  # Clear only 2nd figure's axes, figure 1 is ADDITIVE\n",
    "        ax1.set_title('Axes title')  # Reset as removed by cla()\n",
    "\n",
    "        ax1.imshow(X[:,i,:], cmap = 'gray')\n",
    "    #     ax2.plot(range(i,end), range(i,end), 'rx')\n",
    "        fig1.canvas.draw()\n",
    "    #     plt.pause(0.001)\n",
    "\n",
    "    %matplotlib notebook\n",
    "    fig1 = plt.figure(num='Lung', figsize = (5,5))\n",
    "\n",
    "    ax1 = fig1.add_subplot(111)\n",
    "    ax1.set_xlabel('x label')\n",
    "\n",
    "    end = X.shape[2]\n",
    "    for i in range(end):\n",
    "        ax1.cla()  # Clear only 2nd figure's axes, figure 1 is ADDITIVE\n",
    "        ax1.set_title('Axes title')  # Reset as removed by cla()\n",
    "\n",
    "        ax1.imshow(X[:,:,i], cmap = 'gray')\n",
    "    #     ax2.plot(range(i,end), range(i,end), 'rx')\n",
    "        fig1.canvas.draw()\n",
    "    #     plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ids = 'ID00014637202177757139317'\n",
    "ind = 4\n",
    "ids = os.listdir('./train')[ind]\n",
    "ids = 'ID00232637202260377586117'\n",
    "\n",
    "\n",
    "X = build_3d_image(ids,res = 256, to_keep = 32)\n",
    "\n",
    "# make_3d_vid(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df = df.sort_values(by = ['Weeks'])\n",
    "df.head()\n",
    "\n",
    "ids  =[]\n",
    "data = []\n",
    "FVC0 = []\n",
    "FVC1 = []\n",
    "STD = []\n",
    "\n",
    "for i,elt in enumerate(tqdm(os.listdir('./train'))):\n",
    "    \n",
    "    id1 = elt\n",
    "    df1 = df[df['Patient'] == elt]\n",
    "    m = 100\n",
    "    ind = 0\n",
    "    for i, elt1 in enumerate(df1['Weeks']):\n",
    "        if abs(elt1)<m:\n",
    "            m = abs(elt1)\n",
    "            ind = i\n",
    "#     FVC0.append(df1.iloc[ind]['FVC'])\n",
    "#     FVC1.append(df1.iloc[-1]['FVC'])\n",
    "#     STD.append(df1['FVC'].values.std())\n",
    "    \n",
    "    for img in os.listdir('./train/'+elt):\n",
    "        try:\n",
    "            image_path = './train/'+elt+'/'+img\n",
    "            imSize = imSize\n",
    "            im = preprocess_image(image_path, desired_size=imSize)\n",
    "            im = np.array(im).astype(float)\n",
    "            im = (im - im.min())/(im.max()-im.min())\n",
    "            if str(im.sum())!='nan':\n",
    "                ids.append(id1)\n",
    "                data.append(im)\n",
    "                FVC0.append(df1.iloc[ind]['FVC'])\n",
    "                FVC1.append(df1.iloc[-1]['FVC'])\n",
    "                STD.append(df1['FVC'].values.std())\n",
    "        except:\n",
    "            print('miss')\n",
    "\n",
    "ids = np.array(ids)\n",
    "data = np.array(data)\n",
    "FVC0 = np.array(FVC0)\n",
    "FVC1 = np.array(FVC1)\n",
    "STD = np.array(STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save((ids, data, FVC0, FVC1, STD), 'dataset_2D')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEEP LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv3DTranspose, Activation,Conv2DTranspose, BatchNormalization, Flatten, LeakyReLU, MaxPool3D,Conv2D, Conv3D, AveragePooling3D, Reshape, Input, Dense, Add\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ids, data, FVC0, FVC1, STD) = load('dataset_2D')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FVC0 = np.array(FVC0).astype(float)/1000\n",
    "FVC1 = np.array(FVC1).astype(float)/1000\n",
    "STD = np.array(STD).astype(float)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, elt in enumerate(data):\n",
    "    if str(elt.sum()) == 'nan':\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reshape((data.shape[0], data.shape[1],data.shape[2],1))\n",
    "X_train, X_test, fvc0_train, fvc0_test = train_test_split(data, FVC0, test_size=0.1, random_state=42)\n",
    "fvc1_train, fvc1_test, std_train, std_test = train_test_split(FVC1, STD, test_size=0.1, random_state=42)\n",
    "\n",
    "y_train = [fvc0_train, fvc1_train, std_train]\n",
    "y_test = [fvc0_test, fvc1_test, std_test]\n",
    "\n",
    "del data\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder(input_shape, latent_dim = 512):\n",
    "    inputs = Input(shape = input_shape)\n",
    "    \n",
    "    x = Conv2D(32, (7,7), strides = 1, padding = 'same')(inputs)\n",
    "    \n",
    "    filters = [32,32,64,64,128]\n",
    "    for f in filters:\n",
    "        x = Conv2D(f, (3, 3), strides=2, padding=\"same\")(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        x = BatchNormalization(axis=-1)(x)\n",
    "    volumeSize = K.int_shape(x)   \n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    outputs = Dense(latent_dim)(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model, volumeSize\n",
    "\n",
    "def build_decoder(volumeSize, latent_dim = 512):\n",
    "    \n",
    "#     inputs = Input(shape = (512,))\n",
    "    \n",
    "    inputs = Input(shape=(latent_dim,))\n",
    "    x = Dense(np.prod(volumeSize[1:]))(inputs)\n",
    "    x = Reshape((volumeSize[1], volumeSize[2], volumeSize[3]))(x)\n",
    "    # loop over our number of filters again, but this time in\n",
    "    # reverse order\n",
    "    filters = [128,64,64,32,32]\n",
    "    for f in filters[::-1]:\n",
    "        # apply a CONV_TRANSPOSE => RELU => BN operation\n",
    "        x = Conv2DTranspose(f, (3, 3), strides=2,\n",
    "                        padding=\"same\")(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        x = BatchNormalization(axis=-1)(x)\n",
    "    \n",
    "    x = Conv2D(1, (3, 3), padding=\"same\")(x)\n",
    "    outputs = Activation(\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (imSize,imSize,1)\n",
    "encoder, shape = build_encoder(input_shape, 512)\n",
    "decoder = build_decoder(shape, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.summary()\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (imSize,imSize,1)\n",
    "encoder, shape = build_encoder(input_shape, 2048)\n",
    "decoder = build_decoder(shape, 2048)\n",
    "\n",
    "inputs = Input(shape = input_shape)\n",
    "\n",
    "encoded = encoder(inputs)\n",
    "decoded = decoder(encoded)\n",
    "\n",
    "model = Model(inputs, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss = 'mse'\n",
    "optimizer = SGD(0.1)\n",
    "optimizer = Adam(lr=1e-3)\n",
    "metrics = []\n",
    "\n",
    "model.compile(loss=loss,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "stop = tensorflow.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0.0001, patience=6, verbose=1, mode='auto',\n",
    "    baseline=None, restore_best_weights=True\n",
    ")\n",
    "reduce = tensorflow.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, \n",
    "                                                     mode='auto', min_delta=0.0001, cooldown=0, min_lr=0.00001)\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 90\n",
    "\n",
    "history = model.fit(X_train, X_train, batch_size=batch_size,\n",
    "    validation_data=(X_test, X_test),  epochs=epochs, callbacks = [stop, reduce])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 114\n",
    "\n",
    "\n",
    "size = imSize\n",
    "plt.figure(1)\n",
    "plt.imshow(X_test[ind].reshape((size,size)), cmap = 'gray')\n",
    "\n",
    "plt.figure(2)\n",
    "plt.imshow(X_pred[ind].reshape((size,size)), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save_weights('./checkpoints/encoder/encoder')\n",
    "decoder.save_weights('./checkpoints/decoder/decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=encoder.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FVC0.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FVC1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save((ids, data, FVC0, FVC1, STD), 'dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
