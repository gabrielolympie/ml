{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General librairies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "## Cosine Distance\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "## Keras utilities\n",
    "import keras\n",
    "import tensorflow\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model, Sequential\n",
    "from keras.applications import VGG16\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Conv2D, Lambda,  Dense, Flatten,MaxPooling2D,Dropout, UpSampling2D,GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "# from tensorflow.keras.engine.input_layer import Input\n",
    "from keras.layers import merge\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.regularizers import l2\n",
    "import numpy.random as rng\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "## Sklearn utilities\n",
    "#### Models\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#### Scores\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#### Feature transformations\n",
    "import umap\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Removing some of the useless warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some useful functions to ease the processings\n",
    "def save(file,name, folder = \"\"):\n",
    "    if folder != \"\":\n",
    "        outfile = open('./'+folder+'/'+name+'.pickle', 'wb')\n",
    "    else:\n",
    "        outfile = open(name+'.pickle', 'wb')\n",
    "    pickle.dump(file, outfile)\n",
    "    outfile.close\n",
    "    \n",
    "def load(name, folder = \"\"):\n",
    "    if folder != \"\":\n",
    "        outfile = open('./'+folder+'/'+name+'.pickle', 'rb')\n",
    "    else:\n",
    "        outfile = open(name+'.pickle', 'rb')\n",
    "    file = pickle.load(outfile)\n",
    "    outfile.close\n",
    "    return file\n",
    "\n",
    "def plot(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    x = list(range(len(acc)))\n",
    "    plt.plot(x,acc)\n",
    "    plt.plot(x,val_acc)\n",
    "    \n",
    "def cat_to_num(y):\n",
    "    y1 = []\n",
    "    for i in range(y.shape[0]):\n",
    "        a = 0\n",
    "        for j in range(y.shape[1]):\n",
    "            a += j*y[i,j]\n",
    "        y1.append(int(a))\n",
    "    return y1\n",
    "\n",
    "def generate_integer():\n",
    "    \n",
    "    ints = []\n",
    "    \n",
    "    while len(ints)<5:\n",
    "        r = random.randint(0,63)\n",
    "        if not(r in ints):\n",
    "            ints.append(r)\n",
    "    return ints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Meta Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Net Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and plotting the meta-learning set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OT = pickle.load(open('miniImageNet_category_split_train_phase_train.pickle', 'rb'), encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'labels' : OT['labels']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OT1 = pickle.load(open('miniImageNet_category_split_train_phase_val.pickle', 'rb'), encoding='latin1')\n",
    "df1 = pd.DataFrame({'labels' : OT1['labels']})\n",
    "df1['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OT2 = pickle.load(open('miniImageNet_category_split_train_phase_test.pickle', 'rb'), encoding='latin1')\n",
    "df2 = pd.DataFrame({'labels' : OT2['labels']})\n",
    "df2['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(OT2['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OT2['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OT = pickle.load(open('miniImageNet_category_split_train_phase_train.pickle', 'rb'), encoding='latin1')\n",
    "OT1 = pickle.load(open('miniImageNet_category_split_train_phase_val.pickle', 'rb'), encoding='latin1')\n",
    "OT2 = pickle.load(open('miniImageNet_category_split_train_phase_test.pickle', 'rb'), encoding='latin1')\n",
    "\n",
    "for i in range(5):\n",
    "    plt.figure(i)\n",
    "    fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1,5)\n",
    "    for j in range(5):\n",
    "        fig.axes[j].get_xaxis().set_visible(False)\n",
    "        fig.axes[j].get_yaxis().set_visible(False)\n",
    "    ax1.imshow(OT['data'][600*i+1])\n",
    "    ax2.imshow(OT['data'][600*i+2])    \n",
    "    ax3.imshow(OT['data'][600*i+3])\n",
    "    ax4.imshow(OT['data'][600*i+4])\n",
    "    ax5.imshow(OT['data'][600*i+5])\n",
    "    \n",
    "Y_meta = np.array(OT['labels'])\n",
    "X_meta = OT['data']\n",
    "\n",
    "Y_meta = np.concatenate([Y_meta, np.array(OT1['labels'])], axis = 0)\n",
    "Y_meta = np.concatenate([Y_meta, np.array(OT2['labels'])], axis = 0)\n",
    "\n",
    "X_meta = np.concatenate([X_meta, OT1['data']], axis = 0)\n",
    "X_meta = np.concatenate([X_meta, OT2['data']], axis = 0)\n",
    "\n",
    "y_meta = np_utils.to_categorical(Y_meta)\n",
    "del OT\n",
    "del OT1\n",
    "del OT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the training of the neural net on the meta learning set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_meta, y_meta, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### First training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n",
    "                         width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,\n",
    "                         horizontal_flip=True, fill_mode=\"nearest\")\n",
    "\n",
    "\n",
    "# import resnet\n",
    "# build = resnet.ResnetBuilder()\n",
    "# model = build.build_resnet_18((3,84,84),64)\n",
    "\n",
    "\n",
    "\n",
    "optimizer=SGD(lr=0.001)\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=optimizer,#keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 4\n",
    "\n",
    "history = model.fit_generator(aug.flow(X_train, y_train, batch_size=batch_size),\n",
    "    validation_data=(X_test, y_test), steps_per_epoch=len(X_train) // batch_size,\n",
    "    epochs=epochs)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# save(history, '2.resnet with augmentation', 'results')\n",
    "# save(model, '2.resnet with augmentation', 'model')\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(model, '4.resnet18 with augmentation step 0.001', 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Loading and merging the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OT1 = pickle.load(open('miniImageNet_category_split_test.pickle', 'rb'), encoding='latin1')\n",
    "OT2 = pickle.load(open('miniImageNet_category_split_val.pickle', 'rb'), encoding='latin1')\n",
    "\n",
    "from copy import deepcopy\n",
    "Y_val = deepcopy(OT2['labels'])\n",
    "X_val = deepcopy(OT2['data'])\n",
    "\n",
    "## Concatenating val and test datas in order to get more classes for experiment\n",
    "for i in OT1['labels']:\n",
    "    Y_val.append(i)\n",
    "X_val = np.concatenate([X_val, OT1['data']], axis = 0)\n",
    "\n",
    "## Restructuring the images into an array of size 64*600*84*84*3 to ease the acces to a given class\n",
    "tab = []\n",
    "for i in range(36):\n",
    "    tab.append([])\n",
    "\n",
    "for i in range(len(Y_val)):\n",
    "    tab[Y_val[i]-64].append(X_val[i])\n",
    "\n",
    "X = np.array(tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "k = 5\n",
    "\n",
    "\n",
    "## A function in order to generate a random array of n distincts values between m and M\n",
    "def choose_n_classes(n, m, M):\n",
    "    tab = []\n",
    "    while len(tab)<n:\n",
    "        r = random.randint(m,M)\n",
    "        if not(r in tab):\n",
    "            tab.append(r)\n",
    "    tab.sort()\n",
    "    tab = np.array(tab)\n",
    "    \n",
    "    dico = {}\n",
    "    for i in range(n):\n",
    "        dico[tab[i]] = i\n",
    "    \n",
    "    return tab, dico\n",
    "\n",
    "\n",
    "## A function to generate data ready for an experiment, tab and dico are here in order to repeat the experiment \n",
    "## on the same 5 classes, but with a different number of drawn sample in the class\n",
    "def build_dataset(X, n,k,value = True, tab = True, dico = True):\n",
    "    if value == True:\n",
    "        tab, dico = choose_n_classes(n, 64,99)\n",
    "    \n",
    "    x_train = []\n",
    "    x_test = []\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    \n",
    "    print(dico)\n",
    "    \n",
    "    for elt in tab:\n",
    "        ind, dico1 = choose_n_classes(k, 0, 600)\n",
    "        ind1, dico2 = choose_n_classes(100, 0, 600)\n",
    "        for i in range(600):\n",
    "            if i in ind:\n",
    "                x_train.append(X[elt-64,i])\n",
    "                y_train.append(dico[elt])\n",
    "#             elif i in ind1:\n",
    "#                 x_test.append(X[elt-64,i])\n",
    "#                 y_test.append(dico[elt])\n",
    "            else:\n",
    "                x_test.append(X[elt-64,i])\n",
    "                y_test.append(dico[elt])\n",
    "    return np.array(x_train), np.array(x_test), np.array(y_train), np.array(y_test), tab, dico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the pre-trained model for features extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output layer should be the last batch_normalization of the previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# '2.resnet with augmentation 3rd step'\n",
    "# '2.resnet2 with augmentation 2'  439\n",
    "# '2.densenet with augmentation'  avg_pool\n",
    "\n",
    "\n",
    "# model_transfert2 = load('3.densenet with augmentation step 0.001.pickle', 'model')\n",
    "# model_transfert2 = load('2.resnet2 with augmentation 2', 'model')\n",
    "# model_transfert2 = load('3.resnet18 with augmentation step 0.001', 'model') #average_pooling2d_3 activation_51\n",
    "model_transfert2 = load('4.resnet18 with augmentation step 0.001', 'model') #activation_17 average_pooling2d_1\n",
    "\n",
    "# batch_normalization_35 max_pooling2d_3\n",
    "\n",
    "inputs = model_transfert2.input\n",
    "outputs = model_transfert2.get_layer('average_pooling2d_1').output\n",
    "outputs = Flatten()(outputs)\n",
    "model = Model(inputs=inputs,   outputs=outputs)\n",
    "\n",
    "\n",
    "# inputs = model_transfert2.input\n",
    "# outputs = model_transfert2.get_layer('avg_pool').output\n",
    "# model = Model(inputs=inputs,   outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Single experiment in n-way, k-shots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the experiment and plotting the class used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "k = 1\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test, tab, dico = build_dataset(X, n, k)#, False, tab, dico)\n",
    "\n",
    "# for i in range(5):\n",
    "# #     plt.figure(i)\n",
    "# #     plt.imshow(X_train[k*i])\n",
    "    \n",
    "#     fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1,5)\n",
    "#     for j in range(5):\n",
    "#         fig.axes[j].get_xaxis().set_visible(False)\n",
    "#         fig.axes[j].get_yaxis().set_visible(False)\n",
    "#     ax1.imshow(X_train[5*i])\n",
    "#     ax2.imshow(X_train[5*i+1])\n",
    "#     ax3.imshow(X_train[5*i+2])    \n",
    "#     ax4.imshow(X_train[5*i+3])\n",
    "#     ax5.imshow(X_train[5*i+4])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the pre-trained Model and transform it into a feature extraction tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First test with simple transfer learning and logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test1 = np.zeros(y_test.shape[0])-1\n",
    "\n",
    "Xt = model.predict(X_train)\n",
    "# Xt = model.predict(X_train)\n",
    "print(0)\n",
    "Xv = model.predict(X_test)\n",
    "# Xv = model.predict(X_test)\n",
    "print(1)\n",
    "\n",
    "\n",
    "\n",
    "clf = LogisticRegression(random_state=0, solver='saga', multi_class='multinomial', max_iter = 1000, penalty='l2')\n",
    "clf.fit(Xt,y_train)\n",
    "y_pred = clf.predict(Xv)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "y_learn = np.concatenate([y_train, y_test1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a umap embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_neighbors=15, metric='cosine', n_components=20)\n",
    "\n",
    "y_test_unsupervised = np.zeros(y_test.shape[0])-1\n",
    "y_learn = np.concatenate([y_train, y_test_unsupervised])\n",
    "\n",
    "embedding = reducer.fit(np.concatenate([Xt, Xv], axis=0), y_learn)\n",
    "\n",
    "Xt1 = embedding.transform(Xt)\n",
    "Xv1 = embedding.transform(Xv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(0)\n",
    "plt.scatter(Xt1[:, 0], Xt1[:, 1], c=y_train, cmap='Spectral', s=5)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.colorbar(boundaries=np.arange(6)-0.5).set_ticks(np.arange(5))\n",
    "plt.title('UMAP projection of the train dataset', fontsize=24);\n",
    "\n",
    "plt.figure(1)\n",
    "plt.scatter(Xv1[:, 0], Xv1[:, 1], c=y_test, cmap='Spectral', s=5)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.colorbar(boundaries=np.arange(6)-0.5).set_ticks(np.arange(5))\n",
    "plt.title('UMAP projection of the test dataset', fontsize=24);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial', max_iter = 500, C = 0.4)\n",
    "\n",
    "clf.fit(Xt1,y_train)\n",
    "y_pseudo = clf.predict(Xv1)\n",
    "\n",
    "print(accuracy_score(y_test, y_pseudo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retraining with pseudo labels on original embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial', max_iter = 500, C = 0.4)\n",
    "print(0)\n",
    "clf.fit(np.concatenate([Xt, Xv], axis = 0),np.concatenate([y_train, y_pseudo], axis = 0))\n",
    "print(1)\n",
    "y_pred = clf.predict(Xv)\n",
    "print(2)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.scatter(Xt1[:, 0], Xt1[:, 1], c=y_train, cmap='Spectral', s=5)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.colorbar(boundaries=np.arange(6)-0.5).set_ticks(np.arange(5))\n",
    "plt.title('UMAP projection of the Digits dataset', fontsize=24);\n",
    "\n",
    "plt.figure(1)\n",
    "plt.scatter(Xv1[:, 0], Xv1[:, 1], c=y_test, cmap='Spectral', s=5)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.colorbar(boundaries=np.arange(6)-0.5).set_ticks(np.arange(5))\n",
    "plt.title('UMAP projection of the Digits dataset', fontsize=24);\n",
    "\n",
    "plt.figure(2)\n",
    "plt.scatter(Xv1[:, 0], Xv1[:, 1], c=y_pred, cmap='Spectral', s=5)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.colorbar(boundaries=np.arange(6)-0.5).set_ticks(np.arange(5))\n",
    "plt.title('UMAP projection of the Digits dataset', fontsize=24);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test with different k in the same n classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores1 = []\n",
    "scores2 = []\n",
    "scores3 = []\n",
    "\n",
    "kshots = [1,3, 5,10,25,50,75,100,150,200,300,500]\n",
    "\n",
    "for k in kshots:\n",
    "    print(k)\n",
    "    X_train, X_test, y_train, y_test, tab, dico = build_dataset(X, n, k, False, tab, dico)\n",
    "    \n",
    "    Xt = model.predict(X_train)\n",
    "    Xv = model.predict(X_test)\n",
    "    \n",
    "    print('feature')\n",
    "    \n",
    "    y_test1 = np.zeros(y_test.shape[0])-1\n",
    "    y_learn = np.concatenate([y_train, y_test1])\n",
    "    \n",
    "    reducer = umap.UMAP(n_neighbors=12, metric='cosine', n_components=20)\n",
    "    embedding = reducer.fit(np.concatenate([Xt, Xv], axis=0), y_learn)\n",
    "    \n",
    "    Xt1 = embedding.transform(Xt)\n",
    "    Xv1 = embedding.transform(Xv)\n",
    "    \n",
    "    print('embedding')\n",
    "    \n",
    "    \n",
    "    ## First prediction\n",
    "    clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial', max_iter = 1500)\n",
    "    clf.fit(Xt,y_train)\n",
    "    \n",
    "    y_pred = clf.predict(Xv)\n",
    "    scores1.append(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    print(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    ## Prediction with umap embedding\n",
    "    clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial', max_iter = 1500)\n",
    "    clf.fit(Xt1,y_train)\n",
    "    \n",
    "    y_pseudo = clf.predict(Xv1)\n",
    "    scores2.append(accuracy_score(y_test, y_pseudo))\n",
    "    \n",
    "    print(accuracy_score(y_test, y_pseudo))   \n",
    "    \n",
    "    ## Final prediction\n",
    "    clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial', max_iter = 1500)\n",
    "    clf.fit(np.concatenate([Xt, Xv], axis = 0),np.concatenate([y_train, y_pseudo], axis = 0))\n",
    "    \n",
    "    y_pred = clf.predict(Xv)\n",
    "    scores3.append(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    print(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    \n",
    "plt.plot(kshots, scores1)\n",
    "plt.plot(kshots, scores2)\n",
    "plt.plot(kshots, scores3)\n",
    "\n",
    "df1 = pd.DataFrame({'n_shots': kshots, 'raw_features': scores1, 'umap': scores2, 'pseudolabels':scores3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generic Experiment for validation and averaging of the final score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "scores1 = []\n",
    "scores2 = []\n",
    "scores3 = []\n",
    "\n",
    "n = 5\n",
    "k = 5\n",
    "\n",
    "for i in tqdm(range(150)):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, tab, dico = build_dataset(X, n, k)\n",
    "    print(i)\n",
    "    ## First prediction for comparison\n",
    "    Xt = model.predict(X_train)\n",
    "    Xv = model.predict(X_test)\n",
    "    \n",
    "\n",
    "    \n",
    "    clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial', max_iter = 500, C = 0.3)\n",
    "    clf.fit(Xt,y_train)\n",
    "    y_pred = clf.predict(Xv)\n",
    "    print(accuracy_score(y_test, y_pred))\n",
    "    a = accuracy_score(y_test, y_pred)\n",
    "#     print(a)\n",
    "    scores1.append(a)\n",
    "    \n",
    "    \n",
    "    y_test_unsupervised = np.zeros(y_test.shape[0])-1\n",
    "    y_learn = np.concatenate([y_train, y_test_unsupervised])\n",
    "    \n",
    "    ## Prediction with umap embedding\n",
    "    reducer = umap.UMAP(n_neighbors=15, metric='cosine', n_components=20)\n",
    "    embedding = reducer.fit(np.concatenate([Xt, Xv], axis=0), y_learn)\n",
    "\n",
    "    Xt1 = embedding.transform(Xt)\n",
    "    Xv1 = embedding.transform(Xv)\n",
    "    \n",
    "    clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial', max_iter = 500, C = 0.3)\n",
    "    clf.fit(Xt1,y_train)\n",
    "    y_pseudo = clf.predict(Xv1)\n",
    "    \n",
    "    print(accuracy_score(y_test, y_pseudo))\n",
    "    \n",
    "    a = accuracy_score(y_test, y_pseudo)\n",
    "#     print(a)\n",
    "    scores2.append(a)\n",
    "    \n",
    "    # Final prediction\n",
    "    clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial', max_iter = 500, C = 0.4)\n",
    "    clf.fit(np.concatenate([Xt, Xv], axis = 0),np.concatenate([y_train, y_pseudo], axis = 0))\n",
    "\n",
    "    y_pred = clf.predict(Xv)\n",
    "    accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    a = accuracy_score(y_test, y_pred)\n",
    "    print(a)\n",
    "    \n",
    "    scores3.append(a)\n",
    "    print(np.array(scores3).mean())\n",
    "    \n",
    "print(np.array(scores1).mean())\n",
    "print(np.array(scores2).mean())\n",
    "print(np.array(scores3).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(scores1, bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(scores2, bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(scores3, bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(scores3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
