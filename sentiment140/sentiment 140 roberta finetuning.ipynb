{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import _pickle as pickle\n",
    "def save(file,name, folder = \"\"):\n",
    "    if folder != \"\":\n",
    "        outfile = open('./'+folder+'/'+name+'.pickle', 'wb')\n",
    "    else:\n",
    "        outfile = open(name+'.pickle', 'wb')\n",
    "    pickle.dump(file, outfile)\n",
    "    outfile.close\n",
    "    \n",
    "def load(name, folder = \"\"):\n",
    "    if folder != \"\":\n",
    "        outfile = open('./'+folder+'/'+name+'.pickle', 'rb')\n",
    "    else:\n",
    "        outfile = open(name+'.pickle', 'rb')\n",
    "    file = pickle.load(outfile)\n",
    "    outfile.close\n",
    "    return file\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, TFBertModel, RobertaTokenizer, TFRobertaModel, TFRobertaMainLayer\n",
    "import tensorflow as tf\n",
    "\n",
    "def equal(a, b):\n",
    "#     assert len(a) == len(b)\n",
    "    val = True\n",
    "    for i in range(len(a)):\n",
    "        if a[i] != b[i]:\n",
    "            val = False\n",
    "    return val\n",
    "\n",
    "import difflib\n",
    "import os\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "# max_length = 64\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n",
    "max_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv' ,encoding='latin-1', names = cols)\n",
    "df['target'] = df['target'].values / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reboot(a):\n",
    "    X = list(np.zeros(a))\n",
    "    X_masks = list(np.zeros(a))\n",
    "\n",
    "    X_masked = list(np.zeros(a))\n",
    "    X_masks_masked = list(np.zeros(a))\n",
    "\n",
    "    Y = list(np.zeros(a))\n",
    "    Y_label = list(np.zeros(a))\n",
    "    return X, X_masks, X_masked, X_masks_masked, Y, Y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a = df.shape[0]\n",
    "a = 100000\n",
    "\n",
    "max_length = 64\n",
    "for index, line in tqdm(df.iterrows(), total = df.shape[0]):\n",
    "    \n",
    "    if index % a == 0:\n",
    "        if index != 0:\n",
    "            save((X, X_masks, X_masked, X_masks_masked, Y, Y_label), 'batch_'+str(index//a), 'batch')\n",
    "        X, X_masks, X_masked, X_masks_masked, Y, Y_label = reboot(a)\n",
    "    \n",
    "    ind = index % a\n",
    "    \n",
    "    s = line['text']\n",
    "    target = line['target']\n",
    "    \n",
    "    encoded = tokenizer.encode_plus(s, add_special_tokens = True, max_length = max_length, pad_to_max_length = True)\n",
    "    \n",
    "    input_ids = np.array(encoded['input_ids'])\n",
    "    attention_masks = np.array(encoded['attention_mask'])\n",
    "    \n",
    "    X[ind] = input_ids\n",
    "    X_masks[ind] = input_ids\n",
    "    Y_label[ind] = target\n",
    "    \n",
    "    ## Random masking\n",
    "    M = (input_ids != 1).sum()\n",
    "    to_mask = np.random.randint(0,M,int(0.15*M))\n",
    "    pred = list(np.zeros(max_length))\n",
    "    \n",
    "    for elt in to_mask:\n",
    "        pred[elt] = input_ids[elt]\n",
    "        input_ids[elt] = 50264\n",
    "        attention_masks[elt] = 0\n",
    "        \n",
    "    X_masked[ind] = input_ids\n",
    "    X_masks_masked[ind] = attention_masks\n",
    "    Y[ind] = pred\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building RoberTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, TimeDistributed, Embedding, Concatenate\n",
    "\n",
    "vocab_size = 50265\n",
    "max_length = 64\n",
    "\n",
    "inputs_ids = Input(shape = (max_length,), dtype = 'int32')\n",
    "inputs_mask = Input(shape = (max_length,), dtype = 'int32')\n",
    "\n",
    "inputs = [inputs_ids, inputs_mask]\n",
    "\n",
    "\n",
    "sentence_encoder = TFRobertaModel.from_pretrained('roberta-base',\n",
    "                                               output_attentions = False,\n",
    "                                               output_hidden_states = False,\n",
    "                                               )\n",
    "sentence_encoder.config.type_vocab_size = 2 \n",
    "sentence_encoder.roberta.embeddings.token_type_embeddings = Embedding(2, sentence_encoder.config.hidden_size)\n",
    "\n",
    "encoded = sentence_encoder(inputs_ids, attention_mask = inputs_mask)\n",
    "encoded = encoded[0]\n",
    "\n",
    "drop = Dropout(0.3)(encoded)\n",
    "\n",
    "out = tf.keras.layers.Dense(vocab_size)(drop)\n",
    "\n",
    "\n",
    "model = Model(inputs, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_encoder.save_weights('test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "  \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "def sparse_acc(true,pred):\n",
    "    \n",
    "    pred = tf.cast(tf.math.argmax(pred, axis = -1), dtype = true.dtype)\n",
    "    \n",
    "    p = tf.equal(true, pred)\n",
    "    p = tf.cast(p, dtype = true.dtype)\n",
    "    \n",
    "    mask = tf.math.logical_not(tf.math.equal(true, 0))\n",
    "    mask = tf.cast(mask, dtype = true.dtype)\n",
    "    \n",
    "    p = p*mask\n",
    "    \n",
    "    \n",
    "    return tf.reduce_sum(p) / tf.reduce_sum(mask)\n",
    "    \n",
    "    \n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000, factor = 1):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.factor = factor\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2) / self.factor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_save = 0\n",
    "batch_save = 5\n",
    "model.load_weights('./checkpoints/tweetberta_epoch_'+str(epochs_save)+'_batch_'+str(batch_save)+'/checkpoint.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "# ep_save = 5\n",
    "for i in range(epochs_save, EPOCHS):\n",
    "    epochs_save = 0\n",
    "    print('**********************      EPOCH '+str(i)+'        **************************')\n",
    "    \n",
    "    learning_rate = CustomSchedule(512, factor = 1)\n",
    "    loss_classif     =  loss_function\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "                    3e-5, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "    metrics_classif  =  ['sparse_categorical_accuracy', sparse_acc]\n",
    "    \n",
    "    model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "\n",
    "    for batch in range(batch_save + 1,16):\n",
    "        batch_save = 0\n",
    "        print('*********** BATCH '+str(batch))\n",
    "        \n",
    "        X, X_masks, X_masked, X_masks_masked, Y, Y_label = load('batch_'+str(batch), 'batch')\n",
    "        \n",
    "        X_train = [np.array(X_masked), np.array(X_masks_masked)]\n",
    "        y_train = np.array(Y)\n",
    "        \n",
    "        print('batch_loaded')\n",
    "        \n",
    "        batch_size = 32\n",
    "        epochs = 1\n",
    "        history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs)\n",
    "        \n",
    "        os.mkdir('./checkpoints/tweetberta_epoch_'+str(i)+'_batch_'+str(batch))\n",
    "        model.save_weights('./checkpoints/tweetberta_epoch_'+str(i)+'_batch_'+str(batch)+'/checkpoint.h5py')\n",
    "        sentence_encoder.save_weights('./checkpoints/roberta_layer_epoch_'+str(i)+'_batch_'+str(batch)+'/checkpoint.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
