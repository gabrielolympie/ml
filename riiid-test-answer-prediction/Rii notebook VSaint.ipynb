{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import _pickle as pickle\n",
    "import gc\n",
    "from multiprocess import Pool\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def save(file,name, folder = \"\"):\n",
    "    if folder != \"\":\n",
    "        outfile = open('./'+folder+'/'+name+'.pickle', 'wb')\n",
    "    else:\n",
    "        outfile = open(name+'.pickle', 'wb')\n",
    "    pickle.dump(file, outfile, protocol=4)\n",
    "    outfile.close\n",
    "    \n",
    "def load(name, folder = \"\"):\n",
    "    if folder != \"\":\n",
    "        outfile = open('./'+folder+'/'+name+'.pickle', 'rb')\n",
    "    else:\n",
    "        outfile = open(name+'.pickle', 'rb')\n",
    "    file = pickle.load(outfile)\n",
    "    outfile.close\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv('train.csv')\n",
    "train = load('train')\n",
    "\n",
    "# train[train['content_id'] == 0] = 13433\n",
    "\n",
    "lectures = pd.read_csv('lectures.csv')\n",
    "questions = pd.read_csv('questions.csv')\n",
    "\n",
    "test = pd.read_csv('example_test.csv')\n",
    "sample = pd.read_csv('example_sample_submission.csv')\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['content_type_id'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_questions = {}\n",
    "\n",
    "for q, data in tqdm(train.groupby('content_id'), total = train['content_id'].nunique()):\n",
    "    dico_questions['q_'+str(q)] = data['answered_correctly'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(dico_questions, 'dico_questions_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeDataGenerator:\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        self.data will be a dictionnary to iterate over the stored data\n",
    "        self.all_rows will be the rows of the train set that are used by the generato\n",
    "        self.data_index will be all the data available in the dataset        \n",
    "        '''\n",
    "        self.data = None\n",
    "        self.all_rows = None\n",
    "        self.data_index = None\n",
    "        return None\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        sub = sample[['row_id', 'group_num']].copy()\n",
    "        sub['answered_correctly'] = np.zeros(sub.shape[0])+0.5\n",
    "        return (sample, sub)\n",
    "    \n",
    "    \n",
    "    def load(self, save_name):\n",
    "        self.data,self.all_rows = load(save_name)\n",
    "        self.data_index = np.array(list(self.data.keys()))\n",
    "    \n",
    "    def build_from_train(self, train, n_users, beginner_rate = 0.3, save_name = 'fake_train_generator'):\n",
    "        \"\"\"\n",
    "        train will be the training set you loaded\n",
    "        n_users is a number of user from whom you will sample the data\n",
    "        beginner_rate is the rate of these users who will begin their journey during test\n",
    "        save_name : the name under which the item will be saved\n",
    "        \"\"\"\n",
    "        \n",
    "        ## Sampling a restricted list of users\n",
    "        user_list = train['user_id'].unique()\n",
    "        test_user_list = np.random.choice(user_list, size = n_users)\n",
    "        train.index = train['user_id']\n",
    "        test_data_non_filter = train.loc[test_user_list]\n",
    "        test_data_non_filter.index = list(range(test_data_non_filter.shape[0]))\n",
    "        \n",
    "        ## building a dictionnary with all the rows and container id from a user\n",
    "        dico_user = {}\n",
    "        def agg(x):\n",
    "            return [elt for elt in x]\n",
    "        \n",
    "        print(\"Generating user dictionnary\")\n",
    "        for user, frame in tqdm(test_data_non_filter.groupby('user_id'), total =test_data_non_filter['user_id'].nunique()):\n",
    "            if frame.shape[0] > 0:\n",
    "                dico_user[user] = {}\n",
    "\n",
    "                dico_user[user]['min_indice'] = frame['task_container_id'].min()\n",
    "                dico_user[user]['max_indice'] = frame['task_container_id'].max()\n",
    "\n",
    "                r = random.uniform(0,1)\n",
    "                if r < beginner_rate:\n",
    "                    dico_user[user]['current_indice'] = dico_user[user]['min_indice']\n",
    "                else:\n",
    "                    dico_user[user]['current_indice'] = random.randint(dico_user[user]['min_indice'],dico_user[user]['max_indice']-2)\n",
    "\n",
    "                row_ids = frame[['task_container_id','row_id']].groupby('task_container_id').agg(agg)\n",
    "                row_ids = row_ids.to_dict()['row_id']\n",
    "                dico_user[user]['row_ids'] = row_ids\n",
    "\n",
    "        work_dico = deepcopy(dico_user)\n",
    "        \n",
    "        ## Choosing batch_data to generate\n",
    "        work_dico = deepcopy(dico_user)\n",
    "        batches = {}\n",
    "\n",
    "        all_rows = []\n",
    "        batch_number = 0\n",
    "        \n",
    "        print('Creating batches')\n",
    "        while len(work_dico)> 1:\n",
    "\n",
    "            size = random.randint(20,500)\n",
    "            size = min(size, len(work_dico))\n",
    "\n",
    "\n",
    "            batch = []\n",
    "\n",
    "            users = np.random.choice(np.array(list(work_dico.keys())),replace = False,  size = size)\n",
    "\n",
    "            for u in users:\n",
    "                try:\n",
    "                    batch.extend(work_dico[u]['row_ids'][work_dico[u]['current_indice']])\n",
    "                    all_rows.extend(work_dico[u]['row_ids'][work_dico[u]['current_indice']])\n",
    "                    work_dico[u]['current_indice'] += 1\n",
    "                    if work_dico[u]['current_indice'] == work_dico[u]['max_indice']:\n",
    "                        work_dico.pop(u)\n",
    "                except:\n",
    "                    work_dico.pop(u)\n",
    "\n",
    "            batches[batch_number] = batch\n",
    "            batch_number += 1\n",
    "        \n",
    "        ## building data\n",
    "\n",
    "        data = {}\n",
    "        \n",
    "        print(\"Building dataset\")\n",
    "        test_data_non_filter.index = test_data_non_filter['row_id']\n",
    "        for i in tqdm(batches):\n",
    "            current_data = test_data_non_filter.loc[np.array(batches[i])]\n",
    "            current_data['group_num'] = i\n",
    "\n",
    "            current_data['prior_group_answers_correct'] = [np.nan for elt in range(current_data.shape[0])]\n",
    "            current_data['prior_group_responses'] = [np.nan for elt in range(current_data.shape[0])]\n",
    "\n",
    "            if i != 0:\n",
    "                current_data['prior_group_answers_correct'].iloc[0] = saved_correct_answer\n",
    "                current_data['prior_group_responses'].iloc[0] = saved_answer\n",
    "\n",
    "            saved_answer = str(list(current_data[current_data['content_type_id'] == 0]['user_answer'].values))\n",
    "            saved_correct_answer = str(list(current_data[current_data['content_type_id'] == 0]['answered_correctly'].values))\n",
    "            current_data = current_data.drop(columns = ['user_answer', 'answered_correctly'])\n",
    "\n",
    "            data[i] = current_data\n",
    "\n",
    "        save((data,np.array(all_rows)) , save_name)\n",
    "        \n",
    "        self.data = data\n",
    "        self.all_rows = np.array(all_rows)\n",
    "        self.data_index = np.array(list(data.keys()))\n",
    "        print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FakeDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.build_from_train(train, 15000, beginner_rate = 0.3, save_name = 'fake_train_generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(env.all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.load('fake_train_generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.all_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.index = train['row_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(index = env.all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(train, 'train_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "393656/5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dico = {}\n",
    "count = 0\n",
    "for userid, data in tqdm(train.groupby('user_id'), total = train['user_id'].nunique()):\n",
    "    dico[userid] = data\n",
    "    if len(dico.keys()) == 10000:\n",
    "        save(dico, 'userbatch_'+str(count), 'user_batch')\n",
    "        count+=1\n",
    "        dico = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elt in train.columns:\n",
    "    print(elt + '       '+ str(train[elt].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u115.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "timestamp : relative time since first interaction\n",
    "\n",
    "user_id : identifier of the user\n",
    "\n",
    "content_id : identifier of the content\n",
    "\n",
    "content_type_id : 0 = question, 1 = lecture\n",
    "\n",
    "task_container_id : identifier of a sequence of question (ie correction a la fin de la sequence)\n",
    "\n",
    "user_answer : user answer\n",
    "\n",
    "answered correctly : the user answered correctly to the question\n",
    "\n",
    "prior_quesiton_elapsed_time : avg time the user spend on the last container\n",
    "\n",
    "prior_question_had_explanatione : in a same bundle if the user have seen the answer of the last question or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elt in questions.columns:\n",
    "    print(elt + '       '+ str(questions[elt].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions['bundle_id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts\n",
    "Section 1 listening\n",
    "1 : 6 questions, four oral statement about photo choose right one\n",
    "2 : 25 questions, 3 reponse for one question oraly\n",
    "3 : 39 questions, conversation between people, question written, select best answer\n",
    "4 : 30 questions, talks or narrations ...\n",
    "    \n",
    "Section 2 reading\n",
    "5 : 30 questions, incomplete sentence completion\n",
    "6 : 16 questions, text completion\n",
    "7 : 29 + 25 questions, text understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lectures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lectures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lectures['type_of'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elt in lectures.columns:\n",
    "    print(elt + '       '+ str(lectures[elt].nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\n",
    "    'content_id',\n",
    "    'content_type_id',\n",
    "    'task_container_id',\n",
    "    'user_answer_last',\n",
    "    'answered_correctly_last',\n",
    "    'prior_question_had_explanation',\n",
    "]\n",
    "\n",
    "num_cols = [\n",
    "    \"timestamp\",\n",
    "    \"prior_question_elapsed_time\",\n",
    "]\n",
    "\n",
    "pred_col = 'answered_correctly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence1 : ['content_id' + 'type_id','time_spent_discretised', 'answer', 'answer_correctly']\n",
    "sequence2 (embedding) : ['timestamp']\n",
    "sequence3 (embedding): ['number of event before']\n",
    "mask1 :  'padding_mask'\n",
    "mask2 :  'answer_correctly_mask'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load('train_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user = train[train['user_id'] == 115]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_sequence(df_user):\n",
    "    \n",
    "    df_user =  df_user.sort_values(by = 'timestamp')\n",
    "    df_user.index = list(range(df_user.shape[0]))\n",
    "    \n",
    "    df_user['content_type'] =  df_user['content_type_id'].apply(lambda x : 'q' if x == 0 else 'l')\n",
    "    df_user['content_seq'] = df_user['content_type'].astype(str) + '_' + df_user['content_id'].astype(str)\n",
    "    \n",
    "    ## Encoder\n",
    "    exercise_id = df_user['content_seq'].values\n",
    "    container_id = df_user['task_container_id'].values\n",
    "    timestamp = df_user['timestamp'].values\n",
    "    \n",
    "    ## Decoder\n",
    "    correctness = df_user['answered_correctly'].values\n",
    "    answer = df_user['user_answer'].values\n",
    "    \n",
    "    elapsed_time = df_user['prior_question_elapsed_time'].values[1:] ## Already Padded\n",
    "    prior_question_had_explanation = df_user['prior_question_had_explanation'].values[1:]*1 ## Already Padded\n",
    "    \n",
    "    lag_time = timestamp[1:] - timestamp[:1] + elapsed_time\n",
    "    \n",
    "    dico = {\n",
    "        'exercise_id' : exercise_id,\n",
    "        'container_id' : container_id,\n",
    "        'timestamp' : timestamp,\n",
    "        'correctness' : correctness,\n",
    "        'answer' : answer, \n",
    "        'elapsed_time' : elapsed_time,\n",
    "        'prior_question_had_explanation' : prior_question_had_explanation,\n",
    "        'lag_time' : lag_time\n",
    "    }\n",
    "    return dico\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "dico = build_user_sequence(test_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "count = 0\n",
    "vect = []\n",
    "count = 0\n",
    "p = Pool(12)\n",
    "\n",
    "for elt in tqdm(train.groupby('user_id'), total = train['user_id'].nunique()):\n",
    "    vect.append(elt)\n",
    "    if len(vect) == batch_size:\n",
    "        vect = np.array(vect)\n",
    "        vect_user = vect[:,0]\n",
    "        vect_data = vect[:,1]\n",
    "        vect = []\n",
    "        \n",
    "        processed_dico = p.map(build_user_sequence, vect_data)\n",
    "        \n",
    "        dico_user = {}\n",
    "        for i, elt in enumerate(vect_user):\n",
    "            dico_user[elt] = processed_dico[i]\n",
    "        save(dico_user, 'batch_'+str(count), 'user_batch_saint')\n",
    "        count += 1\n",
    "        \n",
    "p.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_user = load('batch_'+str(0), 'user_batch_saint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building tokenizer\n",
    "lectures = pd.read_csv('lectures.csv')\n",
    "questions = pd.read_csv('questions.csv')\n",
    "user_answer = np.array([-1,0,1,2,3])\n",
    "answered_correctly = np.array([-1,0,1])\n",
    "\n",
    "\n",
    "lectures_id = lectures['lecture_id'].unique()\n",
    "question_id = questions['question_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lectures_id = ['l_' +  elt for elt in  lectures_id.astype(str)]\n",
    "question_id = ['q_' +  elt for elt in  question_id.astype(str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = np.array(['[PAD]'] + lectures_id + question_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters = '')\n",
    "\n",
    "tokenizer.fit_on_texts(\n",
    "    all_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(tokenizer, 'tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load('tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_user[115].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 0\n",
    "for elt in dico_user:\n",
    "    a = len(dico_user[elt]['container_id'])\n",
    "    if a >= m:\n",
    "        m = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionnaries():\n",
    "    lectures = pd.read_csv('lectures.csv')\n",
    "    questions = pd.read_csv('questions.csv')\n",
    "    \n",
    "    ## lecture\n",
    "    id_lectures = lectures['lecture_id']\n",
    "    part_lecture = lectures['part']\n",
    "    tag_lecture = lectures['tag']\n",
    "    \n",
    "    id_to_part = {}\n",
    "    id_to_tag = {}\n",
    "    \n",
    "    for i, line in lectures.iterrows():\n",
    "        ids = 'l_' + str(line['lecture_id'])\n",
    "        id_to_part[ids] = line['part']\n",
    "        id_to_tag[ids] = line['tag']\n",
    "    \n",
    "    for i, line in questions.iterrows():\n",
    "        ids = 'q_' + str(line['question_id'])\n",
    "        id_to_part[ids] = line['part']\n",
    "        try:\n",
    "            id_to_tag[ids] = np.array(str(line['tags']).split(' ')).astype(int)\n",
    "        except:\n",
    "            id_to_tag[ids] = np.array([])\n",
    "            \n",
    "    return id_to_part, id_to_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self,batch_size=32, max_len = 128, folder = 'user_batch_saint'):\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = load('tokenizer')\n",
    "        self.max_len = max_len\n",
    "        self.folder = folder\n",
    "        self.dico_question = load('dico_questions_mean')\n",
    "        self.id_to_part, self.id_to_tag = create_dictionnaries()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 1000000\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ## Load random batch\n",
    "        file_name = random.choice(os.listdir('./'+self.folder))\n",
    "        dico_user = load(file_name.split('.')[0], self.folder)\n",
    "        \n",
    "        list_user = np.random.choice(list(dico_user.keys()), size = self.batch_size)\n",
    "        \n",
    "        ## Encoder\n",
    "        exercise = []\n",
    "        container = []\n",
    "        was_tagged = []\n",
    "        timestamp = []\n",
    "        question_mean = []\n",
    "        parts = []\n",
    "        \n",
    "        ## Ouput\n",
    "        output = []\n",
    "        \n",
    "        ## Decoder\n",
    "        correctness = []\n",
    "        answer = []\n",
    "        elapsed_time = []\n",
    "        lag_time = []\n",
    "        was_explained = []\n",
    "        \n",
    "        \n",
    "        for user in list_user:\n",
    "            ex = list(dico_user[user]['exercise_id'])\n",
    "            cont  = list(dico_user[user]['container_id'])\n",
    "            times = list(dico_user[user]['timestamp'])\n",
    "            \n",
    "            y = list(dico_user[user]['correctness'] + 1)\n",
    "            \n",
    "            ans = list(dico_user[user]['answer'] + 1)\n",
    "            el = list(dico_user[user]['elapsed_time'])\n",
    "            lag = list(dico_user[user]['lag_time'])\n",
    "            expl = dico_user[user]['prior_question_had_explanation']\n",
    "            \n",
    "            a = (expl == 0)\n",
    "            b = (expl == 1)\n",
    "            c = np.bitwise_not(a|b)\n",
    "            expl[c] = 0\n",
    "#             expl[np.isnan(expl)] = 0\n",
    "            expl = list(expl)\n",
    "            \n",
    "            ## Choose if we start from start or not\n",
    "            a = random.uniform(0,1)\n",
    "            if a < 0.5:\n",
    "                start = 0\n",
    "            else:\n",
    "                start = random.choice(list(range(len(ex))))\n",
    "            \n",
    "            ex = ex[start:]\n",
    "            cont = cont[start:]\n",
    "            times = times[start:]\n",
    "            \n",
    "            y = y[start:]\n",
    "            \n",
    "            cor = [0] + y[:-1]\n",
    "            ans = [2] + ans[:-1]\n",
    "            lag = [0] + lag\n",
    "            el =  [0] + el\n",
    "            expl = [0] + expl\n",
    "            \n",
    "            ## Padding\n",
    "            while len(ex) <= self.max_len:\n",
    "                ex += ['[PAD]']\n",
    "                cont += [0]\n",
    "                times += [0]\n",
    "                y += [3]\n",
    "                cor += [3]\n",
    "                ans += [0]\n",
    "                lag += [0]\n",
    "                el += [0]\n",
    "                expl += [0]\n",
    "                \n",
    "            ex = ex[:self.max_len]\n",
    "            cont = cont[:self.max_len]\n",
    "            times = times[:self.max_len]\n",
    "            y = y[:self.max_len]\n",
    "            cor = cor[:self.max_len]\n",
    "            ans = ans[:self.max_len]\n",
    "            lag = lag[:self.max_len]\n",
    "            el = el[:self.max_len]\n",
    "            expl = expl[:self.max_len]\n",
    "            \n",
    "            \n",
    "            \n",
    "            ## Mean of questions\n",
    "            qm = []\n",
    "            for elt in ex:\n",
    "                try:\n",
    "                    qm.append(self.dico_question[elt])\n",
    "                except:\n",
    "                    qm.append(0.5)\n",
    "            \n",
    "            ## Add if a question was tagged in a previous lecture\n",
    "            tagged = []\n",
    "            is_tagged = []\n",
    "            for elt in ex:\n",
    "                if elt[0] == 'l':\n",
    "                    tagged.append(self.id_to_tag[elt])\n",
    "                    is_tagged.append(0)\n",
    "                else:\n",
    "                    try:\n",
    "                        tags = self.id_to_tag[elt]\n",
    "                    except:\n",
    "                        tags = []\n",
    "                    cond = False\n",
    "                    for elt in tags:\n",
    "                        if elt in tagged:\n",
    "                            cond = True\n",
    "                    if cond:\n",
    "                        is_tagged.append(1)\n",
    "                    else:\n",
    "                        is_tagged.append(0)\n",
    "                        \n",
    "            ## Add part of sequence\n",
    "            p = []\n",
    "            for elt in ex:\n",
    "                try:\n",
    "                    p.append(self.id_to_part[elt])\n",
    "                except:\n",
    "                    p.append(0)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            exercise.append(ex)\n",
    "            container.append(cont)\n",
    "            was_tagged.append(is_tagged)\n",
    "            timestamp.append(times)\n",
    "            question_mean.append(qm)\n",
    "            parts.append(p)\n",
    "\n",
    "            ## Ouput\n",
    "            output.append(y)\n",
    "\n",
    "            ## Decoder\n",
    "            correctness.append(cor)\n",
    "            answer.append(ans)\n",
    "            elapsed_time.append(el)\n",
    "            lag_time.append(lag)\n",
    "            was_explained.append(expl)\n",
    "            \n",
    "        exercise = self.tokenizer.texts_to_sequences(exercise)\n",
    "        \n",
    "        ## Numpyisation\n",
    "        exercise = np.array(exercise)  ## 14000\n",
    "        container = np.array(container) ## 10000\n",
    "        was_tagged = np.array(was_tagged) ## 2\n",
    "        timestamp = np.array(timestamp) ## Num log\n",
    "        question_mean = np.array(question_mean) ## Num\n",
    "        parts = np.array(parts) ## 7\n",
    "        \n",
    "        output = np.array(output)\n",
    "        \n",
    "        correctness = np.array(correctness) ## 4\n",
    "        answer = np.array(answer) ## 5\n",
    "        elapsed_time = np.array(elapsed_time) ## Num logged\n",
    "        lag_time = np.array(lag_time) ## Num logged\n",
    "        was_explained = np.array(was_explained) ## 2\n",
    "        \n",
    "        ## Log of high numerical values\n",
    "        timestamp[np.isnan(timestamp)] = 0\n",
    "        timestamp = timestamp.reshape((timestamp.shape[0],timestamp.shape[1], 1))\n",
    "        timestamp = np.log(timestamp+1)/5\n",
    "        \n",
    "        elapsed_time[np.isnan(elapsed_time)] = 0\n",
    "        elapsed_time = elapsed_time.reshape((elapsed_time.shape[0],elapsed_time.shape[1], 1))\n",
    "        elapsed_time = np.log(elapsed_time+1)/5\n",
    "        \n",
    "        lag_time[np.isnan(lag_time)] = 0\n",
    "        lag_time = lag_time.reshape((lag_time.shape[0],lag_time.shape[1], 1))\n",
    "        lag_time = np.log(lag_time+1)/5\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## Other Nums\n",
    "        question_mean[np.isnan(question_mean)] = 0\n",
    "        question_mean = question_mean.reshape((question_mean.shape[0],question_mean.shape[1], 1))\n",
    "        \n",
    "\n",
    "        num_encoder = np.concatenate([timestamp, question_mean], axis = -1).astype('float32')\n",
    "        num_decoder = np.concatenate([elapsed_time, lag_time], axis = -1).astype('float32')\n",
    "        \n",
    "        X = [\n",
    "            exercise,  ## 0\n",
    "            container,  ## 1\n",
    "            was_tagged,  ## 2\n",
    "            parts,  ## 3\n",
    "            num_encoder,  ## 4\n",
    "            \n",
    "            correctness,  ## 5\n",
    "            answer,  ## 6\n",
    "            was_explained,  ## 7\n",
    "            num_decoder     ## 8        \n",
    "        ]\n",
    "\n",
    "        return X, output\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def __get_data(self, batch):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = DataGenerator(batch_size=1024, max_len = 128, folder = 'user_batch_saint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x, y = gen[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_transformers2 import *\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, TimeDistributed, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaintEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers = 2, d_model = 512, num_heads = 8, dff = 1024, \n",
    "                 maximum_position_encoding = 512, rate=0.1, bidirectional_encoder = False):\n",
    "        super(SaintEncoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(14000, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "        \n",
    "        self.container_embedding = tf.keras.layers.Embedding(10000, d_model)\n",
    "        self.was_tagged_embedding = tf.keras.layers.Embedding(2, d_model)\n",
    "        self.parts_embedding = tf.keras.layers.Embedding(8, d_model)\n",
    "        self.nums_embedding = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "        self.bidirectional_encoder = bidirectional_encoder\n",
    "        \n",
    "    def call(self, x, training, container = None, was_tagged = None, parts = None, nums = None, calls = []):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        if self.bidirectional_encoder == False:\n",
    "            look_ahead_mask = create_look_ahead_mask(tf.shape(x)[1])\n",
    "            dec_target_padding_mask = create_padding_mask(x, pad_token = 1)\n",
    "            mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "        else:\n",
    "            mask = create_padding_mask(x)\n",
    "        \n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        container_emb = self.container_embedding(container)\n",
    "        was_tagged_emb = self.was_tagged_embedding(was_tagged)\n",
    "        parts_emb = self.parts_embedding(parts)\n",
    "        nums_emb = self.nums_embedding(nums)\n",
    "        \n",
    "        if 'container' in calls:\n",
    "#             container_emb = self.container_embedding(container)\n",
    "            x += container_emb\n",
    "            \n",
    "        if 'tagged' in calls:\n",
    "#             was_tagged_emb = self.was_tagged_embedding(was_tagged)\n",
    "            x += was_tagged_emb\n",
    "            \n",
    "        if 'parts' in calls:\n",
    "#             parts_emb = self.parts_embedding(parts)\n",
    "            x += parts_emb\n",
    "            \n",
    "        if 'num' in calls:\n",
    "#             nums_emb = self.nums_embedding(nums)\n",
    "            x += nums_emb\n",
    "        \n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x, mask  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaintDecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1, take_encoder = True):\n",
    "        super(SaintDecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "        self.take_encoder = take_encoder\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "        \n",
    "        if self.take_encoder:\n",
    "            attn2, attn_weights_block2 = self.mha2(\n",
    "                enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "            attn2 = self.dropout2(attn2, training=training)\n",
    "            out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "            ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "            res = out2\n",
    "        else:\n",
    "            attn_weights_block2 = attn_weights_block1\n",
    "            ffn_output = self.ffn(out1)  # (batch_size, target_seq_len, d_model)\n",
    "            res = out1\n",
    "        \n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + res)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaintDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "               maximum_position_encoding, rate=0.1, bidirectional_decoder = False, take_encoder = True):\n",
    "        super(SaintDecoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(4, d_model, name = 'embedding')\n",
    "        \n",
    "        self.answer_embeddings =  tf.keras.layers.Embedding(5, d_model, name = 'answer_embeddings')\n",
    "        self.was_explained_embeddings =  tf.keras.layers.Embedding(2, d_model, name = 'was_explained_embeddings')\n",
    "        self.nums_embeddings =  tf.keras.layers.Dense(d_model, name = 'nums_embeddings')\n",
    "\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [SaintDecoderLayer(d_model, num_heads, dff, rate, take_encoder = take_encoder) \n",
    "                           for i in range(num_layers)]\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        self.bidirectional_decoder = bidirectional_decoder\n",
    "    \n",
    "    def call(self, x, enc_output, training = True, padding_mask = None, answer = None, was_explained = None, nums = None, calls = []):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "        \n",
    "        if self.bidirectional_decoder == False:\n",
    "            look_ahead_mask = create_look_ahead_mask(tf.shape(x)[1])\n",
    "            dec_target_padding_mask = create_padding_mask(x)\n",
    "            mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "        else:\n",
    "            mask = create_padding_mask(x, pad_token = 3)\n",
    "        \n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        ## Adding different embeddings\n",
    "        answer_emb = self.answer_embeddings(answer)\n",
    "        was_explained_emb = self.was_explained_embeddings(was_explained)\n",
    "        nums_emb = self.nums_embeddings(nums)\n",
    "        if 'answer' in calls:\n",
    "#             answer_emb = self.answer_embeddings(answer)\n",
    "            x += answer_emb\n",
    "        \n",
    "        if 'explained' in calls:\n",
    "#             was_explained_emb = self.was_explained_embeddings(was_explained)\n",
    "            x += was_explained_emb\n",
    "            \n",
    "        if 'nums' in calls:\n",
    "#             nums_emb = self.nums_embeddings(nums)\n",
    "            x += nums_emb\n",
    "            \n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "\n",
    "inputs_exercise = tf.keras.Input(shape = (max_len,))\n",
    "inputs_container = tf.keras.Input(shape = (max_len,))\n",
    "inputs_was_tagged = tf.keras.Input(shape = (max_len,))\n",
    "inputs_parts = tf.keras.Input(shape = (max_len,))\n",
    "inputs_num_encoder = tf.keras.Input(shape = (max_len,2))\n",
    "\n",
    "inputs_correctness = tf.keras.Input(shape = (max_len,))\n",
    "inputs_answer = tf.keras.Input(shape = (max_len,))\n",
    "inputs_was_explained = tf.keras.Input(shape = (max_len,))\n",
    "inputs_num_decoder = tf.keras.Input(shape = (max_len,2))\n",
    "\n",
    "\n",
    "inputs = [\n",
    "    inputs_exercise,\n",
    "    inputs_container,\n",
    "    inputs_was_tagged,\n",
    "    inputs_parts,\n",
    "    inputs_num_encoder,\n",
    "    \n",
    "    inputs_correctness,\n",
    "    inputs_answer,\n",
    "    inputs_was_explained,\n",
    "    inputs_num_decoder\n",
    "]\n",
    "\n",
    "\n",
    "encoder = SaintEncoder(num_layers = 4, d_model = 512, \n",
    "                       num_heads = 8, dff = 512, \n",
    "                 maximum_position_encoding = max_len, rate=0, bidirectional_encoder = False)\n",
    "\n",
    "decoder = SaintDecoder(num_layers = 4, d_model = 512, \n",
    "                       num_heads = 8, dff = 512, \n",
    "               maximum_position_encoding = max_len, rate=0, bidirectional_decoder = False, take_encoder = True)\n",
    "\n",
    "\n",
    "calls_encoder = ['container', 'tagged', 'parts', 'num']\n",
    "calls_encoder = [\n",
    "    'container', \n",
    "    'tagged', \n",
    "    'parts',             \n",
    "    'num',\n",
    "]\n",
    "\n",
    "encoded, masks = encoder(inputs_exercise, training = True, \n",
    "                         container = inputs_container, was_tagged = inputs_was_tagged, \n",
    "                         parts = inputs_parts, nums = inputs_num_encoder,\n",
    "                        calls = calls_encoder)\n",
    "\n",
    "calls_decoder = [\n",
    "    'answer', \n",
    "    'explained', \n",
    "    'nums',\n",
    "]\n",
    "# calls_decoder = ['answer', 'explained', 'nums']\n",
    "# calls_decoder = []\n",
    "decoded = decoder(inputs_correctness, encoded, training = True, padding_mask = masks, \n",
    "                          answer = inputs_answer, was_explained = inputs_was_explained, nums = inputs_num_decoder,\n",
    "                 calls = calls_decoder)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(4, activation = 'softmax')(decoded)\n",
    "\n",
    "model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./weights/saint_base.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           [(None, 128, 2)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "saint_encoder_1 (SaintEncoder)  ((None, 128, 512), ( 18606592    input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           [(None, 128, 2)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "saint_decoder_1 (SaintDecoder)  (None, 128, 512)     10525696    input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_131 (Dense)               (None, 128, 4)       2052        saint_decoder_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 29,134,340\n",
      "Trainable params: 29,134,340\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.device('/CPU:0'):\n",
    "#     encoded, masks = encoder(x[0], training = True, \n",
    "#                              container = x[1], was_tagged = x[2], \n",
    "#                              parts = x[3], nums = x[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.device('/CPU:0'):\n",
    "#     out = decoder(x[5], encoded, training = True, padding_mask = masks, \n",
    "#                               answer = x[6], was_explained = x[7], nums = x[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                    from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "# pred batch_size, seq_lenght, 3\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask1 = tf.math.logical_not(tf.math.equal(real, 3))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask1 = tf.cast(mask1, dtype=loss_.dtype)\n",
    "    loss_ *= mask1\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "def acc(true, pred):\n",
    "    mask = tf.cast(tf.math.logical_not(tf.math.equal(true, 3)),dtype = true.dtype)\n",
    "    \n",
    "    pred = pred[:,:,:3]\n",
    "    pred = tf.math.argmax(pred, axis=-1, output_type=tf.dtypes.int64, name=None)\n",
    "    pred = tf.cast(pred, dtype = true.dtype)\n",
    "    \n",
    "    pred = pred*mask\n",
    "    true = true*mask\n",
    "    \n",
    "    equal = tf.cast(tf.math.equal(pred, true), dtype = true.dtype)\n",
    "    \n",
    "    n_equal = tf.math.reduce_sum(equal)\n",
    "    n_mask = tf.math.reduce_sum(mask)\n",
    "    n_tot = tf.math.reduce_sum(tf.cast(tf.math.greater(true, -1), dtype = true.dtype))\n",
    "    n_masked = n_tot - n_mask\n",
    "    \n",
    "    return (n_equal - n_masked) / (n_tot - n_masked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Roc_Auc(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, train = None, validation=None):\n",
    "        super(Roc_Auc, self).__init__()\n",
    "        self.train = train\n",
    "        self.validation = validation\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        x_val, y_val = self.validation[0], self.validation[1]\n",
    "        \n",
    "        pred = self.model.predict(x_test, verbose = 0)\n",
    "        \n",
    "        pred = pred[:,:,2]\n",
    "        y_pred = pred.reshape(pred.shape[0]*pred.shape[1])\n",
    "        true = y_test.reshape(pred.shape[0]*pred.shape[1])\n",
    "\n",
    "        y_pred = y_pred[true != 3]\n",
    "        true = true[true != 3]\n",
    "        \n",
    "        y_pred = y_pred[true != 0]\n",
    "        true = true[true != 0]\n",
    "        \n",
    "        true = true - 1 \n",
    "        \n",
    "        metric = roc_auc_score(true, y_pred)\n",
    "        logs['roc_auc_val'] = metric\n",
    "        print(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "loss_classif     =  loss_function # find the right loss for multi-class classification\n",
    "optimizer        =  Adam(3e-5, 1e-8) # find the right optimizer\n",
    "metrics_classif  =  [acc]\n",
    "\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = DataGenerator(batch_size=64, max_len = max_len, folder = 'user_batch_saint')\n",
    "test_gen = DataGenerator(batch_size=1024, max_len = max_len, folder = 'user_batch_saint_test')\n",
    "x_test, y_test = test_gen[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1250 steps, validate on 1024 samples\n",
      "Epoch 1/50\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.4305 - acc: 0.6991{'loss': 0.43053155417442324, 'acc': 0.69905835, 'val_loss': 0.431340117007494, 'val_acc': 0.7057071, 'lr': 3e-05, 'roc_auc_val': 0.7616298495767799}\n",
      "1250/1250 [==============================] - 549s 439ms/step - loss: 0.4305 - acc: 0.6991 - val_loss: 0.4313 - val_acc: 0.7057\n",
      "Epoch 2/50\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.4272 - acc: 0.6991{'loss': 0.4272179885864258, 'acc': 0.6990689, 'val_loss': 0.4315579831600189, 'val_acc': 0.7046973, 'lr': 3e-05, 'roc_auc_val': 0.7615339610980212}\n",
      "1250/1250 [==============================] - 528s 422ms/step - loss: 0.4272 - acc: 0.6991 - val_loss: 0.4316 - val_acc: 0.7047\n",
      "Epoch 3/50\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.4278 - acc: 0.7001{'loss': 0.4278534970998764, 'acc': 0.70012575, 'val_loss': 0.4313716534525156, 'val_acc': 0.7049374, 'lr': 3e-05, 'roc_auc_val': 0.7607599216141382}\n",
      "1250/1250 [==============================] - 533s 426ms/step - loss: 0.4279 - acc: 0.7001 - val_loss: 0.4314 - val_acc: 0.7049\n",
      "Epoch 4/50\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.4266 - acc: 0.7006\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.9999999242136257e-06.\n",
      "{'loss': 0.42661825578212736, 'acc': 0.70062006, 'val_loss': 0.4315062630921602, 'val_acc': 0.70532787, 'lr': 3e-05, 'roc_auc_val': 0.7588080238672412}\n",
      "1250/1250 [==============================] - 532s 425ms/step - loss: 0.4266 - acc: 0.7006 - val_loss: 0.4315 - val_acc: 0.7053\n",
      "Epoch 5/50\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.4273 - acc: 0.7022{'loss': 0.4272680692911148, 'acc': 0.7021843, 'val_loss': 0.4307346474379301, 'val_acc': 0.7065755, 'lr': 2.9999999e-06, 'roc_auc_val': 0.7617691735826291}\n",
      "1250/1250 [==============================] - 532s 426ms/step - loss: 0.4273 - acc: 0.7022 - val_loss: 0.4307 - val_acc: 0.7066\n",
      "Epoch 6/50\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.4274 - acc: 0.7026{'loss': 0.42746520719528197, 'acc': 0.7025719, 'val_loss': 0.4305988159030676, 'val_acc': 0.7070733, 'lr': 2.9999999e-06, 'roc_auc_val': 0.7612909853919336}\n",
      "1250/1250 [==============================] - 535s 428ms/step - loss: 0.4275 - acc: 0.7026 - val_loss: 0.4306 - val_acc: 0.7071\n",
      "Epoch 7/50\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.4280 - acc: 0.7032{'loss': 0.4279861050128937, 'acc': 0.70315135, 'val_loss': 0.4306360427290201, 'val_acc': 0.7070356, 'lr': 2.9999999e-06, 'roc_auc_val': 0.7613990531275787}\n",
      "1250/1250 [==============================] - 532s 425ms/step - loss: 0.4280 - acc: 0.7032 - val_loss: 0.4306 - val_acc: 0.7070\n",
      "Epoch 8/50\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.4276 - acc: 0.7031{'loss': 0.4275803285598755, 'acc': 0.7030517, 'val_loss': 0.4305201955139637, 'val_acc': 0.7071652, 'lr': 2.9999999e-06, 'roc_auc_val': 0.761523055517197}\n",
      "1250/1250 [==============================] - 536s 428ms/step - loss: 0.4276 - acc: 0.7031 - val_loss: 0.4305 - val_acc: 0.7072\n",
      "Epoch 9/50\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.4301 - acc: 0.7030{'loss': 0.43004848141670227, 'acc': 0.70294577, 'val_loss': 0.43046009354293346, 'val_acc': 0.7068207, 'lr': 2.9999999e-06, 'roc_auc_val': 0.7628657228468368}\n",
      "1250/1250 [==============================] - 532s 426ms/step - loss: 0.4300 - acc: 0.7029 - val_loss: 0.4305 - val_acc: 0.7068\n",
      "Epoch 10/50\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.4293 - acc: 0.7023{'loss': 0.42933514323234556, 'acc': 0.702323, 'val_loss': 0.4305031094700098, 'val_acc': 0.70687854, 'lr': 2.9999999e-06, 'roc_auc_val': 0.7623221278563689}\n",
      "1250/1250 [==============================] - 535s 428ms/step - loss: 0.4293 - acc: 0.7023 - val_loss: 0.4305 - val_acc: 0.7069\n",
      "Epoch 11/50\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.4287 - acc: 0.7028{'loss': 0.4287050726890564, 'acc': 0.70280206, 'val_loss': 0.43049376271665096, 'val_acc': 0.70671356, 'lr': 2.9999999e-06, 'roc_auc_val': 0.7622113064994299}\n",
      "1250/1250 [==============================] - 533s 426ms/step - loss: 0.4287 - acc: 0.7028 - val_loss: 0.4305 - val_acc: 0.7067\n",
      "Epoch 12/50\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.4296 - acc: 0.7030\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 2.9999998787388907e-07.\n",
      "{'loss': 0.4295765555381775, 'acc': 0.70300025, 'val_loss': 0.4306689687073231, 'val_acc': 0.7059049, 'lr': 2.9999999e-06, 'roc_auc_val': 0.7633278097815374}\n",
      "1250/1250 [==============================] - 538s 430ms/step - loss: 0.4296 - acc: 0.7030 - val_loss: 0.4307 - val_acc: 0.7059\n",
      "Epoch 13/50\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.4296 - acc: 0.7032{'loss': 0.42958885922431944, 'acc': 0.7031614, 'val_loss': 0.430409150198102, 'val_acc': 0.7073133, 'lr': 2.9999998e-07, 'roc_auc_val': 0.7620568837345996}\n",
      "1250/1250 [==============================] - 541s 433ms/step - loss: 0.4296 - acc: 0.7032 - val_loss: 0.4304 - val_acc: 0.7073\n",
      "Epoch 14/50\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.4286 - acc: 0.7033{'loss': 0.4285698708534241, 'acc': 0.70327884, 'val_loss': 0.4303949996829033, 'val_acc': 0.70694727, 'lr': 2.9999998e-07, 'roc_auc_val': 0.7621783315598848}\n",
      "1250/1250 [==============================] - 541s 432ms/step - loss: 0.4286 - acc: 0.7033 - val_loss: 0.4304 - val_acc: 0.7069\n",
      "Epoch 15/50\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.4267 - acc: 0.7033\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 2.999999821895472e-08.\n",
      "{'loss': 0.4267364072084427, 'acc': 0.7032428, 'val_loss': 0.43042660132050514, 'val_acc': 0.70720744, 'lr': 2.9999998e-07, 'roc_auc_val': 0.7626915321390921}\n",
      "1250/1250 [==============================] - 535s 428ms/step - loss: 0.4267 - acc: 0.7032 - val_loss: 0.4304 - val_acc: 0.7072\n",
      "Epoch 16/50\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.4288 - acc: 0.7033Restoring model weights from the end of the best epoch.\n",
      "{'loss': 0.428847366809845, 'acc': 0.70329976, 'val_loss': 0.4303987566381693, 'val_acc': 0.7068895, 'lr': 3e-08, 'roc_auc_val': 0.7628657228468368}\n",
      "1250/1250 [==============================] - 535s 428ms/step - loss: 0.4288 - acc: 0.7033 - val_loss: 0.4304 - val_acc: 0.7069\n",
      "Epoch 17/50\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.4257 - acc: 0.7030Restoring model weights from the end of the best epoch.\n",
      "{'loss': 0.42574010210037233, 'acc': 0.7029977, 'val_loss': 0.4304484073072672, 'val_acc': 0.7075303, 'lr': 3e-08, 'roc_auc_val': 0.7628657228468368}\n",
      "1250/1250 [==============================] - 541s 433ms/step - loss: 0.4257 - acc: 0.7030 - val_loss: 0.4304 - val_acc: 0.7075\n",
      "Epoch 18/50\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.4266 - acc: 0.7029Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 2.999999892949745e-09.\n",
      "{'loss': 0.42657969324588774, 'acc': 0.7028531, 'val_loss': 0.43044440634548664, 'val_acc': 0.7073858, 'lr': 3e-08, 'roc_auc_val': 0.7628657228468368}\n",
      "1250/1250 [==============================] - 540s 432ms/step - loss: 0.4266 - acc: 0.7029 - val_loss: 0.4304 - val_acc: 0.7074\n",
      "Epoch 19/50\n",
      " 164/1250 [==>...........................] - ETA: 8:07 - loss: 0.4276 - acc: 0.7024WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc,lr\n",
      "{'loss': 0.42750933583260586, 'acc': 0.70241255, 'lr': 2.9999998e-09, 'roc_auc_val': 0.7628663604060888}\n",
      " 164/1250 [==>...........................] - ETA: 8:21 - loss: 0.4276 - acc: 0.7024Epoch 00018: early stopping\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-4063f72d7ea8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# steps_per_epoch = 500\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m#, batch_size=bs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mearly\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "early = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=7, verbose=1, \n",
    "                                                mode='auto', restore_best_weights=True)\n",
    "reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, \n",
    "                                                 mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "roc = Roc_Auc(validation = (x_test,  y_test))\n",
    "\n",
    "bs = 32\n",
    "n_epochs = 50\n",
    "steps_per_epoch = 1250\n",
    "# steps_per_epoch = 500\n",
    "#, batch_size=bs\n",
    "history = model.fit(train_gen, epochs=n_epochs,steps_per_epoch = steps_per_epoch, validation_data=(x_test,  y_test), callbacks = [early, reduce, roc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./weights/saint_base.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(model.save_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x_test, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred[:,:,1]\n",
    "y_pred = pred.reshape(1024*384)\n",
    "true = y_test.reshape(1024*384)\n",
    "\n",
    "y_pred = y_pred[true != 2]\n",
    "true = true[true != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(true, (y_pred >= 0.5)*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./weights/lstmgpt_auc_0.757.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def acc(true, pred):\n",
    "    true1 = np.array(true)\n",
    "    pred1 = np.array(pred)\n",
    "    \n",
    "    pred1 = pred1[true1 < 2]\n",
    "    true1 = true1[true1 < 2]\n",
    "    \n",
    "    \n",
    "    if true1.sum() == 0 or true1.sum() == len(true1):\n",
    "        true1 = np.concatenate([true1, np.array([0,1])])\n",
    "        pred1 = np.concatenate([pred1, np.array([0,1])])\n",
    "    \n",
    "    return roc_auc_score(true1, pred1)\n",
    "\n",
    "def test(true, pred):\n",
    "    p = []\n",
    "    \n",
    "    pred2 = pred.reshape(true.shape[0] * true.shape[1])\n",
    "    true2 = true.reshape(true.shape[0] * true.shape[1])\n",
    "    pred2 = pred2[true2 < 2]\n",
    "    true2 = true2[true2 < 2]\n",
    "    \n",
    "    print(roc_auc_score(true2, pred2))\n",
    "    \n",
    "    for i, elt in enumerate(tqdm(true)):\n",
    "#         print(pred[i])\n",
    "        p.append(acc(elt, pred[i]))\n",
    "    \n",
    "    plt.figure(figsize = (25,15))\n",
    "    plt.hist(p, bins = 50)\n",
    "    \n",
    "    print(np.mean(p))\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred[:,:,:2]\n",
    "\n",
    "def softmax(tab):\n",
    "    e = np.exp(tab)\n",
    "    s = np.sum(e, axis = -1)\n",
    "        \n",
    "    return e[:,:,1] / s\n",
    "\n",
    "pred = softmax(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = test(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ameliorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add context on lecture and tasks\n",
    "\n",
    "cluster lecture and tasks\n",
    "\n",
    "give average score of a given task\n",
    "\n",
    "enhance test set with train set (optimization constraint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
