{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import _pickle as pickle\n",
    "import gc\n",
    "from multiprocess import Pool\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "def save(file,name, folder = \"\"):\n",
    "    if folder != \"\":\n",
    "        outfile = open('./'+folder+'/'+name+'.pickle', 'wb')\n",
    "    else:\n",
    "        outfile = open(name+'.pickle', 'wb')\n",
    "    pickle.dump(file, outfile, protocol=4)\n",
    "    outfile.close\n",
    "    \n",
    "def load(name, folder = \"\"):\n",
    "    if folder != \"\":\n",
    "        outfile = open('./'+folder+'/'+name+'.pickle', 'rb')\n",
    "    else:\n",
    "        outfile = open(name+'.pickle', 'rb')\n",
    "    file = pickle.load(outfile)\n",
    "    outfile.close\n",
    "    return file\n",
    "\n",
    "class Discretiser:\n",
    "    def __init__(self, nbins):\n",
    "        self.nbins = nbins-1\n",
    "        self.map_to = np.arange(self.nbins)/self.nbins\n",
    "        \n",
    "    def fit(self, X):\n",
    "        ## X is a one dimension np array\n",
    "        self.map_from = np.quantile(X, self.map_to)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X1 = (np.interp(X, self.map_from, self.map_to, left=0, right=1, period=None) * self.nbins).astype(int)\n",
    "        return X1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load('train_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user, data in tqdm(train.groupby('user_id'), total = train['user_id'].nunique()):\n",
    "    save(data, str(user), 'individual_users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user = train[train['user_id'] == 115]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_sequence(df_user):\n",
    "    import numpy as np\n",
    "    df_user =  df_user.sort_values(by = 'timestamp')\n",
    "    df_user.index = list(range(df_user.shape[0]))\n",
    "    \n",
    "    df_user['content_type'] =  df_user['content_type_id'].apply(lambda x : 'q' if x == 0 else 'l')\n",
    "    df_user['content_seq'] = df_user['content_type'].astype(str) + '_' + df_user['content_id'].astype(str)\n",
    "    \n",
    "    ## Encoder\n",
    "    exercise_id = df_user['content_seq'].values\n",
    "    container_id = df_user['task_container_id'].values\n",
    "    timestamp = df_user['timestamp'].values/1000  ## Conversion in s\n",
    "    \n",
    "    ## Decoder\n",
    "    correctness = df_user['answered_correctly'].values\n",
    "    answer = df_user['user_answer'].values\n",
    "    \n",
    "    elapsed_time = df_user['prior_question_elapsed_time'].fillna(0).values[1:]/1000 ## Already Padded ## Conversion in s\n",
    "    prior_question_had_explanation = df_user['prior_question_had_explanation'].fillna(0).values[1:]*1 ## Already Padded\n",
    "    \n",
    "    lag_time = np.concatenate([[0],timestamp[1:] - timestamp[:-1] + elapsed_time])\n",
    "    \n",
    "    dico = {\n",
    "        'exercise_id' : exercise_id,\n",
    "        'container_id' : container_id,\n",
    "        'timestamp' : timestamp,\n",
    "        'correctness' : correctness,\n",
    "        'answer' : answer, \n",
    "        'elapsed_time' : elapsed_time,\n",
    "        'prior_question_had_explanation' : prior_question_had_explanation,\n",
    "        'lag_time' : lag_time\n",
    "    }\n",
    "    return dico\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dico = build_user_sequence(test_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico['lag_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "count = 0\n",
    "vect = []\n",
    "p = Pool(12)\n",
    "\n",
    "for elt in tqdm(train.groupby('user_id'), total = train['user_id'].nunique()):\n",
    "    vect.append(elt)\n",
    "    if len(vect) == batch_size:\n",
    "        vect = np.array(vect)\n",
    "        vect_user = vect[:,0]\n",
    "        vect_data = vect[:,1]\n",
    "        vect = []\n",
    "        \n",
    "        processed_dico = p.map(build_user_sequence, vect_data)\n",
    "        \n",
    "        # saving as batches of 2000\n",
    "        dico_user = {}\n",
    "        for i, elt in enumerate(vect_user):\n",
    "            dico_user[elt] = processed_dico[i]\n",
    "        save(dico_user, 'batch_'+str(count), 'user_batch_saint_2000')\n",
    "        \n",
    "        # saving as batches of 100\n",
    "        dico_user = {}\n",
    "        count1 = 0\n",
    "        for i, elt in enumerate(vect_user):\n",
    "            dico_user[elt] = processed_dico[i]\n",
    "            if len(dico_user.keys()) == 100:\n",
    "                save(dico_user, 'batch_'+str(count)+'_'+str(count1), 'user_batch_saint_100')\n",
    "                dico_user = {}\n",
    "                count1+=1\n",
    "        count += 1\n",
    "        \n",
    "p.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and discretisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## timestamp encoder\n",
    "dico_user = load('batch_'+str(0), 'user_batch_saint_2000')\n",
    "el = []\n",
    "for elt in dico_user:\n",
    "    ela = dico_user[elt]['timestamp']\n",
    "    ela[np.isnan(ela)] = 0\n",
    "    el += list(ela)\n",
    "timestamp_enc = Discretiser(300)\n",
    "timestamp_enc.fit(el)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Elapsed time encoder\n",
    "dico_user = load('batch_'+str(0), 'user_batch_saint_2000')\n",
    "el = []\n",
    "for elt in dico_user:\n",
    "    ela = dico_user[elt]['elapsed_time']\n",
    "    ela[np.isnan(ela)] = 0\n",
    "    el += list(ela)\n",
    "elapsed_enc = Discretiser(300)\n",
    "elapsed_enc.fit(el)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lag time encoder\n",
    "dico_user = load('batch_'+str(0), 'user_batch_saint_2000')\n",
    "el = []\n",
    "for elt in dico_user:\n",
    "    ela = dico_user[elt]['lag_time']\n",
    "    ela[np.isnan(ela)] = 0\n",
    "    el += list(ela)\n",
    "lag_time_enc = Discretiser(300)\n",
    "lag_time_enc.fit(el)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question mean encoder\n",
    "dico_question = load('dico_questions_mean')\n",
    "val = list(dico_question.values())\n",
    "qmean_enc = Discretiser(300)\n",
    "qmean_enc.fit(val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving\n",
    "save((timestamp_enc, elapsed_enc, lag_time_enc, qmean_enc), 'discrete_encoders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building tokenizer\n",
    "lectures = pd.read_csv('lectures.csv')\n",
    "questions = pd.read_csv('questions.csv')\n",
    "user_answer = np.array([-1,0,1,2,3])\n",
    "answered_correctly = np.array([-1,0,1])\n",
    "\n",
    "lectures_id = lectures['lecture_id'].unique()\n",
    "question_id = questions['question_id'].unique()\n",
    "\n",
    "lectures_id = ['l_' +  elt for elt in  lectures_id.astype(str)]\n",
    "question_id = ['q_' +  elt for elt in  question_id.astype(str)]\n",
    "\n",
    "all_tokens = np.array(['[PAD]', '[CLS]', '[SEP]', '[MASK]'] + lectures_id + question_id)\n",
    "\n",
    "tokenizer = Tokenizer(filters = '')\n",
    "\n",
    "tokenizer.fit_on_texts(\n",
    "    all_tokens\n",
    ")\n",
    "\n",
    "save(tokenizer, 'tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionnaries():\n",
    "    df = pd.read_csv('questions.csv')\n",
    "    df1 = pd.read_csv('lectures.csv')\n",
    "\n",
    "    def apply(x):\n",
    "        return 'q_'+str(x)\n",
    "\n",
    "    def apply1(x):\n",
    "        return 'l_'+str(x)\n",
    "\n",
    "    def to_tab(x):\n",
    "        if str(x)!='nan':\n",
    "            x = np.array(str(x).split(' ')).astype(int)\n",
    "        else:\n",
    "            x = []\n",
    "        x.sort()\n",
    "        return x\n",
    "\n",
    "    df['tag'] = df['tags'].apply(to_tab)\n",
    "    df['qu'] = df['question_id'].apply(apply)\n",
    "    df1['l'] = df1['lecture_id'].apply(apply1)\n",
    "\n",
    "    ## unique tags part\n",
    "    tags_to_utags = {}\n",
    "    count = 0\n",
    "    for elt in df1['tag']:\n",
    "        if elt in tags_to_utags:\n",
    "            1\n",
    "        else:\n",
    "            tags_to_utags[str(elt)] = count\n",
    "            count+=1\n",
    "\n",
    "    for elt in df['tags']:\n",
    "        if elt in tags_to_utags:\n",
    "            1\n",
    "        else:\n",
    "            tags_to_utags[elt] = count\n",
    "            count+=1\n",
    "    df['utags'] = df['tags'].astype(str).replace(tags_to_utags)\n",
    "    df1['utags'] = df1['tag'].astype(str).replace(tags_to_utags)\n",
    "\n",
    "    ## Graph tags part\n",
    "    dico_l = {}\n",
    "    for t, data in df1.groupby('tag'):\n",
    "        dico_l[t] = data['l'].unique()\n",
    "\n",
    "    import networkx as nx\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(df['qu'])\n",
    "    G.add_nodes_from(df1['l'])\n",
    "\n",
    "    for i, elt in enumerate(tqdm(df['tag'])):\n",
    "        for j in elt:\n",
    "            try:\n",
    "                lec = dico_l[j]\n",
    "            except:\n",
    "                lec = []\n",
    "            for k in lec:\n",
    "                G.add_edge(df['qu'].iloc[i], k)\n",
    "\n",
    "    co = list(nx.connected_components(G))\n",
    "\n",
    "    tags_to_gtags = {}\n",
    "    count = 0\n",
    "    for i, elt in enumerate(tqdm(co)):\n",
    "        for j in elt:\n",
    "            tags_to_gtags[j] = i\n",
    "\n",
    "    df['gtags'] = df['qu'].replace(tags_to_gtags)\n",
    "    df1['gtags'] = df1['l'].replace(tags_to_gtags)\n",
    "\n",
    "    dico_utags = {}\n",
    "    dico_gtags = {}\n",
    "    dico_parts = {}\n",
    "    for pair in zip(df['qu'], df['utags'], df['gtags'], df['part']):\n",
    "        dico_utags[pair[0]] = pair[1]\n",
    "        dico_gtags[pair[0]] = pair[2]\n",
    "        dico_parts[pair[0]] = pair[3]\n",
    "        \n",
    "    for pair in zip(df1['l'], df1['utags'], df1['gtags'], df1['part']):\n",
    "        dico_utags[pair[0]] = pair[1]\n",
    "        dico_gtags[pair[0]] = pair[2]\n",
    "        dico_parts[pair[0]] = pair[3]\n",
    "            \n",
    "    return dico_utags, dico_gtags, dico_parts\n",
    "\n",
    "dico_utags, dico_gtags, dico_parts = create_dictionnaries()\n",
    "\n",
    "save((dico_utags, dico_gtags, dico_parts), 'dico_tags')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sequence Modelling\n",
    "sequence_ids   emb1_main # tokenizer values + cls, pad et mask tokens\n",
    "answer_corr    emb2     # 0,1,2,+mask = 3\n",
    "part           emb8     # 6 parts\n",
    "timestamp      emb3     float discretized in 300 int values\n",
    "answer         emb4     # 0,1,2,3,4,+mask = 5\n",
    "elapsed_time   emb5     float discretized in 300 int values\n",
    "explained      emb6     # 2 possible, lectures put to 0\n",
    "avg_correct    emb7     float discretized in 300 int values\n",
    "\n",
    "## MLM loss\n",
    "sequence unmasked\n",
    "answer_corr unmasked\n",
    "\n",
    "## Next answer prediction loss\n",
    "[CLS] Query [SEP] Sequence Model\n",
    "\n",
    "Sequence Model as in sequence modelling\n",
    "\n",
    "Query\n",
    "question_id     ## id question      \n",
    "answer_corr     ## Mask token\n",
    "part            ## 6 parts\n",
    "timestamp       float discretized in 300 values\n",
    "answer          ## Masked to -1\n",
    "elapsed_time    ## 0\n",
    "explained   0   ## 0\n",
    "avg_correct     ## average score of the question discretized in 300 int values\n",
    "\n",
    "output : unmasked answer_corr    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self,batch_size=32, max_len = 128, folder = 'user_batch_saint_100', strategy = 'begin', mask_rate = 0.15, seq_mask_rate = 0.5, bidirectionnal = True):\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = load('tokenizer')\n",
    "        self.max_len = max_len\n",
    "        self.folder = folder\n",
    "        self.dico_question = load('dico_questions_mean')\n",
    "        self.dico_utags, self.dico_gtags, self.dico_parts = load('dico_tags')\n",
    "        self.timestamp_enc, self.elapsed_enc,self.lag_time_enc, self.qmean_enc = load('discrete_encoders')\n",
    "        self.strategy = strategy\n",
    "        self.mask_rate = mask_rate\n",
    "        self.seq_mask_rate = seq_mask_rate\n",
    "        self.bidirectionnal = bidirectionnal\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 1000000\n",
    "    \n",
    "    def initiate_dico(self):\n",
    "        list_encoder = ['exercise', 'part', 'utag', 'gtag', 'timestamp', 'question_mean']\n",
    "        list_decoder = ['correct', 'answer', 'elapsed_time', 'lag_time', 'was_explained']\n",
    "        list_output = ['exercise', 'answer', 'correct']\n",
    "        \n",
    "        dico_input = {}\n",
    "        for elt in list_encoder + list_decoder:\n",
    "            if elt == 'exercise':\n",
    "                dico_input[elt] = np.zeros((self.batch_size, self.max_len)).astype(str)\n",
    "            else:\n",
    "                dico_input[elt] = np.zeros((self.batch_size, self.max_len)).astype('int32')\n",
    "        \n",
    "        dico_output = {}\n",
    "        for elt in list_output:\n",
    "            if elt == 'exercise':\n",
    "                dico_output[elt] = np.zeros((self.batch_size, self.max_len)).astype(str)\n",
    "            else:\n",
    "                dico_output[elt] = np.zeros((self.batch_size, self.max_len)).astype('int32')\n",
    "        return dico_input, dico_output\n",
    "\n",
    "    def map_part(self, ids):\n",
    "        def replace_dico_part(x):\n",
    "            try:\n",
    "                return self.dico_parts[x]\n",
    "            except:\n",
    "                return 0\n",
    "        return np.array(list(map(replace_dico_part,ids)))\n",
    "    \n",
    "    def map_utags(self, ids):\n",
    "        def replace_dico_utags(x):\n",
    "            try:\n",
    "                if str(self.dico_utags[x]) != 'nan':\n",
    "                    return str(self.dico_utags[x])\n",
    "                else:\n",
    "                    return 0\n",
    "            except:\n",
    "                return 0\n",
    "        return np.array(list(map(replace_dico_utags,ids)))\n",
    "    \n",
    "    def map_gtags(self, ids):\n",
    "        def replace_dico_gtags(x):\n",
    "            try:\n",
    "                if str(self.dico_gtags[x]) != 'nan':\n",
    "                    return str(self.dico_gtags[x])\n",
    "                else:\n",
    "                    return 0\n",
    "            except:\n",
    "                return 0\n",
    "        return np.array(list(map(replace_dico_gtags,ids)))\n",
    "    \n",
    "    def map_mean(self, ids):\n",
    "        def replace_dico_question(x):\n",
    "            try:\n",
    "                return self.dico_question[x]\n",
    "            except:\n",
    "                return 0.5\n",
    "        return np.array(list(map(replace_dico_question,ids)))\n",
    "\n",
    "\n",
    "    \n",
    "    def update_dico(self, dico_input, dico_output, input_vals, output_vals, i):\n",
    "        list_encoder = ['exercise', 'part', 'utag', 'gtag', 'timestamp', 'question_mean']\n",
    "        list_decoder = ['correct', 'answer', 'elapsed_time', 'lag_time', 'was_explained']\n",
    "        list_output = ['exercise', 'answer', 'correct']\n",
    "        \n",
    "        for j, elt in enumerate(list_encoder + list_decoder):\n",
    "            dico_input[elt][i] = input_vals[j]\n",
    "        \n",
    "        for j, elt in enumerate(list_output):\n",
    "            dico_output[elt][i] = output_vals[j]\n",
    "        return dico_input, dico_output\n",
    "\n",
    "    def remove_na(self, x):\n",
    "        x = np.array(list(x))\n",
    "        x[np.isnan(x)] = 0\n",
    "        return x\n",
    "    \n",
    "    def apply_mask(self, x, mask, pad_token, mask_token):\n",
    "        x_out = []\n",
    "        x_in = []\n",
    "        for i, elt in enumerate(mask):\n",
    "            if mask[i] == 1:\n",
    "                x_out.append(x[i])\n",
    "                x_in.append(mask_token)\n",
    "            else:\n",
    "                x_out.append(pad_token)\n",
    "                x_in.append(x[i])\n",
    "        return np.array(x_in), np.array(x_out)\n",
    "\n",
    "    def build_sequence(self, user_history):\n",
    "        dico_sequence = deepcopy(user_history)        \n",
    "        dico_sequence['elapsed_time'] = self.remove_na(dico_sequence['elapsed_time'])\n",
    "        dico_sequence['lag_time'] = self.remove_na(dico_sequence['lag_time'])\n",
    "        dico_sequence['prior_question_had_explanation'] = self.remove_na(dico_sequence['prior_question_had_explanation'])\n",
    "        \n",
    "        dico_sequence['elapsed_time'] = np.concatenate([dico_sequence['elapsed_time'], [0]])\n",
    "        dico_sequence['prior_question_had_explanation'] = np.concatenate([dico_sequence['prior_question_had_explanation'], [0]])\n",
    "        \n",
    "        \n",
    "        ## Cut sequence\n",
    "        if self.strategy == 'begin':\n",
    "            for elt in dico_sequence:\n",
    "                dico_sequence[elt] = dico_sequence[elt][:self.max_len]\n",
    "        else:\n",
    "            for elt in dico_sequence:\n",
    "                dico_sequence[elt] = dico_sequence[elt][-self.max_len:]\n",
    "        \n",
    "        \n",
    "        \n",
    "         ## Masking\n",
    "        # Either mask question => mask parts, qmean, answer, correctness 0.5%\n",
    "        # Or mask correctness => mask answer, elapsed_time, lag_time, explanation 0.5%\n",
    "        # In all case, mask the last question answer, but we keep its signification\n",
    "        \n",
    "        if self.bidirectionnal == True:\n",
    "            r = random.uniform(0,1)\n",
    "            if r < self.seq_mask_rate:\n",
    "                # masking on the question_id\n",
    "                masks = np.random.choice([0,1],replace = True, size = len(dico_sequence['exercise_id'])-1, p = [1-self.mask_rate,self.mask_rate])\n",
    "\n",
    "    #             print(masks.shape)\n",
    "    #             print(dico_sequence['elapsed_time'].shape)\n",
    "    #             print('\\n')\n",
    "\n",
    "                dico_sequence['elapsed_time'], _ = self.apply_mask(dico_sequence['elapsed_time'], masks, 0, 0)     \n",
    "                dico_sequence['prior_question_had_explanation'], _ = self.apply_mask(dico_sequence['prior_question_had_explanation'], masks, 0, 0) \n",
    "\n",
    "                masks = np.concatenate([masks, [0]])\n",
    "                dico_sequence['exercise_id'], dico_sequence['exercise_id_out'] = self.apply_mask(dico_sequence['exercise_id'], masks, '[PAD]', '[MASK]')            \n",
    "                masks[-1] = 1\n",
    "                dico_sequence['answer'], dico_sequence['answer_out'] = self.apply_mask(dico_sequence['answer'], masks, -1, -1)\n",
    "                dico_sequence['correctness'], dico_sequence['correctness_out'] = self.apply_mask(dico_sequence['correctness'], masks, -1, -1)\n",
    "\n",
    "            else:\n",
    "                # Masking only a part of the answers\n",
    "                dico_sequence['exercise_id_out'] = deepcopy(np.array(['[PAD]' for elt in dico_sequence['exercise_id']]))\n",
    "                masks = np.random.choice([0,1],replace = True, size = len(dico_sequence['correctness'])-1, p = [1-self.mask_rate,self.mask_rate])\n",
    "\n",
    "    #             print(masks.shape)\n",
    "    #             print(dico_sequence['elapsed_time'].shape)\n",
    "    #             print('\\n')\n",
    "\n",
    "                dico_sequence['elapsed_time'], _ = self.apply_mask(dico_sequence['elapsed_time'], masks, 0, 0)    \n",
    "                dico_sequence['prior_question_had_explanation'], _ = self.apply_mask(dico_sequence['prior_question_had_explanation'], masks, 0, 0)\n",
    "\n",
    "                masks = np.concatenate([masks, [1]])\n",
    "                dico_sequence['correctness'], dico_sequence['correctness_out'] = self.apply_mask(dico_sequence['correctness'], masks, -1, -1)\n",
    "                dico_sequence['answer'], dico_sequence['answer_out'] = self.apply_mask(dico_sequence['answer'], masks, -1, -1)\n",
    "        \n",
    "        else:\n",
    "            dico_sequence['exercise_id_out'] = dico_sequence['exercise_id']\n",
    "            dico_sequence['correctness_out'] = dico_sequence['correctness']\n",
    "            dico_sequence['answer_out'] = dico_sequence['answer']\n",
    "        \n",
    "        ## Pad sequence\n",
    "        pad_tokens = ['[PAD]', 0, 0, -1, -1, 0, 0, 0, '[PAD]', -1, -1]\n",
    "        for j, elt in enumerate(dico_sequence):\n",
    "            size = len(dico_sequence[elt])\n",
    "            if size <= self.max_len:\n",
    "                adding = self.max_len - size\n",
    "                tok = pad_tokens[j]\n",
    "                if type(tok) == str:\n",
    "                    add = np.array([tok for elt in range(adding)])\n",
    "                else:\n",
    "                    add = np.zeros(adding) + tok\n",
    "                dico_sequence[elt] = np.concatenate([dico_sequence[elt], add], axis = 0)\n",
    "#                 print(dico_sequence[elt].shape)\n",
    "\n",
    "        if self.bidirectionnal == False:\n",
    "            input_vals = [\n",
    "                dico_sequence['exercise_id'],\n",
    "                self.map_part(dico_sequence['exercise_id']),\n",
    "                self.map_utags(dico_sequence['exercise_id']),\n",
    "                self.map_gtags(dico_sequence['exercise_id']),\n",
    "                self.timestamp_enc.transform(dico_sequence['timestamp']),\n",
    "                self.qmean_enc.transform(self.map_mean(dico_sequence['exercise_id'])),\n",
    "\n",
    "                np.concatenate([[0], dico_sequence['correctness'] + 1])[:-1],\n",
    "                np.concatenate([[0], dico_sequence['answer'] + 1])[:-1],\n",
    "                np.concatenate([[0], self.elapsed_enc.transform(dico_sequence['elapsed_time'])])[:-1],\n",
    "                self.lag_time_enc.transform(dico_sequence['lag_time']),\n",
    "                np.concatenate([[0], dico_sequence['prior_question_had_explanation']])[:-1],\n",
    "            ]\n",
    "\n",
    "            output_vals = [\n",
    "                np.concatenate([dico_sequence['exercise_id_out'][1:], ['[PAD]']]),\n",
    "                dico_sequence['answer_out'] + 1,\n",
    "                dico_sequence['correctness_out'] + 1,\n",
    "            ]\n",
    "            \n",
    "        else:\n",
    "            input_vals = [\n",
    "                dico_sequence['exercise_id'],\n",
    "                self.map_part(dico_sequence['exercise_id']),\n",
    "                self.map_utags(dico_sequence['exercise_id']),\n",
    "                self.map_gtags(dico_sequence['exercise_id']),\n",
    "                self.timestamp_enc.transform(dico_sequence['timestamp']),\n",
    "                self.qmean_enc.transform(self.map_mean(dico_sequence['exercise_id'])),\n",
    "\n",
    "                dico_sequence['correctness'] + 1,\n",
    "                dico_sequence['answer'] + 1,\n",
    "                self.elapsed_enc.transform(dico_sequence['elapsed_time']),\n",
    "                self.lag_time_enc.transform(dico_sequence['lag_time']),\n",
    "                dico_sequence['prior_question_had_explanation'],\n",
    "            ]\n",
    "\n",
    "            output_vals = [\n",
    "                dico_sequence['exercise_id_out'],\n",
    "                dico_sequence['answer_out'] + 1,\n",
    "                dico_sequence['correctness_out'] + 1,\n",
    "            ]\n",
    "        \n",
    "        \n",
    "#         x = np.zeros((11,self.max_len))\n",
    "#         y = np.zeros((3, self.max_len))\n",
    "        return input_vals,output_vals\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ## Load random batch\n",
    "        file_name = random.choice(os.listdir('./'+self.folder))\n",
    "        dico_user = load(file_name.split('.')[0], self.folder)\n",
    "        \n",
    "        list_user = np.random.choice(list(dico_user.keys()), size = self.batch_size)\n",
    "        \n",
    "        dico_input, dico_output = self.initiate_dico()\n",
    "        \n",
    "        \n",
    "        for i, elt in enumerate(list_user):\n",
    "            user_history = dico_user[elt]\n",
    "            input_vals, output_vals = self.build_sequence(user_history)\n",
    "            dico_input, dico_output = self.update_dico(dico_input, dico_output, input_vals, output_vals, i)\n",
    "        \n",
    "        x = deepcopy(dico_input['exercise'])\n",
    "        dico_input['exercise'] = np.array(self.tokenizer.texts_to_sequences([\" \".join(list(x)[elt]) for elt in range(len(x))]))\n",
    "        \n",
    "        x = deepcopy(dico_output['exercise'])\n",
    "        dico_output['exercise'] = np.array(self.tokenizer.texts_to_sequences([\" \".join(list(x)[elt]) for elt in range(len(x))]))\n",
    "        \n",
    "        X = list(np.array(list(dico_input.values())).astype('int32'))\n",
    "        y = list(np.array(list(dico_output.values())).astype('int32')) \n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def __get_data(self, batch):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = DataGenerator(batch_size=32, max_len = 128, folder = 'user_batch_saint_100', strategy = 'end', mask_rate = 0.15, seq_mask_rate = 0.9, bidirectionnal = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x, y = gen[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 0\n",
    "x[0][ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0][ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elt in x:\n",
    "    print(elt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elt in y:\n",
    "    print(elt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_transformers2 import *\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, TimeDistributed, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaintBert(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers = 2, d_model = 512, num_heads = 8, \n",
    "                 dff = 1024, input_vocab_size = 14000, maximum_position_encoding = 512, \n",
    "                 rate=0, bidirectional_encoder = True, layer_type = 'attention'):\n",
    "        super(SaintBert, self).__init__()\n",
    "        self.layer_type = layer_type\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        \n",
    "        ## Additional embeddings\n",
    "        self.part_embedding = tf.keras.layers.Embedding(8, d_model)\n",
    "        self.utag_embedding = tf.keras.layers.Embedding(1900, d_model)\n",
    "        self.gtag_embedding = tf.keras.layers.Embedding(100, d_model)\n",
    "        self.timestamp_embedding = tf.keras.layers.Embedding(301, d_model)\n",
    "        self.question_mean_embedding = tf.keras.layers.Embedding(301, d_model)\n",
    "        \n",
    "        self.correct_embedding = tf.keras.layers.Embedding(3, d_model)\n",
    "        self.answer_embedding = tf.keras.layers.Embedding(8, d_model)   \n",
    "        self.elapsed_time_embedding = tf.keras.layers.Embedding(301, d_model)\n",
    "        self.lag_time_embedding = tf.keras.layers.Embedding(301, d_model)\n",
    "        self.was_explained_embedding = tf.keras.layers.Embedding(2, d_model)\n",
    "        \n",
    "        self.conc1 = tf.keras.layers.Concatenate(axis = -1)\n",
    "        self.agg1 = tf.keras.layers.Dense(d_model, activation = 'relu')\n",
    "        \n",
    "        self.conc2 = tf.keras.layers.Concatenate(axis = -1)\n",
    "        self.agg2 = tf.keras.layers.Dense(d_model, activation = 'relu')\n",
    "        \n",
    "        self.conc3 = tf.keras.layers.Concatenate(axis = -1)\n",
    "        self.agg3 = tf.keras.layers.Dense(d_model, activation = 'relu')\n",
    "        \n",
    "        self.conc4 = tf.keras.layers.Concatenate(axis = -1)\n",
    "        self.agg4 = tf.keras.layers.Dense(d_model, activation = 'relu')\n",
    "        \n",
    "        \n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "        if 'lstm' in layer_type:\n",
    "            self.lstm_layers = [tf.keras.layers.LSTM(d_model, return_sequences = True) \n",
    "                                for _ in range(num_layers)]\n",
    "        if 'attention' in layer_type:\n",
    "            self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                               for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "        self.bidirectional_encoder = bidirectional_encoder\n",
    "        \n",
    "    def call(self, x, training,\n",
    "             part = None,\n",
    "             utag = None,\n",
    "             gtag = None,\n",
    "             timestamp = None,\n",
    "             question_mean = None,\n",
    "             correct = None,\n",
    "             answer = None,\n",
    "             elapsed_time = None,\n",
    "             lag_time = None,\n",
    "             was_explained = None,\n",
    "             take = ['part', 'utag', 'gtag', 'timestamp', 'question_mean',\n",
    "                    'correct', 'answer', 'elapsed_time', 'lag_time', 'was_explained']\n",
    "            ):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        if self.bidirectional_encoder == False:\n",
    "            look_ahead_mask = create_look_ahead_mask(tf.shape(x)[1])\n",
    "            dec_target_padding_mask = create_padding_mask(x, pad_token = 1)\n",
    "            mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "        else:\n",
    "            mask = create_padding_mask(x, pad_token = 1)\n",
    "        \n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "#         x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        \n",
    "        part_emb = self.part_embedding(part)\n",
    "        utag_emb = self.utag_embedding(utag)\n",
    "        gtag_emb = self.gtag_embedding(gtag)\n",
    "        timestamp_emb = self.timestamp_embedding(timestamp)\n",
    "        question_mean_emb = self.question_mean_embedding(question_mean)\n",
    "        \n",
    "        correct_emb = self.correct_embedding(correct)\n",
    "        answer_emb = self.answer_embedding(answer)\n",
    "        elapsed_time_emb = self.elapsed_time_embedding(elapsed_time)\n",
    "        lag_time_emb = self.lag_time_embedding(lag_time)\n",
    "        was_explained_emb = self.was_explained_embedding(was_explained)\n",
    "        \n",
    "        question_features = self.conc1([x, part_emb, utag_emb, gtag_emb, question_mean_emb])\n",
    "        question_features = self.agg1(question_features)\n",
    "        \n",
    "        perf_features = self.conc2([correct_emb, answer_emb, elapsed_time_emb, was_explained_emb])\n",
    "        perf_features = self.agg2(perf_features)\n",
    "        \n",
    "        time_features = self.conc3([timestamp_emb, lag_time_emb])\n",
    "        time_features = self.agg3(time_features)\n",
    "        \n",
    "        all_features = self.conc4([question_features, perf_features, time_features])\n",
    "        x = self.agg4(all_features)\n",
    "        \n",
    "#         if 'part' in take:\n",
    "#             x += part_emb\n",
    "        \n",
    "#         if 'utag' in take:\n",
    "#             x += utag_emb\n",
    "            \n",
    "#         if 'gtag' in take:\n",
    "#             x += gtag_emb\n",
    "            \n",
    "#         if 'timestamp' in take:\n",
    "#             x += timestamp_emb\n",
    "            \n",
    "#         if 'question_mean' in take:\n",
    "#             x += question_mean_emb\n",
    "        \n",
    "#         if 'correct' in take:\n",
    "#             x += correct_emb\n",
    "        \n",
    "#         if 'answer' in take:\n",
    "#             x += answer_emb\n",
    "            \n",
    "#         if 'elapsed_time' in take:\n",
    "#             x += elapsed_time_emb\n",
    "            \n",
    "#         if 'lag_time' in take:\n",
    "#             x += lag_time_emb\n",
    "            \n",
    "#         if 'was_explained' in take:\n",
    "#             x += was_explained_emb\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        if self.layer_type == 'attention':\n",
    "            for i in range(self.num_layers):\n",
    "                x = self.enc_layers[i](x, training, mask)\n",
    "                \n",
    "        elif self.layer_type == 'lstm_attention':\n",
    "            for i in range(self.num_layers):\n",
    "                x = self.lstm_layers[i](x)\n",
    "                x = self.enc_layers[i](x, training, mask)\n",
    "        else:\n",
    "            for i in range(self.num_layers):\n",
    "                x = self.lstm_layers[i](x)\n",
    "                \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "\n",
    "inputs_exercise = tf.keras.Input(shape = (max_len,))\n",
    "inputs_part = tf.keras.Input(shape = (max_len,))\n",
    "inputs_utag = tf.keras.Input(shape = (max_len,))\n",
    "inputs_gtag = tf.keras.Input(shape = (max_len,))\n",
    "inputs_timestamp = tf.keras.Input(shape = (max_len,))\n",
    "inputs_question_mean = tf.keras.Input(shape = (max_len,))\n",
    "\n",
    "inputs_correct = tf.keras.Input(shape = (max_len,))\n",
    "inputs_answer = tf.keras.Input(shape = (max_len,))\n",
    "inputs_elapsed_time = tf.keras.Input(shape = (max_len,))\n",
    "inputs_lag_time = tf.keras.Input(shape = (max_len,))\n",
    "inputs_was_explained = tf.keras.Input(shape = (max_len,))\n",
    "\n",
    "# list_encoder = ['exercise', 'part', 'utag', 'gtag', 'timestamp', 'question_mean']\n",
    "# list_decoder = ['correct', 'answer', 'elapsed_time', 'lag_time', 'was_explained']\n",
    "# list_output = ['exercise', 'answer', 'correct']\n",
    "\n",
    "inputs = [\n",
    "    inputs_exercise,\n",
    "    inputs_part,\n",
    "    inputs_utag,\n",
    "    inputs_gtag,\n",
    "    inputs_timestamp,\n",
    "    inputs_question_mean,\n",
    "    \n",
    "    inputs_correct,\n",
    "    inputs_answer,\n",
    "    inputs_elapsed_time,\n",
    "    inputs_lag_time,\n",
    "    inputs_was_explained, \n",
    "]\n",
    "\n",
    "\n",
    "encoder = SaintBert(\n",
    "        num_layers = 8, d_model = 512, num_heads = 8, \n",
    "        dff = 1024, input_vocab_size = 14000, maximum_position_encoding = max_len, \n",
    "        rate=0, bidirectional_encoder = False, layer_type = 'attention'\n",
    "    )\n",
    "\n",
    "call_encoder = [\n",
    "    'part', \n",
    "    'utag', \n",
    "    'gtag', \n",
    "    'timestamp', \n",
    "    'question_mean',\n",
    "                    \n",
    "    'correct', \n",
    "    'answer', \n",
    "    'elapsed_time', \n",
    "    'lag_time', \n",
    "    'was_explained',\n",
    "]\n",
    "\n",
    "\n",
    "encoded = encoder(inputs_exercise, training = True,\n",
    "             part = inputs_part,\n",
    "             utag = inputs_utag,\n",
    "             gtag = inputs_gtag,\n",
    "             timestamp = inputs_timestamp,\n",
    "             question_mean = inputs_question_mean,\n",
    "             correct = inputs_correct,\n",
    "             answer = inputs_answer,\n",
    "             elapsed_time = inputs_elapsed_time,\n",
    "             lag_time = inputs_lag_time,\n",
    "             was_explained = inputs_was_explained,\n",
    "             take = call_encoder,\n",
    "            )\n",
    "\n",
    "question_head = tf.keras.layers.Dense(14000, activation = 'softmax', name = 'question_head')(encoded)\n",
    "answer_head = tf.keras.layers.Dense(5, activation = 'softmax', name = 'answer_head')(encoded)\n",
    "correct_head = tf.keras.layers.Dense(3, activation = 'softmax', name = 'correct_head')(encoded)\n",
    "\n",
    "outputs = [\n",
    "    question_head,\n",
    "    answer_head,\n",
    "    correct_head,\n",
    "]\n",
    "\n",
    "model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./weights_saint/saintgpt_8l_36_question_6.76_correct_70.75_auc_75.72.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                    from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_qu(real_qu, pred_qu):\n",
    "    mask_qu = tf.math.logical_not(tf.math.equal(real_qu, 1))\n",
    "    loss_qu_ = loss_object(real_qu, pred_qu)\n",
    "    mask_qu = tf.cast(mask_qu , dtype=loss_qu_.dtype)\n",
    "    loss_qu_ *= mask_qu\n",
    "    loss_qu_ = tf.reduce_mean(loss_qu_)\n",
    "    return loss_qu_\n",
    "\n",
    "def loss_co(real_co, pred_co):\n",
    "    mask_co = tf.math.logical_not(tf.math.equal(real_co, 0))\n",
    "    loss_co_ = loss_object(real_co, pred_co)\n",
    "    mask_co = tf.cast(mask_co , dtype=loss_co_.dtype)\n",
    "    loss_co_ *= mask_co\n",
    "    loss_co_ = tf.reduce_mean(loss_co_)\n",
    "    return loss_co_*5\n",
    "\n",
    "def loss_an(real_an, pred_an):\n",
    "    mask_an = tf.math.logical_not(tf.math.equal(real_an, 0))\n",
    "    loss_an_ = loss_object(real_an, pred_an)\n",
    "    mask_an = tf.cast(mask_an , dtype=loss_an_.dtype)\n",
    "    loss_an_ *= mask_an\n",
    "    loss_an_ = tf.reduce_mean(loss_an_)\n",
    "    return loss_an_*5\n",
    "\n",
    "def acc_qu(true, pred):\n",
    "    mask = tf.cast(tf.math.logical_not(tf.math.equal(true, 1)),dtype = true.dtype)\n",
    "#     pred = pred[:,:,:3]\n",
    "    pred = tf.math.argmax(pred, axis=-1, output_type=tf.dtypes.int64, name=None)\n",
    "    pred = tf.cast(pred, dtype = true.dtype)\n",
    "    pred = pred*mask\n",
    "    true = true*mask\n",
    "    equal = tf.cast(tf.math.equal(pred, true), dtype = true.dtype)\n",
    "    n_equal = tf.math.reduce_sum(equal)\n",
    "    n_mask = tf.math.reduce_sum(mask)\n",
    "    n_tot = tf.math.reduce_sum(tf.cast(tf.math.greater(true, -1), dtype = true.dtype))\n",
    "    n_masked = n_tot - n_mask\n",
    "    return (n_equal - n_masked) / ((n_tot - n_masked))\n",
    "\n",
    "def acc_co(true, pred):\n",
    "    mask = tf.cast(tf.math.logical_not(tf.math.equal(true, 0)),dtype = true.dtype)\n",
    "#     pred = pred[:,:,:3]\n",
    "    pred = tf.math.argmax(pred, axis=-1, output_type=tf.dtypes.int64, name=None)\n",
    "    pred = tf.cast(pred, dtype = true.dtype)\n",
    "    pred = pred*mask\n",
    "    true = true*mask\n",
    "    equal = tf.cast(tf.math.equal(pred, true), dtype = true.dtype)\n",
    "    n_equal = tf.math.reduce_sum(equal)\n",
    "    n_mask = tf.math.reduce_sum(mask)\n",
    "    n_tot = tf.math.reduce_sum(tf.cast(tf.math.greater(true, -1), dtype = true.dtype))\n",
    "    n_masked = n_tot - n_mask\n",
    "    return (n_equal - n_masked) / (n_tot - n_masked)\n",
    "\n",
    "def acc_an(true, pred):\n",
    "    mask = tf.cast(tf.math.logical_not(tf.math.equal(true, 0)),dtype = true.dtype)\n",
    "#     pred = pred[:,:,:3]\n",
    "    pred = tf.math.argmax(pred, axis=-1, output_type=tf.dtypes.int64, name=None)\n",
    "    pred = tf.cast(pred, dtype = true.dtype)\n",
    "    pred = pred*mask\n",
    "    true = true*mask\n",
    "    equal = tf.cast(tf.math.equal(pred, true), dtype = true.dtype)\n",
    "    n_equal = tf.math.reduce_sum(equal)\n",
    "    n_mask = tf.math.reduce_sum(mask)\n",
    "    n_tot = tf.math.reduce_sum(tf.cast(tf.math.greater(true, -1), dtype = true.dtype))\n",
    "    n_masked = n_tot - n_mask\n",
    "    return (n_equal - n_masked) / (n_tot - n_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import warnings\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, validation=None, logs = {}, dico_params = {}, from_path = None):\n",
    "        super(CustomCallback, self).__init__()\n",
    "#         self.train = train\n",
    "        self.validation = validation\n",
    "        self.epoch = []\n",
    "        \n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "        self.roc_auc = []\n",
    "        self.val_roc_auc = []\n",
    "        self.accuracy = []\n",
    "        \n",
    "        self.question_head_loss = []\n",
    "        self.val_question_head_loss = []\n",
    "        self.question_head_acc_qu = []\n",
    "        self.val_question_head_acc_qu = []\n",
    "        \n",
    "        self.correct_head_loss = []\n",
    "        self.val_correct_head_loss = []\n",
    "        self.correct_head_acc_co = []\n",
    "        self.val_correct_head_acc_co = []\n",
    "        \n",
    "        self.answer_head_loss = []\n",
    "        self.val_answer_head_loss = []\n",
    "        self.answer_head_acc_an = []\n",
    "        self.val_answer_head_acc_an = []\n",
    "        \n",
    "        self.lr = []\n",
    "        self.qushare = []\n",
    "        self.coshare = []\n",
    "        self.anshare = []\n",
    "        \n",
    "        self.dico_params = dico_params\n",
    "        \n",
    "        if from_path is not None:\n",
    "            (self.epoch,\n",
    "            self.loss, self.val_loss, self.roc_auc, self.val_roc_auc, self.accuracy,\n",
    "            self.question_head_loss, self.val_question_head_loss, self.question_head_acc_qu, self.val_question_head_acc_qu,\n",
    "            self.correct_head_loss, self.val_correct_head_loss, self.correct_head_acc_co, self.val_correct_head_acc_co,\n",
    "            self.answer_head_loss, self.val_answer_head_loss, self.answer_head_acc_an, self.val_answer_head_acc_an,\n",
    "            self.lr, self.qushare, self.coshare, self.anshare\n",
    "            ) = load(from_path)\n",
    "        \n",
    "#     def on_epoch_begin(self, epoch, logs={}):\n",
    "#         keys = list(logs.keys())\n",
    "#         print(\"Start epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        keys = list(logs.keys())\n",
    "        values = list(logs.values())\n",
    "        \n",
    "#         print(keys)\n",
    "        \n",
    "        curr_epoch = len(self.loss)\n",
    "        self.epoch.append(curr_epoch)\n",
    "        \n",
    "        self.loss.append(logs['loss'])\n",
    "        self.val_loss.append(logs['val_loss'])\n",
    "        \n",
    "        \n",
    "        self.question_head_loss.append(logs['question_head_loss'])\n",
    "        self.val_question_head_loss.append(logs['val_question_head_loss'])\n",
    "        self.question_head_acc_qu.append(logs['question_head_acc_qu'])\n",
    "        self.val_question_head_acc_qu.append(logs['val_question_head_acc_qu'])\n",
    "        \n",
    "        self.correct_head_loss.append(logs['correct_head_loss'])\n",
    "        self.val_correct_head_loss.append(logs['val_correct_head_loss'])\n",
    "        self.correct_head_acc_co.append(logs['correct_head_acc_co'])\n",
    "        self.val_correct_head_acc_co.append(logs['val_correct_head_acc_co'])\n",
    "        \n",
    "        self.answer_head_loss.append(logs['answer_head_loss'])\n",
    "        self.val_answer_head_loss.append(logs['val_answer_head_loss'])\n",
    "        self.answer_head_acc_an.append(logs['answer_head_acc_an'])\n",
    "        self.val_answer_head_acc_an.append(logs['val_answer_head_acc_an'])\n",
    "        \n",
    "        ## Roc auc calculation on test set\n",
    "        x_val, y_val = self.validation[0], self.validation[1]\n",
    "        pred = self.model.predict(x_val, verbose = 0)\n",
    "        \n",
    "        ## Two computation of roc_auc : on whole sequence and on last element       \n",
    "        ## Whole sequence \n",
    "        y_true = y_val[2]\n",
    "        y_pred = pred[2][:,:,2]\n",
    "        \n",
    "        y_pred = y_pred.reshape(y_true.shape[0] * y_true.shape[1])\n",
    "        y_true = y_true.reshape(y_true.shape[0] * y_true.shape[1])\n",
    "        \n",
    "        y_pred = y_pred[y_true != 0]\n",
    "        y_true = y_true[y_true != 0] - 1\n",
    "        \n",
    "        roc_auc_seq = roc_auc_score(y_true, y_pred)\n",
    "        \n",
    "        ## Last element\n",
    "        x = x_val[0]\n",
    "        \n",
    "        y_true = y_val[2]\n",
    "        y_pred = pred[2][:,:,2]\n",
    "        \n",
    "        y_t = np.zeros(len(x))\n",
    "        y_p = np.zeros(len(x))\n",
    "        \n",
    "        for i, elt in enumerate(x):\n",
    "            y_t[i] = y_true[i][elt != 1][-1]\n",
    "            y_p[i] = y_pred[i][elt != 1][-1]\n",
    "        \n",
    "        y_p = y_p[y_t != 0]\n",
    "        y_t = y_t[y_t != 0] - 1\n",
    "        \n",
    "        roc_auc_val = roc_auc_score(y_t, y_p)\n",
    "        acc_val = accuracy_score(y_t, (y_p >= 0.5)*1)\n",
    "        \n",
    "        self.roc_auc.append(roc_auc_seq)\n",
    "        self.val_roc_auc.append(roc_auc_val)\n",
    "        self.accuracy.append(acc_val)\n",
    "        \n",
    "        logs['roc_auc_seq'] = roc_auc_seq\n",
    "        logs['roc_auc_val'] = roc_auc_val\n",
    "        logs['accuracy_val'] = acc_val\n",
    "        \n",
    "        self.lr.append(self.dico_params['lr'])\n",
    "        self.qushare.append(self.dico_params['qu'])\n",
    "        self.coshare.append(self.dico_params['co'])\n",
    "        self.anshare.append(self.dico_params['an'])        \n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        print(logs)\n",
    "        n_rows = 5\n",
    "        n_cols = 2\n",
    "        fig, ax = plt.subplots(n_rows, n_cols, \n",
    "                        gridspec_kw={'hspace': 0.3, 'wspace': 0.2}, figsize = (20,n_rows*7))\n",
    "        \n",
    "        #General\n",
    "        ax[0,0].plot(self.epoch, self.loss, label = 'loss')\n",
    "        ax[0,0].plot(self.epoch, self.val_loss, label = 'val_loss')\n",
    "        ax[0,0].set_title('losses')\n",
    "        ax[0,0].legend()\n",
    "        \n",
    "        ax[0,1].plot(self.epoch, self.roc_auc, label = 'roc_auc')\n",
    "        ax[0,1].plot(self.epoch, self.val_roc_auc, label = 'val_roc_auc')\n",
    "        ax[0,1].plot(self.epoch, self.accuracy, label = 'accuracy')\n",
    "        ax[0,1].set_title('roc_auc and last elt accuracy')\n",
    "        ax[0,1].legend()\n",
    "        \n",
    "        \n",
    "        ## Question target\n",
    "        ax[1,0].plot(self.epoch, self.question_head_loss, label = 'question_loss')\n",
    "        ax[1,0].plot(self.epoch, self.val_question_head_loss, label = 'val_question_loss')\n",
    "        ax[1,0].set_title('question_loss')\n",
    "        ax[1,0].legend()\n",
    "        \n",
    "        ax[1,1].plot(self.epoch, self.question_head_acc_qu, label = 'question_accuracy')\n",
    "        ax[1,1].plot(self.epoch, self.val_question_head_acc_qu, label = 'val_question_accuracy')\n",
    "        ax[1,1].set_title('question_accuracy')\n",
    "        ax[1,1].legend()\n",
    "        \n",
    "        ## Correct classif target\n",
    "        ax[2,0].plot(self.epoch, self.correct_head_loss, label = 'correct_loss')\n",
    "        ax[2,0].plot(self.epoch, self.val_correct_head_loss, label = 'val_correct_loss')\n",
    "        ax[2,0].set_title('correct_loss')\n",
    "        ax[2,0].legend()\n",
    "        \n",
    "        ax[2,1].plot(self.epoch, self.correct_head_acc_co, label = 'correct_accuracy')\n",
    "        ax[2,1].plot(self.epoch, self.val_correct_head_acc_co, label = 'val_correct_accuracy')\n",
    "        ax[2,1].set_title('correct_accuracy')\n",
    "        ax[2,1].legend()\n",
    "        \n",
    "        ## Answer target\n",
    "        ax[3,0].plot(self.epoch, self.answer_head_loss, label = 'answer_loss')\n",
    "        ax[3,0].plot(self.epoch, self.val_answer_head_loss, label = 'val_answer_loss')\n",
    "        ax[3,0].set_title('answer_loss')\n",
    "        ax[3,0].legend()\n",
    "        \n",
    "        ax[3,1].plot(self.epoch, self.answer_head_acc_an, label = 'answer_accuracy')\n",
    "        ax[3,1].plot(self.epoch, self.val_answer_head_acc_an, label = 'val_answer_accuracy')\n",
    "        ax[3,1].set_title('answer_accuracy')\n",
    "        ax[3,1].legend()\n",
    "        \n",
    "        ## Lr et objective split\n",
    "        ax[4,0].plot(self.epoch, self.lr, label = 'learning_rate')\n",
    "        ax[4,0].set_title('learning_rate')\n",
    "        ax[4,0].legend()\n",
    "        \n",
    "#         ax[4,1].plot(self.epoch, self.clshare, label = 'classification ratio')\n",
    "        ax[4,1].plot(self.epoch, self.qushare, label = 'question ratio')\n",
    "        ax[4,1].plot(self.epoch, self.coshare, label = 'correct ratio')\n",
    "        ax[4,1].plot(self.epoch, self.anshare, label = 'answer ratio')\n",
    "        ax[4,1].set_title('ratio of training objective')\n",
    "        ax[4,1].legend()\n",
    " \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        params = (self.epoch,\n",
    "        self.loss, self.val_loss, self.roc_auc, self.val_roc_auc, self.accuracy,\n",
    "            self.question_head_loss, self.val_question_head_loss, self.question_head_acc_qu, self.val_question_head_acc_qu,\n",
    "            self.correct_head_loss, self.val_correct_head_loss, self.correct_head_acc_co, self.val_correct_head_acc_co,\n",
    "            self.answer_head_loss, self.val_answer_head_loss, self.answer_head_acc_an, self.val_answer_head_acc_an,\n",
    "            self.lr, self.qushare, self.coshare, self.anshare\n",
    "        )\n",
    "        save(params, 'history_epoch_'+str(curr_epoch), 'history_sb++')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "# losses = [loss_cl, loss_qu, loss_co, loss_an]\n",
    "lr = 3e-5\n",
    "qu = 0.33\n",
    "an = 0.33\n",
    "co = 0.33\n",
    "\n",
    "dico_params = {\n",
    "    'lr':lr,\n",
    "    'qu': qu,\n",
    "    'an':an,\n",
    "    'co':co\n",
    "}\n",
    "\n",
    "\n",
    "losses = { \"question_head\": loss_qu, 'answer_head': loss_an, 'correct_head': loss_co}\n",
    "\n",
    "lossWeights = {\"question_head\": qu, 'answer_head': an, 'correct_head': co}\n",
    "\n",
    "metrics = { \"question_head\": acc_qu, 'answer_head': acc_an, 'correct_head': acc_co}\n",
    "\n",
    "loss_classif     =  losses # find the right loss for multi-class classification\n",
    "optimizer        = Adam(lr, 1e-8) # find the right optimizer\n",
    "metrics_classif  =  []\n",
    "\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics,\n",
    "             loss_weights=lossWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = DataGenerator(batch_size=32, max_len = max_len, folder = 'user_batch_saint_100', strategy = 'end', mask_rate = 0.15, seq_mask_rate = 1, bidirectionnal = False)\n",
    "test_gen = DataGenerator(batch_size=512, max_len = max_len, folder = 'user_batch_saint_test', strategy = 'end', mask_rate = 0.15, seq_mask_rate = 1, bidirectionnal = False)\n",
    "x_test, y_test = test_gen[0]\n",
    "# x_train, y_train = train_gen[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "history = CustomCallback(validation = (x_test,  y_test), dico_params = dico_params)#, from_path = './history_sb++/history_epoch_44')\n",
    "\n",
    "callbacks = [history]\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "n_epochs = 500\n",
    "steps_per_epoch = 200\n",
    "\n",
    "model.fit(train_gen, epochs=n_epochs,\n",
    "                    steps_per_epoch = steps_per_epoch, \n",
    "                    validation_data=(x_test,  y_test), \n",
    "                    max_queue_size=20,\n",
    "#                     workers=6,\n",
    "                    callbacks = callbacks,\n",
    "                    verbose = 1\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('./weights_saint/saintgpt_8l_36_question_6.76_correct_70.75_auc_75.72.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x_test, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10\n",
    "M = 20\n",
    "\n",
    "y_pred = pred[2][:,m:M,2]\n",
    "true = y_test[2][:,m:M]\n",
    "\n",
    "y_pred = y_pred[true != 0]\n",
    "true = true[true != 0] - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(true, (y_pred >= 0.5)*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing embedding with gbdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = DataGenerator(batch_size=512, max_len = max_len, folder = 'user_batch_saint_test', strategy = 'end', mask_rate = 0.15, seq_mask_rate = 1, bidirectionnal = False)\n",
    "x_val, y_val = test_gen[0]\n",
    "\n",
    "# test_gen = DataGenerator(batch_size=64, max_len = max_len, folder = 'user_batch_saint_100', strategy = 'end', mask_rate = 0.15, seq_mask_rate = 1, bidirectionnal = False)\n",
    "# x_val1, y_val1 = test_gen[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = DataGenerator(batch_size=64, max_len = max_len, folder = 'user_batch_saint_100', strategy = 'end', mask_rate = 0.15, seq_mask_rate = 1, bidirectionnal = False)\n",
    "emb1 = None\n",
    "y_val1 = None\n",
    "for i in tqdm(range(100)):\n",
    "    x_val1, y_val2 = test_gen[0]\n",
    "    y_val2 = y_val2[2]\n",
    "    emb2 = model1.predict(x_val1)\n",
    "    if emb1 is not None:\n",
    "        emb1 = np.concatenate([emb1, emb2])\n",
    "        y_val1 = np.concatenate([y_val1, y_val2])\n",
    "    else:\n",
    "        emb1 = deepcopy(emb2)\n",
    "        y_val1 = deepcopy(y_val2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = model.inputs\n",
    "out = model.get_layer('saint_bert').output\n",
    "model1 = Model(inp, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = model1.predict(x_val, verbose = 1)\n",
    "# emb1 = model1.predict(x_val1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt =  []\n",
    "yt = []\n",
    "\n",
    "for i, elt in enumerate(tqdm(x_val[0])):\n",
    "    for j, ids in enumerate(elt):\n",
    "        if ids != 1:\n",
    "            if y_val[2][i,j] > 0:\n",
    "                Xt.append(emb[i,j,:])\n",
    "                yt.append(y_val[2][i, j] - 1)\n",
    "                        \n",
    "Xt = np.array(Xt)\n",
    "yt = np.array(yt)\n",
    "\n",
    "Xv = []\n",
    "yv = []\n",
    "\n",
    "for i, elt in enumerate(tqdm(emb1)):\n",
    "    for j, ids in enumerate(elt):\n",
    "        if y_val1[i,j] > 0:\n",
    "                Xv.append(emb1[i,j,:])\n",
    "                yv.append(y_val1[i, j] - 1)\n",
    "\n",
    "Xv = np.array(Xv)\n",
    "yv = np.array(yv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids_to_keep = np.random.choice(list(range(len(Xv))), size = 200000)\n",
    "# Xv = Xv[ids_to_keep]\n",
    "# yv = yv[ids_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xv, yv, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "clf = lgb.LGBMClassifier(max_depth = -1, n_estimators = 400, n_jobs = 12, silent = False)\n",
    "clf.fit(X_train, y_train, eval_set =(X_test, y_test), eval_metric = 'auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(yt, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict_proba(Xt)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(yt, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(clf, 'lightgbm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeDataGenerator:\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        self.data will be a dictionnary to iterate over the stored data\n",
    "        self.all_rows will be the rows of the train set that are used by the generato\n",
    "        self.data_index will be all the data available in the dataset        \n",
    "        '''\n",
    "        self.data = None\n",
    "        self.all_rows = None\n",
    "        self.data_index = None\n",
    "        return None\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        sub = sample[['row_id', 'group_num']].copy()\n",
    "        sub['answered_correctly'] = np.zeros(sub.shape[0])+0.5\n",
    "        return (sample, sub)\n",
    "    \n",
    "    \n",
    "    def load(self, save_name):\n",
    "        self.data,self.all_rows = load(save_name)\n",
    "        self.data_index = np.array(list(self.data.keys()))\n",
    "    \n",
    "    def build_from_train(self, train, n_users, beginner_rate = 0.3, save_name = 'fake_train_generator'):\n",
    "        \"\"\"\n",
    "        train will be the training set you loaded\n",
    "        n_users is a number of user from whom you will sample the data\n",
    "        beginner_rate is the rate of these users who will begin their journey during test\n",
    "        save_name : the name under which the item will be saved\n",
    "        \"\"\"\n",
    "        \n",
    "        ## Sampling a restricted list of users\n",
    "        user_list = train['user_id'].unique()\n",
    "        test_user_list = np.random.choice(user_list, size = n_users)\n",
    "        train.index = train['user_id']\n",
    "        test_data_non_filter = train.loc[test_user_list]\n",
    "        test_data_non_filter.index = list(range(test_data_non_filter.shape[0]))\n",
    "        \n",
    "        ## building a dictionnary with all the rows and container id from a user\n",
    "        dico_user = {}\n",
    "        def agg(x):\n",
    "            return [elt for elt in x]\n",
    "        \n",
    "        print(\"Generating user dictionnary\")\n",
    "        for user, frame in tqdm(test_data_non_filter.groupby('user_id'), total =test_data_non_filter['user_id'].nunique()):\n",
    "            if frame.shape[0] > 0:\n",
    "                dico_user[user] = {}\n",
    "\n",
    "                dico_user[user]['min_indice'] = frame['task_container_id'].min()\n",
    "                dico_user[user]['max_indice'] = frame['task_container_id'].max()\n",
    "\n",
    "                r = random.uniform(0,1)\n",
    "                if r < beginner_rate:\n",
    "                    dico_user[user]['current_indice'] = dico_user[user]['min_indice']\n",
    "                else:\n",
    "                    dico_user[user]['current_indice'] = random.randint(dico_user[user]['min_indice'],dico_user[user]['max_indice']-2)\n",
    "\n",
    "                row_ids = frame[['task_container_id','row_id']].groupby('task_container_id').agg(agg)\n",
    "                row_ids = row_ids.to_dict()['row_id']\n",
    "                dico_user[user]['row_ids'] = row_ids\n",
    "\n",
    "        work_dico = deepcopy(dico_user)\n",
    "        \n",
    "        ## Choosing batch_data to generate\n",
    "        work_dico = deepcopy(dico_user)\n",
    "        batches = {}\n",
    "\n",
    "        all_rows = []\n",
    "        batch_number = 0\n",
    "        \n",
    "        print('Creating batches')\n",
    "        while len(work_dico)> 1:\n",
    "\n",
    "            size = random.randint(20,500)\n",
    "            size = min(size, len(work_dico))\n",
    "\n",
    "\n",
    "            batch = []\n",
    "\n",
    "            users = np.random.choice(np.array(list(work_dico.keys())),replace = False,  size = size)\n",
    "\n",
    "            for u in users:\n",
    "                try:\n",
    "                    batch.extend(work_dico[u]['row_ids'][work_dico[u]['current_indice']])\n",
    "                    all_rows.extend(work_dico[u]['row_ids'][work_dico[u]['current_indice']])\n",
    "                    work_dico[u]['current_indice'] += 1\n",
    "                    if work_dico[u]['current_indice'] == work_dico[u]['max_indice']:\n",
    "                        work_dico.pop(u)\n",
    "                except:\n",
    "                    work_dico.pop(u)\n",
    "\n",
    "            batches[batch_number] = batch\n",
    "            batch_number += 1\n",
    "        \n",
    "        ## building data\n",
    "\n",
    "        data = {}\n",
    "        \n",
    "        print(\"Building dataset\")\n",
    "        test_data_non_filter.index = test_data_non_filter['row_id']\n",
    "        for i in tqdm(batches):\n",
    "            current_data = test_data_non_filter.loc[np.array(batches[i])]\n",
    "            current_data['group_num'] = i\n",
    "\n",
    "            current_data['prior_group_answers_correct'] = [np.nan for elt in range(current_data.shape[0])]\n",
    "            current_data['prior_group_responses'] = [np.nan for elt in range(current_data.shape[0])]\n",
    "\n",
    "            if i != 0:\n",
    "                current_data['prior_group_answers_correct'].iloc[0] = saved_correct_answer\n",
    "                current_data['prior_group_responses'].iloc[0] = saved_answer\n",
    "\n",
    "            saved_answer = str(list(current_data[current_data['content_type_id'] == 0]['user_answer'].values))\n",
    "            saved_correct_answer = str(list(current_data[current_data['content_type_id'] == 0]['answered_correctly'].values))\n",
    "            current_data = current_data.drop(columns = ['user_answer', 'answered_correctly'])\n",
    "\n",
    "            data[i] = current_data\n",
    "\n",
    "        save((data,np.array(all_rows)) , save_name)\n",
    "        \n",
    "        self.data = data\n",
    "        self.all_rows = np.array(all_rows)\n",
    "        self.data_index = np.array(list(data.keys()))\n",
    "        print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FakeDataGenerator()\n",
    "# env.build_from_train(train, 15000, beginner_rate = 0.3, save_name = 'fake_train_generator')\n",
    "env.load('fake_train_generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, sub = env[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = load('train_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.index = train['user_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create():\n",
    "    return {\n",
    "                'exercise_id' : np.array([]),\n",
    "                'container_id' : np.array([]),\n",
    "                'timestamp' : np.array([]),\n",
    "                'correctness' : np.array([]),\n",
    "                'answer' : np.array([]), \n",
    "                'elapsed_time' : np.array([]),\n",
    "                'prior_question_had_explanation' : np.array([]),\n",
    "                'lag_time' : np.array([]),\n",
    "                'first_line' : True\n",
    "            }\n",
    "\n",
    "def update_data(data, data_sav, sub_sav):\n",
    "    prior_correct = data['prior_group_answers_correct'].iloc[0]\n",
    "    prior_answer = data['prior_group_responses'].iloc[0]\n",
    "\n",
    "    prior_correct = np.array(prior_correct.replace('[', '').replace(']', '').split(', ')).astype(int)\n",
    "    prior_answer = np.array(prior_answer.replace('[', '').replace(']', '').split(', ')).astype(int)\n",
    "\n",
    "    corr = np.zeros(data_sav.shape[0]) - 1 \n",
    "    ans = np.zeros(data_sav.shape[0]) - 1\n",
    "\n",
    "    boole = data_sav['content_type_id'].values == 0\n",
    "\n",
    "    corr[boole] = prior_correct\n",
    "    ans[boole] = prior_answer\n",
    "\n",
    "    data_sav['answered_correctly'] = corr\n",
    "    data_sav['user_answer'] = ans\n",
    "    sub_sav['answered_correctly_truth'] = corr\n",
    "    \n",
    "    sub_sav = sub_sav[boole]\n",
    "    \n",
    "    return data_sav, sub_sav\n",
    "\n",
    "def update_dico_user(dico_user, data_sav):\n",
    "    data_sav = data_sav.sort_values(by = ['timestamp'])\n",
    "    for i, line in data_sav.iterrows():\n",
    "        user = line['user_id']\n",
    "        exid = line['content_id']\n",
    "        tid = line['content_type_id']\n",
    "        exid = 'q_'+str(exid) if tid == 0 else 'l_' + str(exid)\n",
    "\n",
    "        dico_user[user]['exercise_id'] = np.concatenate([dico_user[user]['exercise_id'], [exid]])\n",
    "        dico_user[user]['container_id'] = np.concatenate([dico_user[user]['container_id'], [line['task_container_id']]])\n",
    "        dico_user[user]['timestamp'] = np.concatenate([dico_user[user]['timestamp'], [line['timestamp']/1000]])\n",
    "        dico_user[user]['correctness'] = np.concatenate([dico_user[user]['correctness'], [line['answered_correctly']]])\n",
    "        dico_user[user]['answer'] = np.concatenate([dico_user[user]['answer'], [line['user_answer']]])\n",
    "\n",
    "        ## two other depend on if this is the first line\n",
    "        if dico_user[user]['first_line']:\n",
    "            dico_user[user]['first_line'] = False\n",
    "            dico_user[user]['lag_time'] = np.concatenate([dico_user[user]['lag_time'], [0]])\n",
    "\n",
    "        else:\n",
    "            el = line['prior_question_elapsed_time']\n",
    "            if str(el) == 'nan':\n",
    "                el = 0\n",
    "            dico_user[user]['elapsed_time'] = np.concatenate([dico_user[user]['elapsed_time'], [el]])\n",
    "\n",
    "            pr = line['prior_question_had_explanation']\n",
    "            if str(pr) == 'nan':\n",
    "                pr = 0\n",
    "            pr = pr*1\n",
    "            dico_user[user]['prior_question_had_explanation'] = np.concatenate([dico_user[user]['prior_question_had_explanation'], [pr]])\n",
    "\n",
    "            lag = dico_user[user]['timestamp'][-1] - dico_user[user]['timestamp'][-2] + el\n",
    "            if lag < 0:\n",
    "                lag = 0\n",
    "            dico_user[user]['lag_time'] = np.concatenate([dico_user[user]['lag_time'], [lag]])\n",
    "    return dico_user\n",
    "\n",
    "\n",
    "dico_question = load('dico_questions_mean')\n",
    "dico_utags, dico_gtags, dico_parts = load('dico_tags')\n",
    "timestamp_enc, elapsed_enc,lag_time_enc, qmean_enc = load('discrete_encoders')\n",
    "tokenizer = load('tokenizer')\n",
    "\n",
    "def map_part( ids):\n",
    "    def replace_dico_part(x):\n",
    "        try:\n",
    "            return dico_parts[x]\n",
    "        except:\n",
    "            return 0\n",
    "    return np.array(list(map(replace_dico_part,ids)))\n",
    "\n",
    "def map_utags( ids):\n",
    "    def replace_dico_utags(x):\n",
    "        try:\n",
    "            if str(dico_utags[x]) != 'nan':\n",
    "                return str(self.dico_utags[x])\n",
    "            else:\n",
    "                return 0\n",
    "        except:\n",
    "            return 0\n",
    "    return np.array(list(map(replace_dico_utags,ids)))\n",
    "\n",
    "def map_gtags( ids):\n",
    "    def replace_dico_gtags(x):\n",
    "        try:\n",
    "            if str(dico_gtags[x]) != 'nan':\n",
    "                return str(dico_gtags[x])\n",
    "            else:\n",
    "                return 0\n",
    "        except:\n",
    "            return 0\n",
    "    return np.array(list(map(replace_dico_gtags,ids)))\n",
    "\n",
    "def map_mean(ids):\n",
    "    def replace_dico_question(x):\n",
    "        try:\n",
    "            return dico_question[x]\n",
    "        except:\n",
    "            return 0.5\n",
    "    return np.array(list(map(replace_dico_question,ids)))\n",
    "\n",
    "def remove_na(x):\n",
    "    x = np.array(list(x))\n",
    "    x[np.isnan(x)] = 0\n",
    "    return x\n",
    "\n",
    "def build_sequence(user_history, new_inputs, max_len = 128):\n",
    "    ## new input : (exercise_id, timestamp, elapsed)\n",
    "    \n",
    "    dico_sequence = deepcopy(user_history)        \n",
    "    dico_sequence['elapsed_time'] = remove_na(dico_sequence['elapsed_time'])\n",
    "    dico_sequence['lag_time'] = remove_na(dico_sequence['lag_time'])\n",
    "    dico_sequence['prior_question_had_explanation'] = remove_na(dico_sequence['prior_question_had_explanation'])\n",
    "\n",
    "    dico_sequence['elapsed_time'] = np.concatenate([dico_sequence['elapsed_time'], [0]])\n",
    "    dico_sequence['prior_question_had_explanation'] = np.concatenate([dico_sequence['prior_question_had_explanation'], [0]])\n",
    "\n",
    "\n",
    "    ## Cut sequence\n",
    "    for elt in dico_sequence:\n",
    "        if elt != 'first_line':\n",
    "            dico_sequence[elt] = dico_sequence[elt][-(max_len-1):]\n",
    "        \n",
    "    ## Adding new elements\n",
    "    dico_sequence['exercise_id'] = np.concatenate([dico_sequence['exercise_id'], [new_inputs[0]]])\n",
    "    dico_sequence['timestamp'] = np.concatenate([dico_sequence['timestamp'], [new_inputs[1]]])\n",
    "    try:\n",
    "        lag = dico_sequence['timestamp'][-1] - dico_sequence['timestamp'][-2] + new_inputs[2]\n",
    "    except:\n",
    "        lag = 0\n",
    "    if lag < 0:\n",
    "        lag = 0\n",
    "    dico_sequence['lag_time'] = np.concatenate([dico_sequence['lag_time'], [lag]])\n",
    "    query_id = len(dico_sequence['exercise_id']) - 1\n",
    "    \n",
    "    ## Pad sequence\n",
    "    pad_tokens = ['[PAD]', 0, 0, -1, -1, 0, 0, 0, '[PAD]', -1, -1]\n",
    "    for j, elt in enumerate(dico_sequence):\n",
    "        if elt != 'first_line':\n",
    "            size = len(dico_sequence[elt])\n",
    "            if size <= max_len:\n",
    "                adding = max_len - size\n",
    "                tok = pad_tokens[j]\n",
    "                if type(tok) == str:\n",
    "                    add = np.array([tok for elt in range(adding)])\n",
    "                else:\n",
    "                    add = np.zeros(adding) + tok\n",
    "                dico_sequence[elt] = np.concatenate([dico_sequence[elt], add], axis = 0)\n",
    "#                 print(dico_sequence[elt].shape)\n",
    "    lags =  lag_time_enc.transform(dico_sequence['lag_time'])\n",
    "    lags[lags<0] = 0\n",
    "    \n",
    "    input_vals = [\n",
    "        dico_sequence['exercise_id'],\n",
    "        map_part(dico_sequence['exercise_id']),\n",
    "        map_utags(dico_sequence['exercise_id']),\n",
    "        map_gtags(dico_sequence['exercise_id']),\n",
    "        timestamp_enc.transform(dico_sequence['timestamp']),\n",
    "        qmean_enc.transform(map_mean(dico_sequence['exercise_id'])),\n",
    "\n",
    "        np.concatenate([[0], dico_sequence['correctness'] + 1])[:-1],\n",
    "        np.concatenate([[0], dico_sequence['answer'] + 1])[:-1],\n",
    "        np.concatenate([[0], elapsed_enc.transform(dico_sequence['elapsed_time'])])[:-1],\n",
    "        lags,\n",
    "        np.concatenate([[0], dico_sequence['prior_question_had_explanation']])[:-1],\n",
    "    ]\n",
    "    return input_vals, query_id\n",
    "\n",
    "def initiate_dico(batch_size, max_len = 128):\n",
    "    list_encoder = ['exercise', 'part', 'utag', 'gtag', 'timestamp', 'question_mean']\n",
    "    list_decoder = ['correct', 'answer', 'elapsed_time', 'lag_time', 'was_explained']\n",
    "    list_output = ['exercise', 'answer', 'correct']\n",
    "\n",
    "    dico_input = {}\n",
    "    for elt in list_encoder + list_decoder:\n",
    "        if elt == 'exercise':\n",
    "            dico_input[elt] = np.zeros((batch_size, max_len)).astype(str)\n",
    "        else:\n",
    "            dico_input[elt] = np.zeros((batch_size, max_len)).astype('int32')\n",
    "    return dico_input\n",
    "\n",
    "def update_dico(dico_input, input_vals, i):\n",
    "    list_encoder = ['exercise', 'part', 'utag', 'gtag', 'timestamp', 'question_mean']\n",
    "    list_decoder = ['correct', 'answer', 'elapsed_time', 'lag_time', 'was_explained']\n",
    "    list_output = ['exercise', 'answer', 'correct']\n",
    "\n",
    "    for j, elt in enumerate(list_encoder + list_decoder):\n",
    "        dico_input[elt][i] = input_vals[j]\n",
    "    return dico_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dico_user = {}\n",
    "data_sav = None\n",
    "count = 0\n",
    "\n",
    "all_sub = None\n",
    "\n",
    "for i in tqdm(env.data_index):\n",
    "    data, sub = env[i]\n",
    "    \n",
    "    for elt in data['user_id'].unique():\n",
    "        # Loading every piece of information available from past\n",
    "        if not(elt in dico_user):\n",
    "            try:\n",
    "#                 print(elt)\n",
    "                dico_user[elt] = load(str(elt), 'ind_user')\n",
    "                dico_user[elt]['first_line'] = False\n",
    "            except:\n",
    "                dico_user[elt] = create()\n",
    "        \n",
    "    ## Updating data_sav with the new informations\n",
    "    if count != 0:\n",
    "        ## Include values in the mix\n",
    "        data_sav, sub_sav = update_data(data, data_sav, sub_sav)\n",
    "        \n",
    "        if all_sub is not None:\n",
    "            all_sub = pd.concat([all_sub, sub_sav])\n",
    "        else:\n",
    "            all_sub = sub_sav.copy()\n",
    "        print(roc_auc_score(all_sub['answered_correctly_truth'], all_sub['answered_correctly']))\n",
    "        \n",
    "        ## Update dictionnary with data of previous batch\n",
    "        dico_user = update_dico_user(dico_user, data_sav)\n",
    "    \n",
    "    ## Build input for the deep learning model\n",
    "    dico_input = initiate_dico(data.shape[0])\n",
    "    \n",
    "    data.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    query_ids = []\n",
    "    for i, line in data.iterrows():\n",
    "        user = line['user_id']\n",
    "        exid = line['content_id']\n",
    "        tid = line['content_type_id']\n",
    "        exid = 'q_'+str(exid) if tid == 0 else 'l_' + str(exid)\n",
    "        \n",
    "        t = line['timestamp'] / 1000\n",
    "        el = line['prior_question_elapsed_time'] / 1000\n",
    "        \n",
    "        input_vals, query_id = build_sequence(dico_user[user], (exid, t, el))\n",
    "        query_ids.append(query_id)\n",
    "        dico_input = update_dico(dico_input, input_vals, i)\n",
    "    \n",
    "    x = deepcopy(dico_input['exercise'])\n",
    "    dico_input['exercise'] = np.array(tokenizer.texts_to_sequences([\" \".join(list(x)[elt]) for elt in range(len(x))]))\n",
    "    \n",
    "    X = list(np.array(list(dico_input.values())).astype('int32'))\n",
    "    \n",
    "    ## Lgbm variant\n",
    "    predicted = model1.predict(X)\n",
    "    X1 = []\n",
    "    \n",
    "    print(query_ids)\n",
    "    \n",
    "    for i, j in enumerate(query_ids):\n",
    "        X1.append(predicted[i,j,:])\n",
    "    X1 = np.array(X1)\n",
    "    p = clf.predict_proba(X1)[:,1]\n",
    "    \n",
    "    ## Deep Variant\n",
    "#     p1 = model.predict(X)[2][:,:,2]\n",
    "    \n",
    "#     p = []\n",
    "#     for i, j in enumerate(query_ids):\n",
    "#         p.append(p1[i,j])\n",
    "    \n",
    "    sub['answered_correctly'] = p\n",
    "                \n",
    "    data_sav = data.copy()\n",
    "    sub_sav = sub.copy()\n",
    "    count += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self,batch_size=32, max_len = 128, folder = 'user_batch_saint_100', strategy = 'begin', mask_rate = 0.15, seq_mask_rate = 0.5, bidirectionnal = True):\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = load('tokenizer')\n",
    "        self.max_len = max_len\n",
    "        self.folder = folder\n",
    "        self.dico_question = load('dico_questions_mean')\n",
    "        self.dico_utags, self.dico_gtags, self.dico_parts = load('dico_tags')\n",
    "        self.timestamp_enc, self.elapsed_enc,self.lag_time_enc, self.qmean_enc = load('discrete_encoders')\n",
    "        self.strategy = strategy\n",
    "        self.mask_rate = mask_rate\n",
    "        self.seq_mask_rate = seq_mask_rate\n",
    "        self.bidirectionnal = bidirectionnal\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 1000000\n",
    "    \n",
    "    def initiate_dico(self):\n",
    "        list_encoder = ['exercise', 'part', 'utag', 'gtag', 'timestamp', 'question_mean']\n",
    "        list_decoder = ['correct', 'answer', 'elapsed_time', 'lag_time', 'was_explained']\n",
    "        list_output = ['exercise', 'answer', 'correct']\n",
    "        \n",
    "        dico_input = {}\n",
    "        for elt in list_encoder + list_decoder:\n",
    "            if elt == 'exercise':\n",
    "                dico_input[elt] = np.zeros((self.batch_size, self.max_len)).astype(str)\n",
    "            else:\n",
    "                dico_input[elt] = np.zeros((self.batch_size, self.max_len)).astype('int32')\n",
    "        \n",
    "        dico_output = {}\n",
    "        for elt in list_output:\n",
    "            if elt == 'exercise':\n",
    "                dico_output[elt] = np.zeros((self.batch_size, self.max_len)).astype(str)\n",
    "            else:\n",
    "                dico_output[elt] = np.zeros((self.batch_size, self.max_len)).astype('int32')\n",
    "        return dico_input, dico_output\n",
    "\n",
    "    def map_part(self, ids):\n",
    "        def replace_dico_part(x):\n",
    "            try:\n",
    "                return self.dico_parts[x]\n",
    "            except:\n",
    "                return 0\n",
    "        return np.array(list(map(replace_dico_part,ids)))\n",
    "    \n",
    "    def map_utags(self, ids):\n",
    "        def replace_dico_utags(x):\n",
    "            try:\n",
    "                if str(self.dico_utags[x]) != 'nan':\n",
    "                    return str(self.dico_utags[x])\n",
    "                else:\n",
    "                    return 0\n",
    "            except:\n",
    "                return 0\n",
    "        return np.array(list(map(replace_dico_utags,ids)))\n",
    "    \n",
    "    def map_gtags(self, ids):\n",
    "        def replace_dico_gtags(x):\n",
    "            try:\n",
    "                if str(self.dico_gtags[x]) != 'nan':\n",
    "                    return str(self.dico_gtags[x])\n",
    "                else:\n",
    "                    return 0\n",
    "            except:\n",
    "                return 0\n",
    "        return np.array(list(map(replace_dico_gtags,ids)))\n",
    "    \n",
    "    def map_mean(self, ids):\n",
    "        def replace_dico_question(x):\n",
    "            try:\n",
    "                return self.dico_question[x]\n",
    "            except:\n",
    "                return 0.5\n",
    "        return np.array(list(map(replace_dico_question,ids)))\n",
    "\n",
    "\n",
    "    \n",
    "    def update_dico(self, dico_input, dico_output, input_vals, output_vals, i):\n",
    "        list_encoder = ['exercise', 'part', 'utag', 'gtag', 'timestamp', 'question_mean']\n",
    "        list_decoder = ['correct', 'answer', 'elapsed_time', 'lag_time', 'was_explained']\n",
    "        list_output = ['exercise', 'answer', 'correct']\n",
    "        \n",
    "        for j, elt in enumerate(list_encoder + list_decoder):\n",
    "            dico_input[elt][i] = input_vals[j]\n",
    "        \n",
    "        for j, elt in enumerate(list_output):\n",
    "            dico_output[elt][i] = output_vals[j]\n",
    "        return dico_input, dico_output\n",
    "\n",
    "    def remove_na(self, x):\n",
    "        x = np.array(list(x))\n",
    "        x[np.isnan(x)] = 0\n",
    "        return x\n",
    "    \n",
    "    def apply_mask(self, x, mask, pad_token, mask_token):\n",
    "        x_out = []\n",
    "        x_in = []\n",
    "        for i, elt in enumerate(mask):\n",
    "            if mask[i] == 1:\n",
    "                x_out.append(x[i])\n",
    "                x_in.append(mask_token)\n",
    "            else:\n",
    "                x_out.append(pad_token)\n",
    "                x_in.append(x[i])\n",
    "        return np.array(x_in), np.array(x_out)\n",
    "\n",
    "    def build_sequence(self, user_history):\n",
    "        dico_sequence = deepcopy(user_history)        \n",
    "        dico_sequence['elapsed_time'] = self.remove_na(dico_sequence['elapsed_time'])\n",
    "        dico_sequence['lag_time'] = self.remove_na(dico_sequence['lag_time'])\n",
    "        dico_sequence['prior_question_had_explanation'] = self.remove_na(dico_sequence['prior_question_had_explanation'])\n",
    "        \n",
    "        dico_sequence['elapsed_time'] = np.concatenate([dico_sequence['elapsed_time'], [0]])\n",
    "        dico_sequence['prior_question_had_explanation'] = np.concatenate([dico_sequence['prior_question_had_explanation'], [0]])\n",
    "        \n",
    "        \n",
    "        ## Cut sequence\n",
    "        if self.strategy == 'begin':\n",
    "            for elt in dico_sequence:\n",
    "                dico_sequence[elt] = dico_sequence[elt][:self.max_len]\n",
    "        else:\n",
    "            for elt in dico_sequence:\n",
    "                dico_sequence[elt] = dico_sequence[elt][-self.max_len:]\n",
    "        \n",
    "        \n",
    "        \n",
    "         ## Masking\n",
    "        # Either mask question => mask parts, qmean, answer, correctness 0.5%\n",
    "        # Or mask correctness => mask answer, elapsed_time, lag_time, explanation 0.5%\n",
    "        # In all case, mask the last question answer, but we keep its signification\n",
    "        \n",
    "        if self.bidirectionnal == True:\n",
    "            r = random.uniform(0,1)\n",
    "            if r < self.seq_mask_rate:\n",
    "                # masking on the question_id\n",
    "                masks = np.random.choice([0,1],replace = True, size = len(dico_sequence['exercise_id'])-1, p = [1-self.mask_rate,self.mask_rate])\n",
    "\n",
    "    #             print(masks.shape)\n",
    "    #             print(dico_sequence['elapsed_time'].shape)\n",
    "    #             print('\\n')\n",
    "\n",
    "                dico_sequence['elapsed_time'], _ = self.apply_mask(dico_sequence['elapsed_time'], masks, 0, 0)     \n",
    "                dico_sequence['prior_question_had_explanation'], _ = self.apply_mask(dico_sequence['prior_question_had_explanation'], masks, 0, 0) \n",
    "\n",
    "                masks = np.concatenate([masks, [0]])\n",
    "                dico_sequence['exercise_id'], dico_sequence['exercise_id_out'] = self.apply_mask(dico_sequence['exercise_id'], masks, '[PAD]', '[MASK]')            \n",
    "                masks[-1] = 1\n",
    "                dico_sequence['answer'], dico_sequence['answer_out'] = self.apply_mask(dico_sequence['answer'], masks, -1, -1)\n",
    "                dico_sequence['correctness'], dico_sequence['correctness_out'] = self.apply_mask(dico_sequence['correctness'], masks, -1, -1)\n",
    "\n",
    "            else:\n",
    "                # Masking only a part of the answers\n",
    "                dico_sequence['exercise_id_out'] = deepcopy(np.array(['[PAD]' for elt in dico_sequence['exercise_id']]))\n",
    "                masks = np.random.choice([0,1],replace = True, size = len(dico_sequence['correctness'])-1, p = [1-self.mask_rate,self.mask_rate])\n",
    "\n",
    "    #             print(masks.shape)\n",
    "    #             print(dico_sequence['elapsed_time'].shape)\n",
    "    #             print('\\n')\n",
    "\n",
    "                dico_sequence['elapsed_time'], _ = self.apply_mask(dico_sequence['elapsed_time'], masks, 0, 0)    \n",
    "                dico_sequence['prior_question_had_explanation'], _ = self.apply_mask(dico_sequence['prior_question_had_explanation'], masks, 0, 0)\n",
    "\n",
    "                masks = np.concatenate([masks, [1]])\n",
    "                dico_sequence['correctness'], dico_sequence['correctness_out'] = self.apply_mask(dico_sequence['correctness'], masks, -1, -1)\n",
    "                dico_sequence['answer'], dico_sequence['answer_out'] = self.apply_mask(dico_sequence['answer'], masks, -1, -1)\n",
    "        \n",
    "        else:\n",
    "            dico_sequence['exercise_id_out'] = dico_sequence['exercise_id']\n",
    "            dico_sequence['correctness_out'] = dico_sequence['correctness']\n",
    "            dico_sequence['answer_out'] = dico_sequence['answer']\n",
    "        \n",
    "        ## Pad sequence\n",
    "        pad_tokens = ['[PAD]', 0, 0, -1, -1, 0, 0, 0, '[PAD]', -1, -1]\n",
    "        for j, elt in enumerate(dico_sequence):\n",
    "            size = len(dico_sequence[elt])\n",
    "            if size <= self.max_len:\n",
    "                adding = self.max_len - size\n",
    "                tok = pad_tokens[j]\n",
    "                if type(tok) == str:\n",
    "                    add = np.array([tok for elt in range(adding)])\n",
    "                else:\n",
    "                    add = np.zeros(adding) + tok\n",
    "                dico_sequence[elt] = np.concatenate([dico_sequence[elt], add], axis = 0)\n",
    "#                 print(dico_sequence[elt].shape)\n",
    "\n",
    "        if self.bidirectionnal == False:\n",
    "            input_vals = [\n",
    "                dico_sequence['exercise_id'],\n",
    "                self.map_part(dico_sequence['exercise_id']),\n",
    "                self.map_utags(dico_sequence['exercise_id']),\n",
    "                self.map_gtags(dico_sequence['exercise_id']),\n",
    "                self.timestamp_enc.transform(dico_sequence['timestamp']),\n",
    "                self.qmean_enc.transform(self.map_mean(dico_sequence['exercise_id'])),\n",
    "\n",
    "                np.concatenate([[0], dico_sequence['correctness'] + 1])[:-1],\n",
    "                np.concatenate([[0], dico_sequence['answer'] + 1])[:-1],\n",
    "                np.concatenate([[0], self.elapsed_enc.transform(dico_sequence['elapsed_time'])])[:-1],\n",
    "                self.lag_time_enc.transform(dico_sequence['lag_time']),\n",
    "                np.concatenate([[0], dico_sequence['prior_question_had_explanation']])[:-1],\n",
    "            ]\n",
    "\n",
    "            output_vals = [\n",
    "                np.concatenate([dico_sequence['exercise_id_out'][1:], ['[PAD]']]),\n",
    "                dico_sequence['answer_out'] + 1,\n",
    "                dico_sequence['correctness_out'] + 1,\n",
    "            ]\n",
    "            \n",
    "        else:\n",
    "            input_vals = [\n",
    "                dico_sequence['exercise_id'],\n",
    "                self.map_part(dico_sequence['exercise_id']),\n",
    "                self.map_utags(dico_sequence['exercise_id']),\n",
    "                self.map_gtags(dico_sequence['exercise_id']),\n",
    "                self.timestamp_enc.transform(dico_sequence['timestamp']),\n",
    "                self.qmean_enc.transform(self.map_mean(dico_sequence['exercise_id'])),\n",
    "\n",
    "                dico_sequence['correctness'] + 1,\n",
    "                dico_sequence['answer'] + 1,\n",
    "                self.elapsed_enc.transform(dico_sequence['elapsed_time']),\n",
    "                self.lag_time_enc.transform(dico_sequence['lag_time']),\n",
    "                dico_sequence['prior_question_had_explanation'],\n",
    "            ]\n",
    "\n",
    "            output_vals = [\n",
    "                dico_sequence['exercise_id_out'],\n",
    "                dico_sequence['answer_out'] + 1,\n",
    "                dico_sequence['correctness_out'] + 1,\n",
    "            ]\n",
    "        \n",
    "        \n",
    "#         x = np.zeros((11,self.max_len))\n",
    "#         y = np.zeros((3, self.max_len))\n",
    "        return input_vals,output_vals\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ## Load random batch\n",
    "        file_name = random.choice(os.listdir('./'+self.folder))\n",
    "        dico_user = load(file_name.split('.')[0], self.folder)\n",
    "        \n",
    "        list_user = np.random.choice(list(dico_user.keys()), size = self.batch_size)\n",
    "        \n",
    "        dico_input, dico_output = self.initiate_dico()\n",
    "        \n",
    "        \n",
    "        for i, elt in enumerate(list_user):\n",
    "            user_history = dico_user[elt]\n",
    "            input_vals, output_vals = self.build_sequence(user_history)\n",
    "            dico_input, dico_output = self.update_dico(dico_input, dico_output, input_vals, output_vals, i)\n",
    "        \n",
    "        x = deepcopy(dico_input['exercise'])\n",
    "        dico_input['exercise'] = np.array(self.tokenizer.texts_to_sequences([\" \".join(list(x)[elt]) for elt in range(len(x))]))\n",
    "        \n",
    "        x = deepcopy(dico_output['exercise'])\n",
    "        dico_output['exercise'] = np.array(self.tokenizer.texts_to_sequences([\" \".join(list(x)[elt]) for elt in range(len(x))]))\n",
    "        \n",
    "        X = list(np.array(list(dico_input.values())).astype('int32'))\n",
    "        y = list(np.array(list(dico_output.values())).astype('int32')) \n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def __get_data(self, batch):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_sequence(df_user):\n",
    "    import numpy as np\n",
    "    df_user =  df_user.sort_values(by = 'timestamp')\n",
    "    df_user.index = list(range(df_user.shape[0]))\n",
    "    \n",
    "    df_user['content_type'] =  df_user['content_type_id'].apply(lambda x : 'q' if x == 0 else 'l')\n",
    "    df_user['content_seq'] = df_user['content_type'].astype(str) + '_' + df_user['content_id'].astype(str)\n",
    "    \n",
    "    ## Encoder\n",
    "    exercise_id = df_user['content_seq'].values\n",
    "    container_id = df_user['task_container_id'].values\n",
    "    timestamp = df_user['timestamp'].values/1000  ## Conversion in s\n",
    "    \n",
    "    ## Decoder\n",
    "    correctness = df_user['answered_correctly'].values\n",
    "    answer = df_user['user_answer'].values\n",
    "    \n",
    "    elapsed_time = df_user['prior_question_elapsed_time'].fillna(0).values[1:]/1000 ## Already Padded ## Conversion in s\n",
    "    prior_question_had_explanation = df_user['prior_question_had_explanation'].fillna(0).values[1:]*1 ## Already Padded\n",
    "    \n",
    "    lag_time = np.concatenate([[0],timestamp[1:] - timestamp[:-1] + elapsed_time])\n",
    "    \n",
    "    dico = {\n",
    "        'exercise_id' : exercise_id,\n",
    "        'container_id' : container_id,\n",
    "        'timestamp' : timestamp,\n",
    "        'correctness' : correctness,\n",
    "        'answer' : answer, \n",
    "        'elapsed_time' : elapsed_time,\n",
    "        'prior_question_had_explanation' : prior_question_had_explanation,\n",
    "        'lag_time' : lag_time\n",
    "    }\n",
    "    return dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "b = 0\n",
    "for elt in tqdm(dico_user.keys()):\n",
    "    c = dico_user[elt]\n",
    "    if len(c['exercise_id']) <= 1:\n",
    "        a += 1\n",
    "    else:\n",
    "        b+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ameliorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add context on lecture and tasks\n",
    "\n",
    "cluster lecture and tasks\n",
    "\n",
    "give average score of a given task\n",
    "\n",
    "enhance test set with train set (optimization constraint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
