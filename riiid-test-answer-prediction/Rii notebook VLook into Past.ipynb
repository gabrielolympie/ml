{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import _pickle as pickle\n",
    "import gc\n",
    "from multiprocess import Pool\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import tensorflow.keras.utils as np_utils\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "import tensorflow as tf\n",
    "def save(file,name, folder = \"\"):\n",
    "    if folder != \"\":\n",
    "        outfile = open('./'+folder+'/'+name+'.pickle', 'wb')\n",
    "    else:\n",
    "        outfile = open(name+'.pickle', 'wb')\n",
    "    pickle.dump(file, outfile, protocol=4)\n",
    "    outfile.close\n",
    "    \n",
    "def load(name, folder = \"\"):\n",
    "    if folder != \"\":\n",
    "        outfile = open('./'+folder+'/'+name+'.pickle', 'rb')\n",
    "    else:\n",
    "        outfile = open(name+'.pickle', 'rb')\n",
    "    file = pickle.load(outfile)\n",
    "    outfile.close\n",
    "    return file\n",
    "\n",
    "class Discretiser:\n",
    "    def __init__(self, nbins):\n",
    "        self.nbins = nbins-1\n",
    "        self.map_to = np.arange(self.nbins)/self.nbins\n",
    "        \n",
    "    def fit(self, X):\n",
    "        ## X is a one dimension np array\n",
    "        self.map_from = np.quantile(X, self.map_to)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X1 = (np.interp(X, self.map_from, self.map_to, left=0, right=1, period=None) * self.nbins).astype(int)\n",
    "        return X1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load('train_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user, data in tqdm(train.groupby('user_id'), total = train['user_id'].nunique()):\n",
    "    save(data, str(user), 'individual_users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user = train[train['user_id'] == 115]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_sequence(df_user):\n",
    "    import numpy as np\n",
    "    df_user =  df_user.sort_values(by = 'timestamp')\n",
    "    df_user.index = list(range(df_user.shape[0]))\n",
    "    \n",
    "    df_user['content_type'] =  df_user['content_type_id'].apply(lambda x : 'q' if x == 0 else 'l')\n",
    "    df_user['content_seq'] = df_user['content_type'].astype(str) + '_' + df_user['content_id'].astype(str)\n",
    "    \n",
    "    ## Encoder\n",
    "    exercise_id = df_user['content_seq'].values\n",
    "    container_id = df_user['task_container_id'].values\n",
    "    timestamp = df_user['timestamp'].values/1000  ## Conversion in s\n",
    "    \n",
    "    ## Decoder\n",
    "    correctness = df_user['answered_correctly'].values\n",
    "    answer = df_user['user_answer'].values\n",
    "    \n",
    "    elapsed_time = df_user['prior_question_elapsed_time'].fillna(0).values[1:]/1000 ## Already Padded ## Conversion in s\n",
    "    prior_question_had_explanation = df_user['prior_question_had_explanation'].fillna(0).values[1:]*1 ## Already Padded\n",
    "    \n",
    "    lag_time = np.concatenate([[0],timestamp[1:] - timestamp[:-1] + elapsed_time])\n",
    "    \n",
    "    dico = {\n",
    "        'exercise_id' : exercise_id,\n",
    "        'container_id' : container_id,\n",
    "        'timestamp' : timestamp,\n",
    "        'correctness' : correctness,\n",
    "        'answer' : answer, \n",
    "        'elapsed_time' : elapsed_time,\n",
    "        'prior_question_had_explanation' : prior_question_had_explanation,\n",
    "        'lag_time' : lag_time\n",
    "    }\n",
    "    return dico\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dico = build_user_sequence(test_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico['lag_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "count = 0\n",
    "vect = []\n",
    "p = Pool(12)\n",
    "\n",
    "for elt in tqdm(train.groupby('user_id'), total = train['user_id'].nunique()):\n",
    "    vect.append(elt)\n",
    "    if len(vect) == batch_size:\n",
    "        vect = np.array(vect)\n",
    "        vect_user = vect[:,0]\n",
    "        vect_data = vect[:,1]\n",
    "        vect = []\n",
    "        \n",
    "        processed_dico = p.map(build_user_sequence, vect_data)\n",
    "        \n",
    "        # saving as batches of 2000\n",
    "        dico_user = {}\n",
    "        for i, elt in enumerate(vect_user):\n",
    "            dico_user[elt] = processed_dico[i]\n",
    "        save(dico_user, 'batch_'+str(count), 'user_batch_saint_2000')\n",
    "        \n",
    "        # saving as batches of 100\n",
    "        dico_user = {}\n",
    "        count1 = 0\n",
    "        for i, elt in enumerate(vect_user):\n",
    "            dico_user[elt] = processed_dico[i]\n",
    "            if len(dico_user.keys()) == 100:\n",
    "                save(dico_user, 'batch_'+str(count)+'_'+str(count1), 'user_batch_saint_100')\n",
    "                dico_user = {}\n",
    "                count1+=1\n",
    "        count += 1\n",
    "        \n",
    "p.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and discretisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## timestamp encoder\n",
    "dico_user = load('batch_'+str(0), 'user_batch_saint_2000')\n",
    "el = []\n",
    "for elt in dico_user:\n",
    "    ela = dico_user[elt]['timestamp']\n",
    "    ela[np.isnan(ela)] = 0\n",
    "    el += list(ela)\n",
    "timestamp_enc = Discretiser(300)\n",
    "timestamp_enc.fit(el)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Elapsed time encoder\n",
    "dico_user = load('batch_'+str(0), 'user_batch_saint_2000')\n",
    "el = []\n",
    "for elt in dico_user:\n",
    "    ela = dico_user[elt]['elapsed_time']\n",
    "    ela[np.isnan(ela)] = 0\n",
    "    el += list(ela)\n",
    "elapsed_enc = Discretiser(300)\n",
    "elapsed_enc.fit(el)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lag time encoder\n",
    "dico_user = load('batch_'+str(0), 'user_batch_saint_2000')\n",
    "el = []\n",
    "for elt in dico_user:\n",
    "    ela = dico_user[elt]['lag_time']\n",
    "    ela[np.isnan(ela)] = 0\n",
    "    el += list(ela)\n",
    "lag_time_enc = Discretiser(300)\n",
    "lag_time_enc.fit(el)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question mean encoder\n",
    "dico_question = load('dico_questions_mean')\n",
    "val = list(dico_question.values())\n",
    "qmean_enc = Discretiser(300)\n",
    "qmean_enc.fit(val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving\n",
    "save((timestamp_enc, elapsed_enc, lag_time_enc, qmean_enc), 'discrete_encoders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building tokenizer\n",
    "lectures = pd.read_csv('lectures.csv')\n",
    "questions = pd.read_csv('questions.csv')\n",
    "user_answer = np.array([-1,0,1,2,3])\n",
    "answered_correctly = np.array([-1,0,1])\n",
    "\n",
    "lectures_id = lectures['lecture_id'].unique()\n",
    "question_id = questions['question_id'].unique()\n",
    "\n",
    "lectures_id = ['l_' +  elt for elt in  lectures_id.astype(str)]\n",
    "question_id = ['q_' +  elt for elt in  question_id.astype(str)]\n",
    "\n",
    "all_tokens = np.array(['[PAD]', '[CLS]', '[SEP]', '[MASK]'] + lectures_id + question_id)\n",
    "\n",
    "\n",
    "lectures_id_1 = [elt + \"_-1\" for elt in lectures_id]\n",
    "question_id_0 = [elt + \"_0\" for elt in question_id]\n",
    "question_id_1 = [elt + \"_1\" for elt in question_id]\n",
    "\n",
    "tokenizer = Tokenizer(filters = '')\n",
    "\n",
    "tokenizer.fit_on_texts(\n",
    "    all_tokens\n",
    ")\n",
    "\n",
    "\n",
    "all_tokens_res = np.array(['[PAD]', '[CLS]', '[SEP]', '[MASK]'] + lectures_id_1 + question_id_0 + question_id_1)\n",
    "\n",
    "tokenizer_res = Tokenizer(filters = '')\n",
    "\n",
    "tokenizer_res.fit_on_texts(\n",
    "    all_tokens_res\n",
    ")\n",
    "\n",
    "save((tokenizer, tokenizer_res), 'tokenizers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionnaries():\n",
    "    df = pd.read_csv('questions.csv')\n",
    "    df1 = pd.read_csv('lectures.csv')\n",
    "\n",
    "    def apply(x):\n",
    "        return 'q_'+str(x)\n",
    "\n",
    "    def apply1(x):\n",
    "        return 'l_'+str(x)\n",
    "\n",
    "    def to_tab(x):\n",
    "        if str(x)!='nan':\n",
    "            x = np.array(str(x).split(' ')).astype(int)\n",
    "        else:\n",
    "            x = []\n",
    "        x.sort()\n",
    "        return x\n",
    "\n",
    "    df['tag'] = df['tags'].apply(to_tab)\n",
    "    df['qu'] = df['question_id'].apply(apply)\n",
    "    df1['l'] = df1['lecture_id'].apply(apply1)\n",
    "\n",
    "    ## unique tags part\n",
    "    tags_to_utags = {}\n",
    "    count = 0\n",
    "    for elt in df1['tag']:\n",
    "        if elt in tags_to_utags:\n",
    "            1\n",
    "        else:\n",
    "            tags_to_utags[str(elt)] = count\n",
    "            count+=1\n",
    "\n",
    "    for elt in df['tags']:\n",
    "        if elt in tags_to_utags:\n",
    "            1\n",
    "        else:\n",
    "            tags_to_utags[elt] = count\n",
    "            count+=1\n",
    "    df['utags'] = df['tags'].astype(str).replace(tags_to_utags)\n",
    "    df1['utags'] = df1['tag'].astype(str).replace(tags_to_utags)\n",
    "\n",
    "    ## Graph tags part\n",
    "    dico_l = {}\n",
    "    for t, data in df1.groupby('tag'):\n",
    "        dico_l[t] = data['l'].unique()\n",
    "\n",
    "    import networkx as nx\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(df['qu'])\n",
    "    G.add_nodes_from(df1['l'])\n",
    "\n",
    "    for i, elt in enumerate(tqdm(df['tag'])):\n",
    "        for j in elt:\n",
    "            try:\n",
    "                lec = dico_l[j]\n",
    "            except:\n",
    "                lec = []\n",
    "            for k in lec:\n",
    "                G.add_edge(df['qu'].iloc[i], k)\n",
    "\n",
    "    co = list(nx.connected_components(G))\n",
    "\n",
    "    tags_to_gtags = {}\n",
    "    count = 0\n",
    "    for i, elt in enumerate(tqdm(co)):\n",
    "        for j in elt:\n",
    "            tags_to_gtags[j] = i\n",
    "\n",
    "    df['gtags'] = df['qu'].replace(tags_to_gtags)\n",
    "    df1['gtags'] = df1['l'].replace(tags_to_gtags)\n",
    "    \n",
    "    df1['tag'] = df1['tag'].apply(lambda x  :[x])\n",
    "    \n",
    "    dico_utags = {}\n",
    "    dico_gtags = {}\n",
    "    dico_parts = {}\n",
    "    dico_tags = {}\n",
    "    \n",
    "    for pair in zip(df['qu'], df['utags'], df['gtags'], df['part'], df['tag']):\n",
    "        dico_utags[pair[0]] = pair[1]\n",
    "        dico_gtags[pair[0]] = pair[2]\n",
    "        dico_parts[pair[0]] = pair[3]\n",
    "        dico_tags[pair[0]] = pair[4]\n",
    "        \n",
    "    for pair in zip(df1['l'], df1['utags'], df1['gtags'], df1['part'], df1['tag']):\n",
    "        dico_utags[pair[0]] = pair[1]\n",
    "        dico_gtags[pair[0]] = pair[2]\n",
    "        dico_parts[pair[0]] = pair[3]\n",
    "        dico_tags[pair[0]] = pair[4]\n",
    "        \n",
    "    \n",
    "    \n",
    "    return dico_utags, dico_gtags, dico_parts, dico_tags\n",
    "\n",
    "dico_utags, dico_gtags, dico_parts, dico_tags = create_dictionnaries()\n",
    "\n",
    "save((dico_utags, dico_gtags, dico_parts, dico_tags), 'dico_tags')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sequence Modelling\n",
    "sequence_ids   emb1_main # tokenizer values + cls, pad et mask tokens\n",
    "answer_corr    emb2     # 0,1,2,+mask = 3\n",
    "part           emb8     # 6 parts\n",
    "timestamp      emb3     float discretized in 300 int values\n",
    "answer         emb4     # 0,1,2,3,4,+mask = 5\n",
    "elapsed_time   emb5     float discretized in 300 int values\n",
    "explained      emb6     # 2 possible, lectures put to 0\n",
    "avg_correct    emb7     float discretized in 300 int values\n",
    "\n",
    "## MLM loss\n",
    "sequence unmasked\n",
    "answer_corr unmasked\n",
    "\n",
    "## Next answer prediction loss\n",
    "[CLS] Query [SEP] Sequence Model\n",
    "\n",
    "Sequence Model as in sequence modelling\n",
    "\n",
    "Query\n",
    "question_id     ## id question      \n",
    "answer_corr     ## Mask token\n",
    "part            ## 6 parts\n",
    "timestamp       float discretized in 300 values\n",
    "answer          ## Masked to -1\n",
    "elapsed_time    ## 0\n",
    "explained   0   ## 0\n",
    "avg_correct     ## average score of the question discretized in 300 int values\n",
    "\n",
    "output : unmasked answer_corr    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self,batch_size=32, max_len = 128, folder = 'user_batch_saint_100', strategy = 'begin', mask_rate = 0.15, seq_mask_rate = 0.5, bidirectionnal = True):\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer, self.tokenizer_res = load('tokenizers')\n",
    "        self.max_len = max_len\n",
    "        self.folder = folder\n",
    "        self.dico_question = load('dico_questions_mean')\n",
    "        self.dico_utags, self.dico_gtags, self.dico_parts, self.dico_tags = load('dico_tags')\n",
    "        self.timestamp_enc, self.elapsed_enc,self.lag_time_enc, self.qmean_enc = load('discrete_encoders')\n",
    "        self.strategy = strategy\n",
    "        self.mask_rate = mask_rate\n",
    "        self.seq_mask_rate = seq_mask_rate\n",
    "        self.bidirectionnal = bidirectionnal\n",
    "        self.num_tags = 188\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 1000000\n",
    "    \n",
    "    def initiate_dico(self):\n",
    "        list_encoder = ['exercise','parts', 'question_mean', 'timestamp', 'tags']\n",
    "        list_decoder = ['correct_dec', 'explained_dec', 'elapsed_time_dec', 'answer_dec']\n",
    "        list_output = ['correct_out']\n",
    "        \n",
    "        dico_input = {}\n",
    "        for elt in list_encoder + list_decoder:\n",
    "            if elt in ['exercise']:\n",
    "                dico_input[elt] = np.zeros((self.batch_size, self.max_len)).astype(str)\n",
    "            elif elt in ['tags']:\n",
    "                dico_input[elt] = np.zeros((self.batch_size, self.max_len, self.num_tags)).astype('int32')\n",
    "            else:\n",
    "                dico_input[elt] = np.zeros((self.batch_size, self.max_len)).astype('int32')\n",
    "                \n",
    "        dico_output = {}\n",
    "        for elt in list_output:\n",
    "            if elt == 'exercise':\n",
    "                dico_output[elt] = np.zeros((self.batch_size, self.max_len)).astype(str)\n",
    "            else:\n",
    "                dico_output[elt] = np.zeros((self.batch_size, self.max_len)).astype('int32')\n",
    "        return dico_input, dico_output\n",
    "\n",
    "    def map_part(self, ids):\n",
    "        def replace_dico_part(x):\n",
    "            try:\n",
    "                return self.dico_parts[x]\n",
    "            except:\n",
    "                return 0\n",
    "        return np.array(list(map(replace_dico_part,ids)))\n",
    "    \n",
    "    def map_utags(self, ids):\n",
    "        def replace_dico_utags(x):\n",
    "            try:\n",
    "                if str(self.dico_utags[x]) != 'nan':\n",
    "                    return str(self.dico_utags[x])\n",
    "                else:\n",
    "                    return 0\n",
    "            except:\n",
    "                return 0\n",
    "        return np.array(list(map(replace_dico_utags,ids)))\n",
    "    \n",
    "    def map_gtags(self, ids):\n",
    "        def replace_dico_gtags(x):\n",
    "            try:\n",
    "                if str(self.dico_gtags[x]) != 'nan':\n",
    "                    return str(self.dico_gtags[x])\n",
    "                else:\n",
    "                    return 0\n",
    "            except:\n",
    "                return 0\n",
    "        return np.array(list(map(replace_dico_gtags,ids)))\n",
    "    \n",
    "    def map_mean(self, ids):\n",
    "        def replace_dico_question(x):\n",
    "            try:\n",
    "                return self.dico_question[x]\n",
    "            except:\n",
    "                return 0.5\n",
    "        return np.array(list(map(replace_dico_question,ids))).astype('float32')\n",
    "\n",
    "    def map_tags(self, ids):\n",
    "        tags = np.zeros((len(ids), 188))\n",
    "\n",
    "        def map_apply(x):\n",
    "            try:\n",
    "                return self.dico_tags[x]\n",
    "            except:\n",
    "                return []\n",
    "            \n",
    "        found_tags = list(map(map_apply, ids))\n",
    "\n",
    "        for i , elt in enumerate(found_tags):\n",
    "            for j in elt:\n",
    "                tags[i,j] += 1\n",
    "        return tags\n",
    "    \n",
    "    def update_dico(self, dico_input, dico_output, input_vals, output_vals, i):\n",
    "        list_encoder = ['exercise','parts', 'question_mean', 'timestamp', 'tags']\n",
    "        list_decoder = ['correct_dec', 'explained_dec', 'elapsed_time_dec', 'answer_dec']\n",
    "        list_output = ['correct_out']\n",
    "        \n",
    "        for j, elt in enumerate(list_encoder + list_decoder):\n",
    "            dico_input[elt][i] = input_vals[j]\n",
    "        \n",
    "        for j, elt in enumerate(list_output):\n",
    "            dico_output[elt][i] = output_vals[j]\n",
    "        return dico_input, dico_output\n",
    "\n",
    "    def remove_na(self, x):\n",
    "        x = np.array(list(x))\n",
    "        x[np.isnan(x)] = 0\n",
    "        return x\n",
    "    \n",
    "    def apply_mask(self, x, mask, pad_token, mask_token):\n",
    "        x_out = []\n",
    "        x_in = []\n",
    "        for i, elt in enumerate(mask):\n",
    "            if mask[i] == 1:\n",
    "                x_out.append(x[i])\n",
    "                x_in.append(mask_token)\n",
    "            else:\n",
    "                x_out.append(pad_token)\n",
    "                x_in.append(x[i])\n",
    "        return np.array(x_in), np.array(x_out)\n",
    "\n",
    "    def build_sequence(self, user_history):\n",
    "        dico_sequence = deepcopy(user_history)        \n",
    "        dico_sequence['elapsed_time'] = self.remove_na(dico_sequence['elapsed_time'])\n",
    "        dico_sequence['lag_time'] = self.remove_na(dico_sequence['lag_time'])\n",
    "        dico_sequence['prior_question_had_explanation'] = self.remove_na(dico_sequence['prior_question_had_explanation'])\n",
    "        \n",
    "        dico_sequence['elapsed_time'] = np.concatenate([dico_sequence['elapsed_time'], [0]])\n",
    "        dico_sequence['prior_question_had_explanation'] = np.concatenate([dico_sequence['prior_question_had_explanation'], [0]])\n",
    "        \n",
    "        seq_size = len(dico_sequence['exercise_id'])\n",
    "        \n",
    "        ## Cut sequence\n",
    "        if self.strategy == 'begin':\n",
    "            for elt in dico_sequence:\n",
    "                dico_sequence[elt] = dico_sequence[elt][:self.max_len]\n",
    "        else:\n",
    "            if seq_size <= 100:\n",
    "                for elt in dico_sequence:\n",
    "                    dico_sequence[elt] = dico_sequence[elt][-self.max_len:]\n",
    "            else:\n",
    "                start = random.randint(0, seq_size - 100)\n",
    "                for elt in dico_sequence:\n",
    "                    dico_sequence[elt] = dico_sequence[elt][start : start + self.max_len]\n",
    "        \n",
    "        dico_sequence['exercise_id_out'] = dico_sequence['exercise_id']\n",
    "        dico_sequence['correctness_out'] = dico_sequence['correctness']\n",
    "        dico_sequence['answer_out'] = dico_sequence['answer']\n",
    "        \n",
    "        dico_sequence['exercise_correct'] = np.array([str(dico_sequence['exercise_id'][i]) + '_' + str(dico_sequence['correctness'][i]) for i in range(len(dico_sequence['exercise_id']))])\n",
    "        \n",
    "        ## Pad sequence\n",
    "        pad_tokens = ['[PAD]', 0, 0, -1, -1, 0, 0, 0, '[PAD]', -1, -1, '[PAD]']\n",
    "        for j, elt in enumerate(dico_sequence):\n",
    "            size = len(dico_sequence[elt])\n",
    "            if size <= self.max_len:\n",
    "                adding = self.max_len - size\n",
    "                tok = pad_tokens[j]\n",
    "                if type(tok) == str:\n",
    "                    add = np.array([tok for elt in range(adding)])\n",
    "                else:\n",
    "                    add = np.zeros(adding) + tok\n",
    "                dico_sequence[elt] = np.concatenate([dico_sequence[elt], add], axis = 0)\n",
    "#                 print(dico_sequence[elt].shape)\n",
    "        \n",
    "        dico_sequence['correctness'][dico_sequence['correctness'] == -1] = 2\n",
    "\n",
    "# list_encoder = ['exercise_enc','parts_enc', 'tags_enc', 'question_mean_enc', 'timestamp_enc']\n",
    "# list_decoder = ['correct_dec', 'explained_dec', 'elapsed_time_dec', 'answer_dec']\n",
    "        \n",
    "        input_vals = [\n",
    "            dico_sequence['exercise_id'],\n",
    "            self.map_part(dico_sequence['exercise_id']),\n",
    "            self.qmean_enc.transform(self.map_mean(dico_sequence['exercise_id'])),\n",
    "            self.timestamp_enc.transform(dico_sequence['timestamp']),\n",
    "            self.map_tags(dico_sequence['exercise_id']),\n",
    "\n",
    "            dico_sequence['correctness'],\n",
    "            dico_sequence['prior_question_had_explanation'],\n",
    "            self.elapsed_enc.transform(dico_sequence['elapsed_time']),\n",
    "            dico_sequence['answer'] + 1,            \n",
    "        ]\n",
    "#         print(np.concatenate([['[CLS]'], dico_sequence['exercise_correct'][:-1]]))\n",
    "\n",
    "        dico_sequence['correctness_out'][dico_sequence['correctness_out'] == -1] = 2\n",
    "\n",
    "        output_vals = [\n",
    "            dico_sequence['correctness_out'],\n",
    "        ]\n",
    "            \n",
    "        return input_vals,output_vals\n",
    "    \n",
    "    def mask(self, x, rate, mask_token):\n",
    "        bs = x.shape[0]\n",
    "        fd = x.shape[1]\n",
    "        \n",
    "        mask = np.random.choice([0,1], size = (bs, fd), p = [1-rate, rate])\n",
    "        \n",
    "        masked_inputs = x*(1-mask)\n",
    "        masked_outputs = x*mask\n",
    "        masked_outputs += (1-mask)*mask_token\n",
    "        return masked_inputs, masked_outputs\n",
    "    \n",
    "    def sample_strategy(self, x):\n",
    "        if x < 500:\n",
    "            return x\n",
    "        else:\n",
    "            return 500\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        ## Load random batch\n",
    "        file_name = random.choice(os.listdir('./'+self.folder))\n",
    "        dico_user = load(file_name.split('.')[0], self.folder)\n",
    "        \n",
    "        p = []\n",
    "        for elt in dico_user:\n",
    "            p.append(self.sample_strategy(len(dico_user[elt]['exercise_id'])))\n",
    "        \n",
    "        p = np.array(p)\n",
    "        p = p/p.sum()\n",
    "        \n",
    "        list_user = np.random.choice(list(dico_user.keys()), size = self.batch_size, replace = True, p = p)\n",
    "        \n",
    "        dico_input, dico_output = self.initiate_dico()\n",
    "        \n",
    "        \n",
    "        for i, elt in enumerate(list_user):\n",
    "            user_history = dico_user[elt]\n",
    "            input_vals, output_vals = self.build_sequence(user_history)\n",
    "            dico_input, dico_output = self.update_dico(dico_input, dico_output, input_vals, output_vals, i)\n",
    "\n",
    "        x = deepcopy(dico_input['exercise'])\n",
    "        dico_input['exercise'] = np.array(self.tokenizer.texts_to_sequences(x.tolist()))\n",
    "        \n",
    "#         x = deepcopy(dico_input['exercise_dec'])\n",
    "#         dico_input['exercise_dec'] = np.array(self.tokenizer.texts_to_sequences(x.tolist()))\n",
    "        \n",
    "        X = []\n",
    "        for elt in dico_input:\n",
    "                X.append(np.array(list(dico_input[elt])).astype('int32'))\n",
    "        \n",
    "        X = {\n",
    "             'exercise':X[0],\n",
    "             'parts':X[1],\n",
    "             'question_mean':X[2],\n",
    "             'timestamp':X[3],\n",
    "             'tags':X[4],\n",
    "\n",
    "             'correct_dec':X[5],\n",
    "             'explained_dec':X[6],\n",
    "             'elapsed_time_dec':X[7],\n",
    "             'answer_dec':X[8],\n",
    "        }\n",
    "        \n",
    "        y = dico_output['correct_out'].astype('int32')\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def __get_data(self, batch):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = DataGenerator(batch_size=64, max_len = 100, folder = 'user_batch_saint_100', strategy = 'end', mask_rate = 0.15, seq_mask_rate = 1, bidirectionnal = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 106 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x,y= gen[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exercise': array([[4890, 9786, 6043, ...,  340, 5236, 5246],\n",
       "        [6511, 5646, 9972, ..., 6587, 4878, 4358],\n",
       "        [4460, 1175,  640, ...,    1,    1,    1],\n",
       "        ...,\n",
       "        [5448, 4011, 4546, ...,    1,    1,    1],\n",
       "        [9712, 5703, 8917, ..., 8783, 5685, 6043],\n",
       "        [ 885, 1647, 1730, ..., 1459, 1730, 1459]]),\n",
       " 'parts': array([[5, 5, 5, ..., 5, 5, 5],\n",
       "        [5, 5, 5, ..., 5, 5, 5],\n",
       "        [5, 2, 2, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [5, 5, 5, ..., 0, 0, 0],\n",
       "        [5, 5, 5, ..., 5, 5, 5],\n",
       "        [2, 2, 2, ..., 2, 2, 2]]),\n",
       " 'question_mean': array([[ 51, 184,  67, ...,  35,  29, 128],\n",
       "        [ 18, 109,  95, ...,  53,  70, 179],\n",
       "        [ 87, 138, 105, ...,  35,  35,  35],\n",
       "        ...,\n",
       "        [ 90,   7, 147, ...,  35,  35,  35],\n",
       "        [181, 215,  46, ...,  21,  64,  67],\n",
       "        [214, 172,  59, ...,  14,  59,  14]]),\n",
       " 'timestamp': array([[ 65,  69,  69, ..., 150, 150, 150],\n",
       "        [ 27,  28,  28, ...,  41,  41,  41],\n",
       "        [  1,   1,   2, ...,   1,   1,   1],\n",
       "        ...,\n",
       "        [  1,   2,   4, ...,   1,   1,   1],\n",
       "        [ 76,  76,  76, ..., 134, 136, 138],\n",
       "        [282, 282, 282, ..., 295, 295, 295]]),\n",
       " 'tags': array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 1, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]]]),\n",
       " 'correct_dec': array([[1, 1, 1, ..., 2, 1, 1],\n",
       "        [0, 1, 0, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 2, 2, 2],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 2, 2, 2],\n",
       "        [1, 1, 0, ..., 0, 0, 1],\n",
       "        [1, 1, 0, ..., 0, 1, 1]]),\n",
       " 'explained_dec': array([[1, 1, 1, ..., 1, 1, 0],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1]]),\n",
       " 'elapsed_time_dec': array([[ 27,  46, 277, ..., 148, 247,   6],\n",
       "        [198, 220, 206, ..., 198,  24,  20],\n",
       "        [231, 137,  67, ...,   6,   6,   6],\n",
       "        ...,\n",
       "        [282, 265, 256, ...,   6,   6,   6],\n",
       "        [ 46,  78, 261, ..., 148, 198, 148],\n",
       "        [ 58,  78, 137, ...,  58, 148,  58]]),\n",
       " 'answer_dec': array([[3, 3, 1, ..., 0, 4, 4],\n",
       "        [2, 4, 3, ..., 4, 1, 4],\n",
       "        [2, 2, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [3, 2, 1, ..., 0, 0, 0],\n",
       "        [2, 4, 1, ..., 3, 2, 1],\n",
       "        [1, 4, 2, ..., 2, 4, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going from tabular to custom transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_transformers2 import *\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, TimeDistributed, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custEncoder(tf.keras.Model):  \n",
    "    def __init__(self, num_layers = 2, d_model = 512, num_heads = 8, dff = 1024, \n",
    "                 input_vocab_size = 14000, maximum_position_encoding = 512, rate=0, \n",
    "                 bidirectional_encoder = False):\n",
    "        super(custEncoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "         \n",
    "        self.enc_layers1 = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "        self.bidirectional_encoder = bidirectional_encoder\n",
    "        \n",
    "    def call(self, x, training, features_for_padding = None, n_devices = 1):\n",
    "        seq_len = 100#tf.shape(x)[1]\n",
    "        \n",
    "        if self.bidirectional_encoder == False:\n",
    "            look_ahead_mask = create_look_ahead_mask(tf.shape(features_for_padding)[1])\n",
    "            dec_target_padding_mask = create_padding_mask(features_for_padding, pad_token = 1)\n",
    "            mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "            \n",
    "        else:\n",
    "            mask = create_padding_mask(features_for_padding, pad_token = 1)\n",
    "        \n",
    "        # adding embedding and position encoding.        \n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        \n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers1[i](x, training, mask) \n",
    "        return x   #, mask  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackwardDecoderLayer(tf.keras.layers.Layer):\n",
    "    ## This is the normal layers, but with look backward masks and without skip connections on the last layers to avoid flowing\n",
    "    # present information in the prediction\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1, n_windows = 2):\n",
    "        super(BackwardDecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = [MultiHeadAttention(d_model, num_heads) for _ in range(n_windows)]\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "        \n",
    "        self.n_windows = n_windows\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, windows_masks):\n",
    "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "                \n",
    "        \n",
    "        attn2, attn_weights_block2 = None, None\n",
    "        for i in range(self.n_windows):\n",
    "#             print(out1)\n",
    "#             print(enc_output)\n",
    "            att_temp, att_w_temp = self.mha2[i](out1, enc_output, enc_output, windows_masks[i])\n",
    "            if attn2 == None:\n",
    "                attn2, attn_weights_block2 = att_temp, att_w_temp\n",
    "            else:\n",
    "                attn2 += att_temp\n",
    "                att_w_temp += att_w_temp\n",
    "        \n",
    "        size = tf.shape(attn2)[1]\n",
    "        masking = tf.cast(tf.expand_dims(tf.transpose(1 - tf.one_hot([0 for _ in range(self.d_model)], size)), axis=0), dtype = attn2.dtype) \n",
    "        \n",
    "        attn2 *= masking\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)     \n",
    "        \n",
    "        return out3, attn_weights_block2, attn_weights_block2 \n",
    "    \n",
    "class AltededDecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1, n_windows = 2):\n",
    "        super(AltededDecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = [MultiHeadAttention(d_model, num_heads) for _ in range(n_windows)]\n",
    "        self.n_windows = n_windows\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, windows_masks):\n",
    "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = None, None\n",
    "        for i in range(self.n_windows):\n",
    "            att_temp, att_w_temp = self.mha2[i](out1, enc_output, enc_output, windows_masks[i])\n",
    "            if attn2 == None:\n",
    "                attn2, attn_weights_block2 = att_temp, att_w_temp\n",
    "            else:\n",
    "                attn2 += att_temp\n",
    "                att_w_temp += att_w_temp\n",
    "        \n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windowed_attention(size, windows_size = 'all', present = True):\n",
    "    if windows_size == 'all':\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    else:\n",
    "        mask = 1-tf.linalg.band_part(tf.ones((size, size)), windows_size, 0)\n",
    "    \n",
    "    if present == False:\n",
    "        mask += tf.linalg.band_part(tf.ones((size, size)), 0, 0)\n",
    "    return mask\n",
    "    \n",
    "    \n",
    "class custDecoder(tf.keras.Model):\n",
    "    def __init__(self, num_layers = 2, d_model=512, num_heads=8, dff=1024, target_vocab_size=14000,\n",
    "               maximum_position_encoding = 512, rate=0, bidirectional_decoder = False, n_windows = 2):\n",
    "        super(custDecoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [AltededDecoderLayer(d_model, num_heads, dff, rate, n_windows = n_windows) \n",
    "                           for _ in range(num_layers)]\n",
    "        \n",
    "        self.back_dec_layer = BackwardDecoderLayer(d_model, num_heads, dff, rate, n_windows = n_windows)\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "        self.bidirectional_decoder = bidirectional_decoder\n",
    "    \n",
    "    def call(self, x, enc_output, training = True, windows = ['all'], features_for_padding = None, n_devices = 1):\n",
    "\n",
    "        seq_len = tf.shape(features_for_padding)[1]\n",
    "        attention_weights = {}\n",
    "        \n",
    "        if self.bidirectional_decoder == False:\n",
    "            look_ahead_mask = create_windowed_attention(tf.shape(features_for_padding)[1], windows_size = 'all', present = True)\n",
    "            \n",
    "            present_windows_masks = [create_windowed_attention(tf.shape(features_for_padding)[1], windows_size = elt, present = True) for elt in windows]\n",
    "            past_windows_masks = [create_windowed_attention(tf.shape(features_for_padding)[1], windows_size = elt, present = False) for elt in windows]\n",
    "            padding_mask = create_padding_mask(features_for_padding, pad_token = 1)\n",
    "            mask = tf.maximum(padding_mask, look_ahead_mask)\n",
    "            present_windows_masks = [tf.maximum(padding_mask, elt) for elt in present_windows_masks]\n",
    "            past_windows_masks = [tf.maximum(padding_mask, elt) for elt in past_windows_masks]\n",
    "            \n",
    "        else:\n",
    "            mask = create_padding_mask(x, pad_token = 1)\n",
    "        \n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 mask, present_windows_masks)\n",
    "        x, block1, block2 = self.back_dec_layer(x, enc_output, training, mask, past_windows_masks)\n",
    "#         print(x)\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(x, in_shape, out_shape):\n",
    "    if x == 'tags':\n",
    "        return tf.keras.layers.Dense(out_shape, activation = 'relu')\n",
    "    else:\n",
    "        return tf.keras.layers.Embedding(in_shape, out_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_model_emb(n_devices = 1):\n",
    "    list_encoder = ['exercise','parts', 'question_mean', 'timestamp','tags']\n",
    "    list_decoder = ['correct_dec', 'explained_dec', 'elapsed_time_dec', 'answer_dec']\n",
    "    inputs_names = list_encoder + list_decoder\n",
    "    \n",
    "    ## training auto encoder to predict itselt\n",
    "    seq_len = 100\n",
    "    d_model = 128\n",
    "    def create_inputs(x, seq_len):\n",
    "        if x != 'tags':\n",
    "            return tf.keras.Input(shape = (seq_len, ), name = x)\n",
    "        else:\n",
    "            return tf.keras.Input(shape = (seq_len, 188), name = x)\n",
    "\n",
    "    inputs = {elt : create_inputs(elt, seq_len) for elt in list_encoder + list_decoder}\n",
    "    \n",
    "    in_shapes = [14000, 8,  301, 301, 188, 3,  3,  301, 8]\n",
    "    out_shapes = [d_model,d_model,d_model,d_model,d_model,d_model,d_model,d_model,d_model]\n",
    "    \n",
    "    embeddings = {inputs_names[i] : get_embedding(inputs_names[i], in_shapes[i], out_shapes[i]) for i in range(len(inputs_names))}\n",
    "\n",
    "    dec_in = ['exercise', 'parts', 'question_mean', 'timestamp', \n",
    "             'correct_dec', 'explained_dec', 'elapsed_time_dec', 'answer_dec', \n",
    "             'tags']\n",
    "    dec_inputs = [inputs[elt] for elt in dec_in]\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Encoder Blocks Creation\n",
    "    encoder = custEncoder(num_layers = 4, d_model = 128, num_heads = 8, dff = 1024, \n",
    "                          input_vocab_size = 14000, maximum_position_encoding = 512, rate=0, \n",
    "                          bidirectional_encoder = False)\n",
    "    \n",
    "    ## Decoder Blocks Creation\n",
    "    n_windows = 1\n",
    "    windows = ['all']\n",
    "    \n",
    "    decoder = custDecoder(num_layers = 4, d_model=128, num_heads=8, dff=1024, \n",
    "                          target_vocab_size=14000, maximum_position_encoding = 512, rate=0, \n",
    "                          bidirectional_decoder = False, n_windows=n_windows)\n",
    "\n",
    "    ## Encoder Calls\n",
    "\n",
    "    enc_embeddings = [embeddings[elt](inputs[elt]) for elt in list_encoder]\n",
    "    \n",
    "    emb = enc_embeddings[0]\n",
    "    for elt in enc_embeddings[1:]:\n",
    "        emb += elt\n",
    "        \n",
    "    encoded = encoder(emb,\n",
    "                      training = True,\n",
    "                      features_for_padding = inputs['exercise'])\n",
    "    \n",
    "    ## Decoder Calls\n",
    "    dec_embeddings = [embeddings[elt](inputs[elt]) for elt in inputs_names]\n",
    "    embd = dec_embeddings[0]\n",
    "    for elt in dec_embeddings[1:]:\n",
    "        embd += elt\n",
    "    \n",
    "    decoded= decoder(embd,\n",
    "                    enc_output = encoded,\n",
    "                    training = True, \n",
    "                    windows = windows,\n",
    "                    features_for_padding = inputs['exercise'])\n",
    "\n",
    "    ## Adding a skip connexion to the input sequence informations\n",
    "    skip = tf.keras.layers.Concatenate(axis = -1)([decoded, encoded])\n",
    "    model_emb = Model(inputs, skip)\n",
    "    \n",
    "    return model_emb\n",
    "\n",
    "def get_model_cl(model_emb, n_devices = 1):\n",
    "    list_encoder = ['exercise','parts', 'question_mean', 'timestamp','tags']\n",
    "    list_decoder = ['correct_dec', 'explained_dec', 'elapsed_time_dec', 'answer_dec']\n",
    "\n",
    "    ## training auto encoder to predict itselt\n",
    "    seq_len = 100\n",
    "    def create_inputs(x, seq_len):\n",
    "        if x != 'tags':\n",
    "            return tf.keras.Input(shape = (seq_len, ), name = x, dtype = tf.int32)\n",
    "        else:\n",
    "            return tf.keras.Input(shape = (seq_len, 188), name = x)\n",
    "\n",
    "    inputs = {elt : create_inputs(elt, seq_len) for elt in list_encoder + list_decoder}\n",
    "\n",
    "\n",
    "    skip = model_emb(inputs)\n",
    "\n",
    "    embedding = tf.keras.layers.Dense(128, activation = 'relu')(skip)\n",
    "\n",
    "    prediction = tf.keras.layers.Dense(3, activation = 'softmax')(embedding)\n",
    "    model = Model(inputs, prediction)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_emb = get_model_emb()\n",
    "model = get_model_cl(model_emb)\n",
    "# model.load_weights('./final_model/weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "exercise (InputLayer)           [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "parts (InputLayer)              [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 100, 128)     1792000     exercise[0][0]                   \n",
      "                                                                 exercise[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 128)     1024        parts[0][0]                      \n",
      "                                                                 parts[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "question_mean (InputLayer)      [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_4 (TensorFlowOp multiple             0           embedding[1][0]                  \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 100, 128)     38528       question_mean[0][0]              \n",
      "                                                                 question_mean[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "timestamp (InputLayer)          [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_5 (TensorFlowOp multiple             0           tf_op_layer_add_4[0][0]          \n",
      "                                                                 embedding_2[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 100, 128)     38528       timestamp[0][0]                  \n",
      "                                                                 timestamp[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tags (InputLayer)               [(None, 100, 188)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_6 (TensorFlowOp multiple             0           tf_op_layer_add_5[0][0]          \n",
      "                                                                 embedding_3[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 100, 128)     24192       tags[0][0]                       \n",
      "                                                                 tags[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "correct_dec (InputLayer)        [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_7 (TensorFlowOp multiple             0           tf_op_layer_add_6[0][0]          \n",
      "                                                                 dense[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 100, 128)     384         correct_dec[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "explained_dec (InputLayer)      [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add (TensorFlowOpLa multiple             0           embedding[0][0]                  \n",
      "                                                                 embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_8 (TensorFlowOp multiple             0           tf_op_layer_add_7[0][0]          \n",
      "                                                                 embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 100, 128)     384         explained_dec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "elapsed_time_dec (InputLayer)   [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_1 (TensorFlowOp multiple             0           tf_op_layer_add[0][0]            \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_9 (TensorFlowOp multiple             0           tf_op_layer_add_8[0][0]          \n",
      "                                                                 embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 100, 128)     38528       elapsed_time_dec[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "answer_dec (InputLayer)         [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_2 (TensorFlowOp multiple             0           tf_op_layer_add_1[0][0]          \n",
      "                                                                 embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_10 (TensorFlowO multiple             0           tf_op_layer_add_9[0][0]          \n",
      "                                                                 embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 100, 128)     1024        answer_dec[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_3 (TensorFlowOp multiple             0           tf_op_layer_add_2[0][0]          \n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_11 (TensorFlowO multiple             0           tf_op_layer_add_10[0][0]         \n",
      "                                                                 embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "cust_encoder (custEncoder)      (None, 100, 128)     1319424     tf_op_layer_add_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cust_decoder (custDecoder)      (None, None, 128)    1980800     tf_op_layer_add_11[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 100, 256)     0           cust_decoder[0][0]               \n",
      "                                                                 cust_encoder[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 5,234,816\n",
      "Trainable params: 5,234,816\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_emb.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "answer_dec (InputLayer)         [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "correct_dec (InputLayer)        [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "elapsed_time_dec (InputLayer)   [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "exercise (InputLayer)           [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "explained_dec (InputLayer)      [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "parts (InputLayer)              [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "question_mean (InputLayer)      [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tags (InputLayer)               [(None, 100, 188)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "timestamp (InputLayer)          [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model (Model)                   (None, 100, 256)     5234816     answer_dec[0][0]                 \n",
      "                                                                 correct_dec[0][0]                \n",
      "                                                                 elapsed_time_dec[0][0]           \n",
      "                                                                 exercise[0][0]                   \n",
      "                                                                 explained_dec[0][0]              \n",
      "                                                                 parts[0][0]                      \n",
      "                                                                 question_mean[0][0]              \n",
      "                                                                 tags[0][0]                       \n",
      "                                                                 timestamp[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_75 (Dense)                (None, 100, 128)     32896       model[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_76 (Dense)                (None, 100, 3)       387         dense_75[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 5,268,099\n",
      "Trainable params: 5,268,099\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AUCCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, validation=None, logs = {}, dico_params = {}, from_path = None):\n",
    "        super(AUCCallback, self).__init__()\n",
    "        self.validation = validation\n",
    "        self.epoch = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        ## Roc auc calculation on test set\n",
    "        x_val, y_val = self.validation[0], self.validation[1]\n",
    "        pred = model.predict(x_val, verbose = 0, batch_size = 64)\n",
    "        \n",
    "        y_true = y_val\n",
    "        y_pred = pred[:,:,1]\n",
    "        \n",
    "        y_pred = y_pred.reshape(y_true.shape[0] * y_true.shape[1])\n",
    "        y_true = y_true.reshape(y_true.shape[0] * y_true.shape[1])\n",
    "        \n",
    "        y_pred = y_pred[y_true != 2]\n",
    "        y_true = y_true[y_true != 2]\n",
    "        \n",
    "        y_true = y_true.astype(int)\n",
    "        \n",
    "        roc_auc = roc_auc_score(y_true, y_pred)\n",
    "        \n",
    "        logs['roc_auc'] = roc_auc\n",
    "        print(logs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                    from_logits=True, reduction='none')\n",
    "\n",
    "# loss_object = tf.keras.losses.MAE()\n",
    "max_len = 100\n",
    "batch_size = 64\n",
    "def loss_mae(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 20))\n",
    "    loss = tf.keras.losses.MSE(real, pred)\n",
    "    mask = tf.cast(mask, dtype = loss.dtype)\n",
    "    mask = mask[:,:,0]\n",
    "    loss *= mask\n",
    "    loss = tf.math.reduce_sum(loss) / tf.math.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "def loss_cross(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 2))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask , dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    loss_ = tf.reduce_mean(loss_)\n",
    "    return loss_\n",
    "\n",
    "def acc(true, pred):\n",
    "    mask = tf.cast(tf.math.logical_not(tf.math.equal(true, 2)),dtype = true.dtype)\n",
    "    pred = tf.math.argmax(pred, axis=-1, output_type=tf.dtypes.int64, name=None)\n",
    "    pred = tf.cast(pred, dtype = true.dtype)\n",
    "    pred = pred*mask\n",
    "    true = true*mask\n",
    "    equal = tf.cast(tf.math.equal(pred, true), dtype = true.dtype)\n",
    "    n_equal = tf.math.reduce_sum(equal)\n",
    "    n_mask = tf.math.reduce_sum(mask)\n",
    "    n_tot = tf.math.reduce_sum(tf.cast(tf.math.greater(true, -1), dtype = true.dtype))\n",
    "    n_masked = n_tot - n_mask\n",
    "    n_masked = n_tot - n_mask\n",
    "    return (n_equal - n_masked) / (n_tot - n_masked)\n",
    "\n",
    "temperature = 8\n",
    "batch_size = 64\n",
    "seq_len = max_len\n",
    "\n",
    "def loss_auc(true, pred, batch_size = batch_size):\n",
    "    mask = tf.cast(tf.math.logical_not(tf.math.equal(true, 2)),dtype = true.dtype)\n",
    "    true *= mask\n",
    "    \n",
    "    pred = pred[:,:,1]\n",
    "    \n",
    "    pred = tf.reshape(pred, [-1,1])\n",
    "    true = tf.reshape(true, [-1,1])\n",
    "    \n",
    "    pred = tf.repeat(pred, batch_size*seq_len, axis = -1)\n",
    "    \n",
    "    diff1 = tf.math.exp(- temperature * (pred - tf.transpose(pred)))\n",
    "    \n",
    "    true = tf.repeat(true, batch_size*seq_len, axis = -1)\n",
    "    zero_un_comp = tf.cast(tf.math.maximum(true - tf.transpose(true), 0), dtype = diff1.dtype)\n",
    "    diff1 *= zero_un_comp\n",
    "    return tf.math.reduce_sum(diff1)/tf.math.reduce_sum(zero_un_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "# lr = CustomSchedule(128, 4000)\n",
    "\n",
    "lr = 1e-4\n",
    "loss_classif     =  loss_auc # find the right loss for multi-class classification\n",
    "optimizer        = Adam(lr, 1e-8)\n",
    "metrics_classif  =  [acc]\n",
    "\n",
    "model.compile(\n",
    "            loss = loss_classif,\n",
    "            optimizer=optimizer,\n",
    "            metrics=metrics_classif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = DataGenerator(batch_size=64, max_len = max_len, folder = 'user_batch_saint_100', strategy = 'end', mask_rate = 0.15, seq_mask_rate = 1, bidirectionnal = False)\n",
    "test_gen = DataGenerator(batch_size=1024, max_len = max_len, folder = 'user_batch_saint_test', strategy = 'end', mask_rate = 0.15, seq_mask_rate = 1, bidirectionnal = False)\n",
    "x_test, y_test = test_gen[0]\n",
    "# x_train, y_train = train_gen[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save((x_test, y_test), 'comparable_test_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x_test, y_test) = load('comparable_test_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate on 1024 samples\n",
      "Epoch 1/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.8274 - acc: 0.6502{'loss': 0.8269288573265076, 'acc': 0.650234, 'val_loss': 0.5827089212834835, 'val_acc': 0.6451028, 'roc_auc': 0.7569713235302846}\n",
      "500/500 [==============================] - 153s 306ms/step - loss: 0.8269 - acc: 0.6502 - val_loss: 0.5827 - val_acc: 0.6451\n",
      "Epoch 2/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5676 - acc: 0.6481{'loss': 0.5671761851906777, 'acc': 0.6482136, 'val_loss': 0.5757704451680183, 'val_acc': 0.6451028, 'roc_auc': 0.7595037081137457}\n",
      "500/500 [==============================] - 132s 263ms/step - loss: 0.5672 - acc: 0.6482 - val_loss: 0.5758 - val_acc: 0.6451\n",
      "Epoch 3/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5654 - acc: 0.6471{'loss': 0.5654036285877228, 'acc': 0.64717543, 'val_loss': 0.5727940946817398, 'val_acc': 0.6451028, 'roc_auc': 0.760430420388426}\n",
      "500/500 [==============================] - 142s 284ms/step - loss: 0.5654 - acc: 0.6472 - val_loss: 0.5728 - val_acc: 0.6451\n",
      "Epoch 4/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5609 - acc: 0.6463{'loss': 0.5608078501820565, 'acc': 0.646273, 'val_loss': 0.5710290372371674, 'val_acc': 0.6450913, 'roc_auc': 0.7612059666213838}\n",
      "500/500 [==============================] - 140s 280ms/step - loss: 0.5608 - acc: 0.6463 - val_loss: 0.5710 - val_acc: 0.6451\n",
      "Epoch 5/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5574 - acc: 0.6483{'loss': 0.557289752304554, 'acc': 0.64820313, 'val_loss': 0.5711222775280476, 'val_acc': 0.6450913, 'roc_auc': 0.7608943350692134}\n",
      "500/500 [==============================] - 138s 277ms/step - loss: 0.5573 - acc: 0.6482 - val_loss: 0.5711 - val_acc: 0.6451\n",
      "Epoch 6/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5621 - acc: 0.6467{'loss': 0.5621375043392182, 'acc': 0.64675885, 'val_loss': 0.5692149270325899, 'val_acc': 0.6450913, 'roc_auc': 0.76191174712874}\n",
      "500/500 [==============================] - 138s 275ms/step - loss: 0.5621 - acc: 0.6468 - val_loss: 0.5692 - val_acc: 0.6451\n",
      "Epoch 7/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5611 - acc: 0.6488{'loss': 0.561146142423153, 'acc': 0.6487775, 'val_loss': 0.5689777005463839, 'val_acc': 0.6450913, 'roc_auc': 0.7620657349202384}\n",
      "500/500 [==============================] - 138s 276ms/step - loss: 0.5611 - acc: 0.6488 - val_loss: 0.5690 - val_acc: 0.6451\n",
      "Epoch 8/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5606 - acc: 0.6471{'loss': 0.5606505614519119, 'acc': 0.6470315, 'val_loss': 0.5685393307358027, 'val_acc': 0.6450913, 'roc_auc': 0.7623029129238831}\n",
      "500/500 [==============================] - 137s 273ms/step - loss: 0.5607 - acc: 0.6470 - val_loss: 0.5685 - val_acc: 0.6451\n",
      "Epoch 9/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5581 - acc: 0.6481{'loss': 0.5580575257539749, 'acc': 0.6480049, 'val_loss': 0.5684314947575331, 'val_acc': 0.6451028, 'roc_auc': 0.7626777256749224}\n",
      "500/500 [==============================] - 134s 268ms/step - loss: 0.5581 - acc: 0.6480 - val_loss: 0.5684 - val_acc: 0.6451\n",
      "Epoch 10/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5595 - acc: 0.6479{'loss': 0.5595634780526161, 'acc': 0.6478307, 'val_loss': 0.5680700577795506, 'val_acc': 0.6450913, 'roc_auc': 0.7626764229395862}\n",
      "500/500 [==============================] - 140s 280ms/step - loss: 0.5596 - acc: 0.6478 - val_loss: 0.5681 - val_acc: 0.6451\n",
      "Epoch 11/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5576 - acc: 0.6460{'loss': 0.5574906002879143, 'acc': 0.6459588, 'val_loss': 0.5691428519785404, 'val_acc': 0.6451028, 'roc_auc': 0.7626939175751133}\n",
      "500/500 [==============================] - 136s 272ms/step - loss: 0.5575 - acc: 0.6460 - val_loss: 0.5691 - val_acc: 0.6451\n",
      "Epoch 12/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5560 - acc: 0.6483{'loss': 0.5560532524585724, 'acc': 0.64833677, 'val_loss': 0.5676681697368622, 'val_acc': 0.6451028, 'roc_auc': 0.7629447254239089}\n",
      "500/500 [==============================] - 136s 273ms/step - loss: 0.5561 - acc: 0.6483 - val_loss: 0.5677 - val_acc: 0.6451\n",
      "Epoch 13/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5559 - acc: 0.6464{'loss': 0.5557876557111741, 'acc': 0.64646333, 'val_loss': 0.5683684833347797, 'val_acc': 0.6451028, 'roc_auc': 0.7628807363746462}\n",
      "500/500 [==============================] - 140s 279ms/step - loss: 0.5558 - acc: 0.6465 - val_loss: 0.5684 - val_acc: 0.6451\n",
      "Epoch 14/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5575 - acc: 0.6478{'loss': 0.5575145449638367, 'acc': 0.6477626, 'val_loss': 0.5668616686016321, 'val_acc': 0.6450913, 'roc_auc': 0.7634331148020155}\n",
      "500/500 [==============================] - 140s 280ms/step - loss: 0.5575 - acc: 0.6478 - val_loss: 0.5669 - val_acc: 0.6451\n",
      "Epoch 15/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5580 - acc: 0.6488{'loss': 0.5581393310427666, 'acc': 0.6487818, 'val_loss': 0.5671973824501038, 'val_acc': 0.6450913, 'roc_auc': 0.7631590393703036}\n",
      "500/500 [==============================] - 139s 277ms/step - loss: 0.5581 - acc: 0.6488 - val_loss: 0.5672 - val_acc: 0.6451\n",
      "Epoch 16/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5606 - acc: 0.6472{'loss': 0.5604050374627113, 'acc': 0.64720935, 'val_loss': 0.5680792946368456, 'val_acc': 0.6450913, 'roc_auc': 0.7625665931082615}\n",
      "500/500 [==============================] - 136s 271ms/step - loss: 0.5604 - acc: 0.6472 - val_loss: 0.5681 - val_acc: 0.6451\n",
      "Epoch 17/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5554 - acc: 0.6487{'loss': 0.5555180376172065, 'acc': 0.6487035, 'val_loss': 0.5669554583728313, 'val_acc': 0.6450913, 'roc_auc': 0.7632755267092408}\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "500/500 [==============================] - 138s 277ms/step - loss: 0.5555 - acc: 0.6487 - val_loss: 0.5670 - val_acc: 0.6451\n",
      "Epoch 18/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5546 - acc: 0.6472{'loss': 0.5547010949254036, 'acc': 0.64723015, 'val_loss': 0.5651601627469063, 'val_acc': 0.6450913, 'roc_auc': 0.7643964525577628}\n",
      "500/500 [==============================] - 140s 280ms/step - loss: 0.5547 - acc: 0.6472 - val_loss: 0.5652 - val_acc: 0.6451\n",
      "Epoch 19/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5533 - acc: 0.6468{'loss': 0.553294525206089, 'acc': 0.64683634, 'val_loss': 0.5649734679609537, 'val_acc': 0.6450913, 'roc_auc': 0.7645356926181553}\n",
      "500/500 [==============================] - 136s 272ms/step - loss: 0.5533 - acc: 0.6468 - val_loss: 0.5650 - val_acc: 0.6451\n",
      "Epoch 20/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5577 - acc: 0.6466{'loss': 0.5577275797724723, 'acc': 0.6464828, 'val_loss': 0.5644466206431389, 'val_acc': 0.6451028, 'roc_auc': 0.7648046743040948}\n",
      "500/500 [==============================] - 139s 278ms/step - loss: 0.5577 - acc: 0.6465 - val_loss: 0.5644 - val_acc: 0.6451\n",
      "Epoch 21/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5542 - acc: 0.6470{'loss': 0.5543552134633064, 'acc': 0.64705664, 'val_loss': 0.5645635165274143, 'val_acc': 0.6450913, 'roc_auc': 0.7649164333343648}\n",
      "500/500 [==============================] - 135s 271ms/step - loss: 0.5544 - acc: 0.6471 - val_loss: 0.5646 - val_acc: 0.6451\n",
      "Epoch 22/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5592 - acc: 0.6472{'loss': 0.5592380945086479, 'acc': 0.6472841, 'val_loss': 0.5641121882945299, 'val_acc': 0.6450913, 'roc_auc': 0.7651583660156807}\n",
      "500/500 [==============================] - 135s 271ms/step - loss: 0.5592 - acc: 0.6473 - val_loss: 0.5641 - val_acc: 0.6451\n",
      "Epoch 23/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5522 - acc: 0.6469{'loss': 0.5521228920817375, 'acc': 0.6469182, 'val_loss': 0.5643454696983099, 'val_acc': 0.6450913, 'roc_auc': 0.7650249923927821}\n",
      "500/500 [==============================] - 136s 272ms/step - loss: 0.5521 - acc: 0.6469 - val_loss: 0.5643 - val_acc: 0.6451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5543 - acc: 0.6469{'loss': 0.554313084423542, 'acc': 0.6468757, 'val_loss': 0.5642851665616035, 'val_acc': 0.6450913, 'roc_auc': 0.7651704989537387}\n",
      "500/500 [==============================] - 133s 267ms/step - loss: 0.5543 - acc: 0.6469 - val_loss: 0.5643 - val_acc: 0.6451\n",
      "Epoch 25/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5576 - acc: 0.6476{'loss': 0.5575913789868355, 'acc': 0.6475551, 'val_loss': 0.5641149878501892, 'val_acc': 0.6450913, 'roc_auc': 0.7653072051925702}\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
      "500/500 [==============================] - 137s 273ms/step - loss: 0.5576 - acc: 0.6476 - val_loss: 0.5641 - val_acc: 0.6451\n",
      "Epoch 26/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5510 - acc: 0.6474{'loss': 0.5510411232709884, 'acc': 0.6474212, 'val_loss': 0.5635127332061529, 'val_acc': 0.6450913, 'roc_auc': 0.765677218518538}\n",
      "500/500 [==============================] - 136s 272ms/step - loss: 0.5510 - acc: 0.6474 - val_loss: 0.5635 - val_acc: 0.6451\n",
      "Epoch 27/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5516 - acc: 0.6465{'loss': 0.5517217084169388, 'acc': 0.6464877, 'val_loss': 0.5635440926998854, 'val_acc': 0.6450913, 'roc_auc': 0.7656532054119646}\n",
      "500/500 [==============================] - 136s 272ms/step - loss: 0.5517 - acc: 0.6465 - val_loss: 0.5635 - val_acc: 0.6451\n",
      "Epoch 28/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5557 - acc: 0.6473{'loss': 0.5558267801403999, 'acc': 0.6472855, 'val_loss': 0.5633954554796219, 'val_acc': 0.6450913, 'roc_auc': 0.7657701348978366}\n",
      "500/500 [==============================] - 139s 277ms/step - loss: 0.5558 - acc: 0.6473 - val_loss: 0.5634 - val_acc: 0.6451\n",
      "Epoch 29/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5521 - acc: 0.6493{'loss': 0.552082189142704, 'acc': 0.64925635, 'val_loss': 0.5632163435220718, 'val_acc': 0.6450913, 'roc_auc': 0.7658835485165159}\n",
      "500/500 [==============================] - 135s 270ms/step - loss: 0.5521 - acc: 0.6493 - val_loss: 0.5632 - val_acc: 0.6451\n",
      "Epoch 30/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5608 - acc: 0.6480{'loss': 0.560875100672245, 'acc': 0.64796793, 'val_loss': 0.5632651709020138, 'val_acc': 0.6450913, 'roc_auc': 0.7658237734471665}\n",
      "500/500 [==============================] - 135s 270ms/step - loss: 0.5609 - acc: 0.6480 - val_loss: 0.5633 - val_acc: 0.6451\n",
      "Epoch 31/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5519 - acc: 0.6480{'loss': 0.5519320772290229, 'acc': 0.6479298, 'val_loss': 0.5630733221769333, 'val_acc': 0.6450913, 'roc_auc': 0.7659523564346613}\n",
      "500/500 [==============================] - 134s 268ms/step - loss: 0.5519 - acc: 0.6479 - val_loss: 0.5631 - val_acc: 0.6451\n",
      "Epoch 32/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5536 - acc: 0.6471{'loss': 0.5535586409568787, 'acc': 0.64708287, 'val_loss': 0.5631023831665516, 'val_acc': 0.6450913, 'roc_auc': 0.7659927044732951}\n",
      "500/500 [==============================] - 137s 275ms/step - loss: 0.5536 - acc: 0.6471 - val_loss: 0.5631 - val_acc: 0.6451\n",
      "Epoch 33/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5537 - acc: 0.6486{'loss': 0.5536745221018792, 'acc': 0.64853567, 'val_loss': 0.5630697105079889, 'val_acc': 0.6450913, 'roc_auc': 0.7659782089790249}\n",
      "500/500 [==============================] - 141s 283ms/step - loss: 0.5537 - acc: 0.6485 - val_loss: 0.5631 - val_acc: 0.6451\n",
      "Epoch 34/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5520 - acc: 0.6450{'loss': 0.55188044911623, 'acc': 0.6451572, 'val_loss': 0.5631084479391575, 'val_acc': 0.6450913, 'roc_auc': 0.7659798811467701}\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07.\n",
      "500/500 [==============================] - 141s 282ms/step - loss: 0.5519 - acc: 0.6452 - val_loss: 0.5631 - val_acc: 0.6451\n",
      "Epoch 35/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5554 - acc: 0.6458{'loss': 0.5554816203713417, 'acc': 0.64582574, 'val_loss': 0.5631003249436617, 'val_acc': 0.6450913, 'roc_auc': 0.7659718005120053}\n",
      "500/500 [==============================] - 139s 278ms/step - loss: 0.5555 - acc: 0.6458 - val_loss: 0.5631 - val_acc: 0.6451\n",
      "Epoch 36/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5533 - acc: 0.6458{'loss': 0.5533937622904778, 'acc': 0.6457364, 'val_loss': 0.5629272069782019, 'val_acc': 0.6450913, 'roc_auc': 0.7660825764312291}\n",
      "500/500 [==============================] - 131s 262ms/step - loss: 0.5534 - acc: 0.6457 - val_loss: 0.5629 - val_acc: 0.6451\n",
      "Epoch 37/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5510 - acc: 0.6483{'loss': 0.5509106442332268, 'acc': 0.64825135, 'val_loss': 0.5629511214792728, 'val_acc': 0.6450913, 'roc_auc': 0.7660723593744375}\n",
      "500/500 [==============================] - 143s 285ms/step - loss: 0.5509 - acc: 0.6483 - val_loss: 0.5630 - val_acc: 0.6451\n",
      "Epoch 38/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5532 - acc: 0.6487{'loss': 0.5532006148099899, 'acc': 0.64866805, 'val_loss': 0.5630045384168625, 'val_acc': 0.6450913, 'roc_auc': 0.7660012011523445}\n",
      "500/500 [==============================] - 141s 283ms/step - loss: 0.5532 - acc: 0.6487 - val_loss: 0.5630 - val_acc: 0.6451\n",
      "Epoch 39/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5554 - acc: 0.6472{'loss': 0.5555955787301063, 'acc': 0.6471843, 'val_loss': 0.5630189180374146, 'val_acc': 0.6450913, 'roc_auc': 0.7660019304284333}\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.600000018697756e-07.\n",
      "500/500 [==============================] - 142s 284ms/step - loss: 0.5556 - acc: 0.6472 - val_loss: 0.5630 - val_acc: 0.6451\n",
      "Epoch 40/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5483 - acc: 0.6494{'loss': 0.5482451331615448, 'acc': 0.6493754, 'val_loss': 0.5629664994776249, 'val_acc': 0.6450913, 'roc_auc': 0.7660368531110938}\n",
      "500/500 [==============================] - 142s 284ms/step - loss: 0.5482 - acc: 0.6494 - val_loss: 0.5630 - val_acc: 0.6451\n",
      "Epoch 41/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5527 - acc: 0.6499{'loss': 0.5526841197609902, 'acc': 0.6499215, 'val_loss': 0.5629648808389902, 'val_acc': 0.6450913, 'roc_auc': 0.7660365851593971}\n",
      "500/500 [==============================] - 142s 284ms/step - loss: 0.5527 - acc: 0.6499 - val_loss: 0.5630 - val_acc: 0.6451\n",
      "Epoch 42/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5539 - acc: 0.6456{'loss': 0.5537684599757194, 'acc': 0.6456373, 'val_loss': 0.5629829540848732, 'val_acc': 0.6450913, 'roc_auc': 0.7660362687313498}\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 3.199999980552093e-08.\n",
      "500/500 [==============================] - 142s 283ms/step - loss: 0.5538 - acc: 0.6456 - val_loss: 0.5630 - val_acc: 0.6451\n",
      "Epoch 43/500\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5554 - acc: 0.6471{'loss': 0.5553271310925484, 'acc': 0.6471256, 'val_loss': 0.562971044331789, 'val_acc': 0.6450913, 'roc_auc': 0.7660411206280757}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "500/500 [==============================] - 142s 285ms/step - loss: 0.5553 - acc: 0.6471 - val_loss: 0.5630 - val_acc: 0.6451\n",
      "Epoch 00043: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b7654c0860>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "early = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=7, verbose=1, \n",
    "                                                mode='auto', restore_best_weights=True)\n",
    "reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1, \n",
    "                                                     mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "\n",
    "# history = CustomCallback(validation = (x_test,  y_test), dico_params = dico_params)#, from_path = './history_sb++/history_epoch_44')\n",
    "\n",
    "# callbacks = [history]\n",
    "callbacks = [AUCCallback(validation=(x_test, y_test), logs = {}), early, reduce]\n",
    "\n",
    "batch_size = 32\n",
    "n_epochs = 500\n",
    "steps_per_epoch = 500\n",
    "\n",
    "model.fit(train_gen, epochs=n_epochs,\n",
    "                    steps_per_epoch = steps_per_epoch, \n",
    "                    validation_data=(x_test,  y_test), \n",
    "                    max_queue_size=20,\n",
    "#                     workers=6,\n",
    "                    callbacks = callbacks,\n",
    "                    verbose = 1\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./tabnetweights/transfor_tabnet_checkpoint/checkpoint_auc_0.676_training_slow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('./weights_saint/saintgpt_8l_36_question_6.76_correct_70.75_auc_75.72.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = DataGenerator(batch_size=1500, max_len = 100, folder = 'user_batch_saint_test', strategy = 'end', mask_rate = 0.15, seq_mask_rate = 1, bidirectionnal = False)\n",
    "x_val, y_val = test_gen[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 6s 4ms/sample\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x_val, verbose = 1, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('./weights_saint/regression_gpt_0.782.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred[:,:,1]\n",
    "true = y_val[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.1211948e-01, 7.3734021e-01, 8.4906143e-01, ..., 7.9688042e-01,\n",
       "        3.7523357e-03, 8.4981734e-01],\n",
       "       [6.8513292e-01, 1.3770592e-02, 8.3314478e-01, ..., 7.6666725e-01,\n",
       "        7.8694075e-01, 7.3627847e-01],\n",
       "       [8.7314904e-01, 6.7786288e-01, 6.7945403e-01, ..., 6.9739890e-01,\n",
       "        7.2291321e-01, 6.7629993e-01],\n",
       "       ...,\n",
       "       [7.9683322e-01, 8.3482569e-01, 6.7364383e-01, ..., 7.6892072e-01,\n",
       "        7.9261053e-01, 8.5245222e-01],\n",
       "       [8.0961227e-01, 7.8024876e-01, 7.9400384e-01, ..., 8.2823658e-01,\n",
       "        7.4703372e-01, 8.6797190e-01],\n",
       "       [7.0255893e-01, 7.0560461e-01, 7.8182685e-01, ..., 1.3260354e-04,\n",
       "        1.6105296e-04, 1.5079365e-04]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 50\n",
    "M = 70\n",
    "\n",
    "y_pred = pred[:,m:M]\n",
    "y_true = true[:,m:M]\n",
    "\n",
    "y_pred = y_pred.reshape(y_true.shape[0] * y_true.shape[1])\n",
    "y_true = y_true.reshape(y_true.shape[0] * y_true.shape[1])\n",
    "\n",
    "y_pred = y_pred[y_true != 2]\n",
    "y_true = y_true[y_true != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25705,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6630616611554172"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_true, (y_pred >= 0.5)*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.761754311890405"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b74127be80>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = []\n",
    "p = []\n",
    "p1 = []\n",
    "for i in range(100):\n",
    "    m = i\n",
    "    M = i+1\n",
    "\n",
    "    y_pred = pred[:,m:M]\n",
    "    y_true = true[:,m:M]\n",
    "\n",
    "    y_pred = y_pred.reshape(y_true.shape[0] * y_true.shape[1])\n",
    "    y_true = y_true.reshape(y_true.shape[0] * y_true.shape[1])\n",
    "\n",
    "    y_pred = y_pred[y_true != 2]\n",
    "    y_true = y_true[y_true != 2]\n",
    "    \n",
    "    s.append(i+1)\n",
    "    p.append(roc_auc_score(y_true, y_pred))\n",
    "    p1.append(accuracy_score(y_true, (y_pred >= 0.5)*1))\n",
    "plt.plot(s, p)\n",
    "# plt.plot(s, p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing embedding with gbdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = DataGenerator(batch_size=1500, max_len = 100, folder = 'user_batch_saint_test', strategy = 'end', mask_rate = 0.15, seq_mask_rate = 1, bidirectionnal = False)\n",
    "x_val, y_val = test_gen[0]\n",
    "\n",
    "# test_gen = DataGenerator(batch_size=64, max_len = max_len, folder = 'user_batch_saint_100', strategy = 'end', mask_rate = 0.15, seq_mask_rate = 1, bidirectionnal = False)\n",
    "# x_val1, y_val1 = test_gen[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = model.inputs\n",
    "out = model.get_layer('tf_op_layer_add').output\n",
    "model1 = Model(inp, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val1, y_val2 = test_gen[0]\n",
    "y_val2 = y_val2\n",
    "emb2 = model1.predict(x_val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = DataGenerator(batch_size=64, max_len = max_len, folder = 'user_batch_saint_100', strategy = 'end', mask_rate = 0.15, seq_mask_rate = 1, bidirectionnal = False)\n",
    "emb1 = None\n",
    "y_val1 = None\n",
    "for i in tqdm(range(100)):\n",
    "    x_val1, y_val2 = test_gen[0]\n",
    "    y_val2 = y_val2\n",
    "    emb2 = model1.predict(x_val1)\n",
    "#     emb2 = np.concatenate([emb2, x_val1[-1]], axis = -1)\n",
    "    if emb1 is not None:\n",
    "        emb1 = np.concatenate([emb1, emb2])\n",
    "        y_val1 = np.concatenate([y_val1, y_val2])\n",
    "    else:\n",
    "        emb1 = deepcopy(emb2)\n",
    "        y_val1 = deepcopy(y_val2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = model1.predict(x_val, verbose = 1)\n",
    "# emb = np.concatenate([emb, x_val[-1]], axis = -1)\n",
    "# emb1 = model1.predict(x_val1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = y_val[:,:,0]/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt =  []\n",
    "yt = []\n",
    "\n",
    "for i, elt in enumerate(tqdm(x_val[0])):\n",
    "    for j, ids in enumerate(elt):\n",
    "        if ids != 1:\n",
    "            if y_val[i,j] < 2:\n",
    "                Xt.append(emb[i,j,:])\n",
    "                yt.append(y_val[i, j])\n",
    "                        \n",
    "Xt = np.array(Xt)\n",
    "yt = np.array(yt)\n",
    "\n",
    "Xv = []\n",
    "yv = []\n",
    "\n",
    "for i, elt in enumerate(tqdm(emb1)):\n",
    "    for j, ids in enumerate(elt):\n",
    "        if y_val1[i,j] < 2:\n",
    "                Xv.append(emb1[i,j,:])\n",
    "                yv.append(y_val1[i, j])\n",
    "\n",
    "Xv = np.array(Xv)\n",
    "yv = np.array(yv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids_to_keep = np.random.choice(list(range(len(Xv))), size = 200000)\n",
    "# Xv = Xv[ids_to_keep]\n",
    "# yv = yv[ids_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xv, yv, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "reducer = umap.UMAP(metric='cosine',\n",
    "     n_components=32, n_neighbors=15, \n",
    "     verbose=True)\n",
    "\n",
    "reducer.fit(X_train[:30000], y_train[:30000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "reducer = PCA(n_components=32)\n",
    "reducer.fit(X_train[:30000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = reducer.transform(X_train)\n",
    "X_test1 = reducer.transform(X_test)\n",
    "Xt1 = reducer.transform(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(reducer, 'umap_reducer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "clf = lgb.LGBMClassifier(max_depth = -1, n_estimators = 500, n_jobs = 12, silent = False, early_stopping_rounds = 15)\n",
    "clf.fit(X_train1, y_train, eval_set =(Xt1, yt), eval_metric = 'auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(Xt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(yt, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict_proba(Xt1)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(yt, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(clf, 'lightgbm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeDataGenerator:\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        self.data will be a dictionnary to iterate over the stored data\n",
    "        self.all_rows will be the rows of the train set that are used by the generato\n",
    "        self.data_index will be all the data available in the dataset        \n",
    "        '''\n",
    "        self.data = None\n",
    "        self.all_rows = None\n",
    "        self.data_index = None\n",
    "        return None\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        sub = sample[['row_id', 'group_num']].copy()\n",
    "        sub['answered_correctly'] = np.zeros(sub.shape[0])+0.5\n",
    "        return (sample, sub)\n",
    "    \n",
    "    \n",
    "    def load(self, save_name):\n",
    "        self.data,self.all_rows = load(save_name)\n",
    "        self.data_index = np.array(list(self.data.keys()))\n",
    "    \n",
    "    def build_from_train(self, train, n_users, beginner_rate = 0.3, save_name = 'fake_train_generator'):\n",
    "        \"\"\"\n",
    "        train will be the training set you loaded\n",
    "        n_users is a number of user from whom you will sample the data\n",
    "        beginner_rate is the rate of these users who will begin their journey during test\n",
    "        save_name : the name under which the item will be saved\n",
    "        \"\"\"\n",
    "        \n",
    "        ## Sampling a restricted list of users\n",
    "        user_list = train['user_id'].unique()\n",
    "        test_user_list = np.random.choice(user_list, size = n_users)\n",
    "        train.index = train['user_id']\n",
    "        test_data_non_filter = train.loc[test_user_list]\n",
    "        test_data_non_filter.index = list(range(test_data_non_filter.shape[0]))\n",
    "        \n",
    "        ## building a dictionnary with all the rows and container id from a user\n",
    "        dico_user = {}\n",
    "        def agg(x):\n",
    "            return [elt for elt in x]\n",
    "        \n",
    "        print(\"Generating user dictionnary\")\n",
    "        for user, frame in tqdm(test_data_non_filter.groupby('user_id'), total =test_data_non_filter['user_id'].nunique()):\n",
    "            if frame.shape[0] > 0:\n",
    "                dico_user[user] = {}\n",
    "\n",
    "                dico_user[user]['min_indice'] = frame['task_container_id'].min()\n",
    "                dico_user[user]['max_indice'] = frame['task_container_id'].max()\n",
    "\n",
    "                r = random.uniform(0,1)\n",
    "                if r < beginner_rate:\n",
    "                    dico_user[user]['current_indice'] = dico_user[user]['min_indice']\n",
    "                else:\n",
    "                    dico_user[user]['current_indice'] = random.randint(dico_user[user]['min_indice'],dico_user[user]['max_indice']-2)\n",
    "\n",
    "                row_ids = frame[['task_container_id','row_id']].groupby('task_container_id').agg(agg)\n",
    "                row_ids = row_ids.to_dict()['row_id']\n",
    "                dico_user[user]['row_ids'] = row_ids\n",
    "\n",
    "        work_dico = deepcopy(dico_user)\n",
    "        \n",
    "        ## Choosing batch_data to generate\n",
    "        work_dico = deepcopy(dico_user)\n",
    "        batches = {}\n",
    "\n",
    "        all_rows = []\n",
    "        batch_number = 0\n",
    "        \n",
    "        print('Creating batches')\n",
    "        while len(work_dico)> 1:\n",
    "\n",
    "            size = random.randint(20,500)\n",
    "            size = min(size, len(work_dico))\n",
    "\n",
    "\n",
    "            batch = []\n",
    "\n",
    "            users = np.random.choice(np.array(list(work_dico.keys())),replace = False,  size = size)\n",
    "\n",
    "            for u in users:\n",
    "                try:\n",
    "                    batch.extend(work_dico[u]['row_ids'][work_dico[u]['current_indice']])\n",
    "                    all_rows.extend(work_dico[u]['row_ids'][work_dico[u]['current_indice']])\n",
    "                    work_dico[u]['current_indice'] += 1\n",
    "                    if work_dico[u]['current_indice'] == work_dico[u]['max_indice']:\n",
    "                        work_dico.pop(u)\n",
    "                except:\n",
    "                    work_dico.pop(u)\n",
    "\n",
    "            batches[batch_number] = batch\n",
    "            batch_number += 1\n",
    "        \n",
    "        ## building data\n",
    "\n",
    "        data = {}\n",
    "        \n",
    "        print(\"Building dataset\")\n",
    "        test_data_non_filter.index = test_data_non_filter['row_id']\n",
    "        for i in tqdm(batches):\n",
    "            current_data = test_data_non_filter.loc[np.array(batches[i])]\n",
    "            current_data['group_num'] = i\n",
    "\n",
    "            current_data['prior_group_answers_correct'] = [np.nan for elt in range(current_data.shape[0])]\n",
    "            current_data['prior_group_responses'] = [np.nan for elt in range(current_data.shape[0])]\n",
    "\n",
    "            if i != 0:\n",
    "                current_data['prior_group_answers_correct'].iloc[0] = saved_correct_answer\n",
    "                current_data['prior_group_responses'].iloc[0] = saved_answer\n",
    "\n",
    "            saved_answer = str(list(current_data[current_data['content_type_id'] == 0]['user_answer'].values))\n",
    "            saved_correct_answer = str(list(current_data[current_data['content_type_id'] == 0]['answered_correctly'].values))\n",
    "            current_data = current_data.drop(columns = ['user_answer', 'answered_correctly'])\n",
    "\n",
    "            data[i] = current_data\n",
    "\n",
    "        save((data,np.array(all_rows)) , save_name)\n",
    "        \n",
    "        self.data = data\n",
    "        self.all_rows = np.array(all_rows)\n",
    "        self.data_index = np.array(list(data.keys()))\n",
    "        print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FakeDataGenerator()\n",
    "# env.build_from_train(train, 15000, beginner_rate = 0.3, save_name = 'fake_train_generator')\n",
    "env.load('fake_train_generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, sub = env[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = load('train_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.index = train['user_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create():\n",
    "    return {\n",
    "                'exercise_id' : np.array([]),\n",
    "                'container_id' : np.array([]),\n",
    "                'timestamp' : np.array([]),\n",
    "                'correctness' : np.array([]),\n",
    "                'answer' : np.array([]), \n",
    "                'elapsed_time' : np.array([]),\n",
    "                'prior_question_had_explanation' : np.array([]),\n",
    "                'lag_time' : np.array([]),\n",
    "                'first_line' : True\n",
    "            }\n",
    "\n",
    "def update_data(data, data_sav, sub_sav):\n",
    "    prior_correct = data['prior_group_answers_correct'].iloc[0]\n",
    "    prior_answer = data['prior_group_responses'].iloc[0]\n",
    "\n",
    "    prior_correct = np.array(prior_correct.replace('[', '').replace(']', '').split(', ')).astype(int)\n",
    "    prior_answer = np.array(prior_answer.replace('[', '').replace(']', '').split(', ')).astype(int)\n",
    "\n",
    "    corr = np.zeros(data_sav.shape[0]) - 1 \n",
    "    ans = np.zeros(data_sav.shape[0]) - 1\n",
    "\n",
    "    boole = data_sav['content_type_id'].values == 0\n",
    "\n",
    "    corr[boole] = prior_correct\n",
    "    ans[boole] = prior_answer\n",
    "\n",
    "    data_sav['answered_correctly'] = corr\n",
    "    data_sav['user_answer'] = ans\n",
    "    sub_sav['answered_correctly_truth'] = corr\n",
    "    \n",
    "    sub_sav = sub_sav[boole]\n",
    "    \n",
    "    return data_sav, sub_sav\n",
    "\n",
    "def update_dico_user(dico_user, data_sav):\n",
    "    data_sav = data_sav.sort_values(by = ['timestamp'])\n",
    "    for i, line in data_sav.iterrows():\n",
    "        user = line['user_id']\n",
    "        exid = line['content_id']\n",
    "        tid = line['content_type_id']\n",
    "        exid = 'q_'+str(exid) if tid == 0 else 'l_' + str(exid)\n",
    "\n",
    "        dico_user[user]['exercise_id'] = np.concatenate([dico_user[user]['exercise_id'], [exid]])\n",
    "        dico_user[user]['container_id'] = np.concatenate([dico_user[user]['container_id'], [line['task_container_id']]])\n",
    "        dico_user[user]['timestamp'] = np.concatenate([dico_user[user]['timestamp'], [line['timestamp']/1000]])\n",
    "        dico_user[user]['correctness'] = np.concatenate([dico_user[user]['correctness'], [line['answered_correctly']]])\n",
    "        dico_user[user]['answer'] = np.concatenate([dico_user[user]['answer'], [line['user_answer']]])\n",
    "\n",
    "        ## two other depend on if this is the first line\n",
    "        if dico_user[user]['first_line']:\n",
    "            dico_user[user]['first_line'] = False\n",
    "            dico_user[user]['lag_time'] = np.concatenate([dico_user[user]['lag_time'], [0]])\n",
    "\n",
    "        else:\n",
    "            el = line['prior_question_elapsed_time']\n",
    "            if str(el) == 'nan':\n",
    "                el = 0\n",
    "            dico_user[user]['elapsed_time'] = np.concatenate([dico_user[user]['elapsed_time'], [el]])\n",
    "\n",
    "            pr = line['prior_question_had_explanation']\n",
    "            if str(pr) == 'nan':\n",
    "                pr = 0\n",
    "            pr = pr*1\n",
    "            dico_user[user]['prior_question_had_explanation'] = np.concatenate([dico_user[user]['prior_question_had_explanation'], [pr]])\n",
    "\n",
    "            lag = dico_user[user]['timestamp'][-1] - dico_user[user]['timestamp'][-2] + el\n",
    "            if lag < 0:\n",
    "                lag = 0\n",
    "            dico_user[user]['lag_time'] = np.concatenate([dico_user[user]['lag_time'], [lag]])\n",
    "    return dico_user\n",
    "\n",
    "\n",
    "dico_question = load('dico_questions_mean')\n",
    "dico_utags, dico_gtags, dico_parts = load('dico_tags')\n",
    "timestamp_enc, elapsed_enc,lag_time_enc, qmean_enc = load('discrete_encoders')\n",
    "tokenizer = load('tokenizer')\n",
    "\n",
    "def map_part( ids):\n",
    "    def replace_dico_part(x):\n",
    "        try:\n",
    "            return dico_parts[x]\n",
    "        except:\n",
    "            return 0\n",
    "    return np.array(list(map(replace_dico_part,ids)))\n",
    "\n",
    "def map_utags( ids):\n",
    "    def replace_dico_utags(x):\n",
    "        try:\n",
    "            if str(dico_utags[x]) != 'nan':\n",
    "                return str(self.dico_utags[x])\n",
    "            else:\n",
    "                return 0\n",
    "        except:\n",
    "            return 0\n",
    "    return np.array(list(map(replace_dico_utags,ids)))\n",
    "\n",
    "def map_gtags( ids):\n",
    "    def replace_dico_gtags(x):\n",
    "        try:\n",
    "            if str(dico_gtags[x]) != 'nan':\n",
    "                return str(dico_gtags[x])\n",
    "            else:\n",
    "                return 0\n",
    "        except:\n",
    "            return 0\n",
    "    return np.array(list(map(replace_dico_gtags,ids)))\n",
    "\n",
    "def map_mean(ids):\n",
    "    def replace_dico_question(x):\n",
    "        try:\n",
    "            return dico_question[x]\n",
    "        except:\n",
    "            return 0.5\n",
    "    return np.array(list(map(replace_dico_question,ids)))\n",
    "\n",
    "def remove_na(x):\n",
    "    x = np.array(list(x))\n",
    "    x[np.isnan(x)] = 0\n",
    "    return x\n",
    "\n",
    "def build_sequence(user_history, new_inputs, max_len = 128):\n",
    "    ## new input : (exercise_id, timestamp, elapsed)\n",
    "    \n",
    "    dico_sequence = deepcopy(user_history)        \n",
    "    dico_sequence['elapsed_time'] = remove_na(dico_sequence['elapsed_time'])\n",
    "    dico_sequence['lag_time'] = remove_na(dico_sequence['lag_time'])\n",
    "    dico_sequence['prior_question_had_explanation'] = remove_na(dico_sequence['prior_question_had_explanation'])\n",
    "\n",
    "    dico_sequence['elapsed_time'] = np.concatenate([dico_sequence['elapsed_time'], [0]])\n",
    "    dico_sequence['prior_question_had_explanation'] = np.concatenate([dico_sequence['prior_question_had_explanation'], [0]])\n",
    "\n",
    "\n",
    "    ## Cut sequence\n",
    "    for elt in dico_sequence:\n",
    "        if elt != 'first_line':\n",
    "            dico_sequence[elt] = dico_sequence[elt][-(max_len-1):]\n",
    "        \n",
    "    ## Adding new elements\n",
    "    dico_sequence['exercise_id'] = np.concatenate([dico_sequence['exercise_id'], [new_inputs[0]]])\n",
    "    dico_sequence['timestamp'] = np.concatenate([dico_sequence['timestamp'], [new_inputs[1]]])\n",
    "    try:\n",
    "        lag = dico_sequence['timestamp'][-1] - dico_sequence['timestamp'][-2] + new_inputs[2]\n",
    "    except:\n",
    "        lag = 0\n",
    "    if lag < 0:\n",
    "        lag = 0\n",
    "    dico_sequence['lag_time'] = np.concatenate([dico_sequence['lag_time'], [lag]])\n",
    "    query_id = len(dico_sequence['exercise_id']) - 1\n",
    "    \n",
    "    ## Pad sequence\n",
    "    pad_tokens = ['[PAD]', 0, 0, -1, -1, 0, 0, 0, '[PAD]', -1, -1]\n",
    "    for j, elt in enumerate(dico_sequence):\n",
    "        if elt != 'first_line':\n",
    "            size = len(dico_sequence[elt])\n",
    "            if size <= max_len:\n",
    "                adding = max_len - size\n",
    "                tok = pad_tokens[j]\n",
    "                if type(tok) == str:\n",
    "                    add = np.array([tok for elt in range(adding)])\n",
    "                else:\n",
    "                    add = np.zeros(adding) + tok\n",
    "                dico_sequence[elt] = np.concatenate([dico_sequence[elt], add], axis = 0)\n",
    "#                 print(dico_sequence[elt].shape)\n",
    "    lags =  lag_time_enc.transform(dico_sequence['lag_time'])\n",
    "    lags[lags<0] = 0\n",
    "    \n",
    "    input_vals = [\n",
    "        dico_sequence['exercise_id'],\n",
    "        map_part(dico_sequence['exercise_id']),\n",
    "        map_utags(dico_sequence['exercise_id']),\n",
    "        map_gtags(dico_sequence['exercise_id']),\n",
    "        timestamp_enc.transform(dico_sequence['timestamp']),\n",
    "        qmean_enc.transform(map_mean(dico_sequence['exercise_id'])),\n",
    "\n",
    "        np.concatenate([[0], dico_sequence['correctness'] + 1])[:-1],\n",
    "        np.concatenate([[0], dico_sequence['answer'] + 1])[:-1],\n",
    "        np.concatenate([[0], elapsed_enc.transform(dico_sequence['elapsed_time'])])[:-1],\n",
    "        lags,\n",
    "        np.concatenate([[0], dico_sequence['prior_question_had_explanation']])[:-1],\n",
    "    ]\n",
    "    return input_vals, query_id\n",
    "\n",
    "def initiate_dico(batch_size, max_len = 128):\n",
    "    list_encoder = ['exercise', 'part', 'utag', 'gtag', 'timestamp', 'question_mean']\n",
    "    list_decoder = ['correct', 'answer', 'elapsed_time', 'lag_time', 'was_explained']\n",
    "    list_output = ['exercise', 'answer', 'correct']\n",
    "\n",
    "    dico_input = {}\n",
    "    for elt in list_encoder + list_decoder:\n",
    "        if elt == 'exercise':\n",
    "            dico_input[elt] = np.zeros((batch_size, max_len)).astype(str)\n",
    "        else:\n",
    "            dico_input[elt] = np.zeros((batch_size, max_len)).astype('int32')\n",
    "    return dico_input\n",
    "\n",
    "def update_dico(dico_input, input_vals, i):\n",
    "    list_encoder = ['exercise', 'part', 'utag', 'gtag', 'timestamp', 'question_mean']\n",
    "    list_decoder = ['correct', 'answer', 'elapsed_time', 'lag_time', 'was_explained']\n",
    "    list_output = ['exercise', 'answer', 'correct']\n",
    "\n",
    "    for j, elt in enumerate(list_encoder + list_decoder):\n",
    "        dico_input[elt][i] = input_vals[j]\n",
    "    return dico_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dico_user = {}\n",
    "data_sav = None\n",
    "count = 0\n",
    "\n",
    "all_sub = None\n",
    "\n",
    "for i in tqdm(env.data_index):\n",
    "    data, sub = env[i]\n",
    "    \n",
    "    for elt in data['user_id'].unique():\n",
    "        # Loading every piece of information available from past\n",
    "        if not(elt in dico_user):\n",
    "            try:\n",
    "#                 print(elt)\n",
    "                dico_user[elt] = load(str(elt), 'ind_user')\n",
    "                dico_user[elt]['first_line'] = False\n",
    "            except:\n",
    "                dico_user[elt] = create()\n",
    "        \n",
    "    ## Updating data_sav with the new informations\n",
    "    if count != 0:\n",
    "        ## Include values in the mix\n",
    "        data_sav, sub_sav = update_data(data, data_sav, sub_sav)\n",
    "        \n",
    "        if all_sub is not None:\n",
    "            all_sub = pd.concat([all_sub, sub_sav])\n",
    "        else:\n",
    "            all_sub = sub_sav.copy()\n",
    "        print(roc_auc_score(all_sub['answered_correctly_truth'], all_sub['answered_correctly']))\n",
    "        \n",
    "        ## Update dictionnary with data of previous batch\n",
    "        dico_user = update_dico_user(dico_user, data_sav)\n",
    "    \n",
    "    ## Build input for the deep learning model\n",
    "    dico_input = initiate_dico(data.shape[0])\n",
    "    \n",
    "    data.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    query_ids = []\n",
    "    for i, line in data.iterrows():\n",
    "        user = line['user_id']\n",
    "        exid = line['content_id']\n",
    "        tid = line['content_type_id']\n",
    "        exid = 'q_'+str(exid) if tid == 0 else 'l_' + str(exid)\n",
    "        \n",
    "        t = line['timestamp'] / 1000\n",
    "        el = line['prior_question_elapsed_time'] / 1000\n",
    "        \n",
    "        input_vals, query_id = build_sequence(dico_user[user], (exid, t, el))\n",
    "        query_ids.append(query_id)\n",
    "        dico_input = update_dico(dico_input, input_vals, i)\n",
    "    \n",
    "    x = deepcopy(dico_input['exercise'])\n",
    "    dico_input['exercise'] = np.array(tokenizer.texts_to_sequences([\" \".join(list(x)[elt]) for elt in range(len(x))]))\n",
    "    \n",
    "    X = list(np.array(list(dico_input.values())).astype('int32'))\n",
    "    \n",
    "    ## Lgbm variant\n",
    "    predicted = model1.predict(X)\n",
    "    X1 = []\n",
    "    \n",
    "    print(query_ids)\n",
    "    \n",
    "    for i, j in enumerate(query_ids):\n",
    "        X1.append(predicted[i,j,:])\n",
    "    X1 = np.array(X1)\n",
    "    p = clf.predict_proba(X1)[:,1]\n",
    "    \n",
    "    ## Deep Variant\n",
    "#     p1 = model.predict(X)[2][:,:,2]\n",
    "    \n",
    "#     p = []\n",
    "#     for i, j in enumerate(query_ids):\n",
    "#         p.append(p1[i,j])\n",
    "    \n",
    "    sub['answered_correctly'] = p\n",
    "                \n",
    "    data_sav = data.copy()\n",
    "    sub_sav = sub.copy()\n",
    "    count += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self,batch_size=32, max_len = 128, folder = 'user_batch_saint_100', strategy = 'begin', mask_rate = 0.15, seq_mask_rate = 0.5, bidirectionnal = True):\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = load('tokenizer')\n",
    "        self.max_len = max_len\n",
    "        self.folder = folder\n",
    "        self.dico_question = load('dico_questions_mean')\n",
    "        self.dico_utags, self.dico_gtags, self.dico_parts = load('dico_tags')\n",
    "        self.timestamp_enc, self.elapsed_enc,self.lag_time_enc, self.qmean_enc = load('discrete_encoders')\n",
    "        self.strategy = strategy\n",
    "        self.mask_rate = mask_rate\n",
    "        self.seq_mask_rate = seq_mask_rate\n",
    "        self.bidirectionnal = bidirectionnal\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 1000000\n",
    "    \n",
    "    def initiate_dico(self):\n",
    "        list_encoder = ['exercise', 'part', 'utag', 'gtag', 'timestamp', 'question_mean']\n",
    "        list_decoder = ['correct', 'answer', 'elapsed_time', 'lag_time', 'was_explained']\n",
    "        list_output = ['exercise', 'answer', 'correct']\n",
    "        \n",
    "        dico_input = {}\n",
    "        for elt in list_encoder + list_decoder:\n",
    "            if elt == 'exercise':\n",
    "                dico_input[elt] = np.zeros((self.batch_size, self.max_len)).astype(str)\n",
    "            else:\n",
    "                dico_input[elt] = np.zeros((self.batch_size, self.max_len)).astype('int32')\n",
    "        \n",
    "        dico_output = {}\n",
    "        for elt in list_output:\n",
    "            if elt == 'exercise':\n",
    "                dico_output[elt] = np.zeros((self.batch_size, self.max_len)).astype(str)\n",
    "            else:\n",
    "                dico_output[elt] = np.zeros((self.batch_size, self.max_len)).astype('int32')\n",
    "        return dico_input, dico_output\n",
    "\n",
    "    def map_part(self, ids):\n",
    "        def replace_dico_part(x):\n",
    "            try:\n",
    "                return self.dico_parts[x]\n",
    "            except:\n",
    "                return 0\n",
    "        return np.array(list(map(replace_dico_part,ids)))\n",
    "    \n",
    "    def map_utags(self, ids):\n",
    "        def replace_dico_utags(x):\n",
    "            try:\n",
    "                if str(self.dico_utags[x]) != 'nan':\n",
    "                    return str(self.dico_utags[x])\n",
    "                else:\n",
    "                    return 0\n",
    "            except:\n",
    "                return 0\n",
    "        return np.array(list(map(replace_dico_utags,ids)))\n",
    "    \n",
    "    def map_gtags(self, ids):\n",
    "        def replace_dico_gtags(x):\n",
    "            try:\n",
    "                if str(self.dico_gtags[x]) != 'nan':\n",
    "                    return str(self.dico_gtags[x])\n",
    "                else:\n",
    "                    return 0\n",
    "            except:\n",
    "                return 0\n",
    "        return np.array(list(map(replace_dico_gtags,ids)))\n",
    "    \n",
    "    def map_mean(self, ids):\n",
    "        def replace_dico_question(x):\n",
    "            try:\n",
    "                return self.dico_question[x]\n",
    "            except:\n",
    "                return 0.5\n",
    "        return np.array(list(map(replace_dico_question,ids)))\n",
    "\n",
    "\n",
    "    \n",
    "    def update_dico(self, dico_input, dico_output, input_vals, output_vals, i):\n",
    "        list_encoder = ['exercise', 'part', 'utag', 'gtag', 'timestamp', 'question_mean']\n",
    "        list_decoder = ['correct', 'answer', 'elapsed_time', 'lag_time', 'was_explained']\n",
    "        list_output = ['exercise', 'answer', 'correct']\n",
    "        \n",
    "        for j, elt in enumerate(list_encoder + list_decoder):\n",
    "            dico_input[elt][i] = input_vals[j]\n",
    "        \n",
    "        for j, elt in enumerate(list_output):\n",
    "            dico_output[elt][i] = output_vals[j]\n",
    "        return dico_input, dico_output\n",
    "\n",
    "    def remove_na(self, x):\n",
    "        x = np.array(list(x))\n",
    "        x[np.isnan(x)] = 0\n",
    "        return x\n",
    "    \n",
    "    def apply_mask(self, x, mask, pad_token, mask_token):\n",
    "        x_out = []\n",
    "        x_in = []\n",
    "        for i, elt in enumerate(mask):\n",
    "            if mask[i] == 1:\n",
    "                x_out.append(x[i])\n",
    "                x_in.append(mask_token)\n",
    "            else:\n",
    "                x_out.append(pad_token)\n",
    "                x_in.append(x[i])\n",
    "        return np.array(x_in), np.array(x_out)\n",
    "\n",
    "    def build_sequence(self, user_history):\n",
    "        dico_sequence = deepcopy(user_history)        \n",
    "        dico_sequence['elapsed_time'] = self.remove_na(dico_sequence['elapsed_time'])\n",
    "        dico_sequence['lag_time'] = self.remove_na(dico_sequence['lag_time'])\n",
    "        dico_sequence['prior_question_had_explanation'] = self.remove_na(dico_sequence['prior_question_had_explanation'])\n",
    "        \n",
    "        dico_sequence['elapsed_time'] = np.concatenate([dico_sequence['elapsed_time'], [0]])\n",
    "        dico_sequence['prior_question_had_explanation'] = np.concatenate([dico_sequence['prior_question_had_explanation'], [0]])\n",
    "        \n",
    "        \n",
    "        ## Cut sequence\n",
    "        if self.strategy == 'begin':\n",
    "            for elt in dico_sequence:\n",
    "                dico_sequence[elt] = dico_sequence[elt][:self.max_len]\n",
    "        else:\n",
    "            for elt in dico_sequence:\n",
    "                dico_sequence[elt] = dico_sequence[elt][-self.max_len:]\n",
    "        \n",
    "        \n",
    "        \n",
    "         ## Masking\n",
    "        # Either mask question => mask parts, qmean, answer, correctness 0.5%\n",
    "        # Or mask correctness => mask answer, elapsed_time, lag_time, explanation 0.5%\n",
    "        # In all case, mask the last question answer, but we keep its signification\n",
    "        \n",
    "        if self.bidirectionnal == True:\n",
    "            r = random.uniform(0,1)\n",
    "            if r < self.seq_mask_rate:\n",
    "                # masking on the question_id\n",
    "                masks = np.random.choice([0,1],replace = True, size = len(dico_sequence['exercise_id'])-1, p = [1-self.mask_rate,self.mask_rate])\n",
    "\n",
    "    #             print(masks.shape)\n",
    "    #             print(dico_sequence['elapsed_time'].shape)\n",
    "    #             print('\\n')\n",
    "\n",
    "                dico_sequence['elapsed_time'], _ = self.apply_mask(dico_sequence['elapsed_time'], masks, 0, 0)     \n",
    "                dico_sequence['prior_question_had_explanation'], _ = self.apply_mask(dico_sequence['prior_question_had_explanation'], masks, 0, 0) \n",
    "\n",
    "                masks = np.concatenate([masks, [0]])\n",
    "                dico_sequence['exercise_id'], dico_sequence['exercise_id_out'] = self.apply_mask(dico_sequence['exercise_id'], masks, '[PAD]', '[MASK]')            \n",
    "                masks[-1] = 1\n",
    "                dico_sequence['answer'], dico_sequence['answer_out'] = self.apply_mask(dico_sequence['answer'], masks, -1, -1)\n",
    "                dico_sequence['correctness'], dico_sequence['correctness_out'] = self.apply_mask(dico_sequence['correctness'], masks, -1, -1)\n",
    "\n",
    "            else:\n",
    "                # Masking only a part of the answers\n",
    "                dico_sequence['exercise_id_out'] = deepcopy(np.array(['[PAD]' for elt in dico_sequence['exercise_id']]))\n",
    "                masks = np.random.choice([0,1],replace = True, size = len(dico_sequence['correctness'])-1, p = [1-self.mask_rate,self.mask_rate])\n",
    "\n",
    "    #             print(masks.shape)\n",
    "    #             print(dico_sequence['elapsed_time'].shape)\n",
    "    #             print('\\n')\n",
    "\n",
    "                dico_sequence['elapsed_time'], _ = self.apply_mask(dico_sequence['elapsed_time'], masks, 0, 0)    \n",
    "                dico_sequence['prior_question_had_explanation'], _ = self.apply_mask(dico_sequence['prior_question_had_explanation'], masks, 0, 0)\n",
    "\n",
    "                masks = np.concatenate([masks, [1]])\n",
    "                dico_sequence['correctness'], dico_sequence['correctness_out'] = self.apply_mask(dico_sequence['correctness'], masks, -1, -1)\n",
    "                dico_sequence['answer'], dico_sequence['answer_out'] = self.apply_mask(dico_sequence['answer'], masks, -1, -1)\n",
    "        \n",
    "        else:\n",
    "            dico_sequence['exercise_id_out'] = dico_sequence['exercise_id']\n",
    "            dico_sequence['correctness_out'] = dico_sequence['correctness']\n",
    "            dico_sequence['answer_out'] = dico_sequence['answer']\n",
    "        \n",
    "        ## Pad sequence\n",
    "        pad_tokens = ['[PAD]', 0, 0, -1, -1, 0, 0, 0, '[PAD]', -1, -1]\n",
    "        for j, elt in enumerate(dico_sequence):\n",
    "            size = len(dico_sequence[elt])\n",
    "            if size <= self.max_len:\n",
    "                adding = self.max_len - size\n",
    "                tok = pad_tokens[j]\n",
    "                if type(tok) == str:\n",
    "                    add = np.array([tok for elt in range(adding)])\n",
    "                else:\n",
    "                    add = np.zeros(adding) + tok\n",
    "                dico_sequence[elt] = np.concatenate([dico_sequence[elt], add], axis = 0)\n",
    "#                 print(dico_sequence[elt].shape)\n",
    "\n",
    "        if self.bidirectionnal == False:\n",
    "            input_vals = [\n",
    "                dico_sequence['exercise_id'],\n",
    "                self.map_part(dico_sequence['exercise_id']),\n",
    "                self.map_utags(dico_sequence['exercise_id']),\n",
    "                self.map_gtags(dico_sequence['exercise_id']),\n",
    "                self.timestamp_enc.transform(dico_sequence['timestamp']),\n",
    "                self.qmean_enc.transform(self.map_mean(dico_sequence['exercise_id'])),\n",
    "\n",
    "                np.concatenate([[0], dico_sequence['correctness'] + 1])[:-1],\n",
    "                np.concatenate([[0], dico_sequence['answer'] + 1])[:-1],\n",
    "                np.concatenate([[0], self.elapsed_enc.transform(dico_sequence['elapsed_time'])])[:-1],\n",
    "                self.lag_time_enc.transform(dico_sequence['lag_time']),\n",
    "                np.concatenate([[0], dico_sequence['prior_question_had_explanation']])[:-1],\n",
    "            ]\n",
    "\n",
    "            output_vals = [\n",
    "                np.concatenate([dico_sequence['exercise_id_out'][1:], ['[PAD]']]),\n",
    "                dico_sequence['answer_out'] + 1,\n",
    "                dico_sequence['correctness_out'] + 1,\n",
    "            ]\n",
    "            \n",
    "        else:\n",
    "            input_vals = [\n",
    "                dico_sequence['exercise_id'],\n",
    "                self.map_part(dico_sequence['exercise_id']),\n",
    "                self.map_utags(dico_sequence['exercise_id']),\n",
    "                self.map_gtags(dico_sequence['exercise_id']),\n",
    "                self.timestamp_enc.transform(dico_sequence['timestamp']),\n",
    "                self.qmean_enc.transform(self.map_mean(dico_sequence['exercise_id'])),\n",
    "\n",
    "                dico_sequence['correctness'] + 1,\n",
    "                dico_sequence['answer'] + 1,\n",
    "                self.elapsed_enc.transform(dico_sequence['elapsed_time']),\n",
    "                self.lag_time_enc.transform(dico_sequence['lag_time']),\n",
    "                dico_sequence['prior_question_had_explanation'],\n",
    "            ]\n",
    "\n",
    "            output_vals = [\n",
    "                dico_sequence['exercise_id_out'],\n",
    "                dico_sequence['answer_out'] + 1,\n",
    "                dico_sequence['correctness_out'] + 1,\n",
    "            ]\n",
    "        \n",
    "        \n",
    "#         x = np.zeros((11,self.max_len))\n",
    "#         y = np.zeros((3, self.max_len))\n",
    "        return input_vals,output_vals\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ## Load random batch\n",
    "        file_name = random.choice(os.listdir('./'+self.folder))\n",
    "        dico_user = load(file_name.split('.')[0], self.folder)\n",
    "        \n",
    "        list_user = np.random.choice(list(dico_user.keys()), size = self.batch_size)\n",
    "        \n",
    "        dico_input, dico_output = self.initiate_dico()\n",
    "        \n",
    "        \n",
    "        for i, elt in enumerate(list_user):\n",
    "            user_history = dico_user[elt]\n",
    "            input_vals, output_vals = self.build_sequence(user_history)\n",
    "            dico_input, dico_output = self.update_dico(dico_input, dico_output, input_vals, output_vals, i)\n",
    "        \n",
    "        x = deepcopy(dico_input['exercise'])\n",
    "        dico_input['exercise'] = np.array(self.tokenizer.texts_to_sequences([\" \".join(list(x)[elt]) for elt in range(len(x))]))\n",
    "        \n",
    "        x = deepcopy(dico_output['exercise'])\n",
    "        dico_output['exercise'] = np.array(self.tokenizer.texts_to_sequences([\" \".join(list(x)[elt]) for elt in range(len(x))]))\n",
    "        \n",
    "        X = list(np.array(list(dico_input.values())).astype('int32'))\n",
    "        y = list(np.array(list(dico_output.values())).astype('int32')) \n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def __get_data(self, batch):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_sequence(df_user):\n",
    "    import numpy as np\n",
    "    df_user =  df_user.sort_values(by = 'timestamp')\n",
    "    df_user.index = list(range(df_user.shape[0]))\n",
    "    \n",
    "    df_user['content_type'] =  df_user['content_type_id'].apply(lambda x : 'q' if x == 0 else 'l')\n",
    "    df_user['content_seq'] = df_user['content_type'].astype(str) + '_' + df_user['content_id'].astype(str)\n",
    "    \n",
    "    ## Encoder\n",
    "    exercise_id = df_user['content_seq'].values\n",
    "    container_id = df_user['task_container_id'].values\n",
    "    timestamp = df_user['timestamp'].values/1000  ## Conversion in s\n",
    "    \n",
    "    ## Decoder\n",
    "    correctness = df_user['answered_correctly'].values\n",
    "    answer = df_user['user_answer'].values\n",
    "    \n",
    "    elapsed_time = df_user['prior_question_elapsed_time'].fillna(0).values[1:]/1000 ## Already Padded ## Conversion in s\n",
    "    prior_question_had_explanation = df_user['prior_question_had_explanation'].fillna(0).values[1:]*1 ## Already Padded\n",
    "    \n",
    "    lag_time = np.concatenate([[0],timestamp[1:] - timestamp[:-1] + elapsed_time])\n",
    "    \n",
    "    dico = {\n",
    "        'exercise_id' : exercise_id,\n",
    "        'container_id' : container_id,\n",
    "        'timestamp' : timestamp,\n",
    "        'correctness' : correctness,\n",
    "        'answer' : answer, \n",
    "        'elapsed_time' : elapsed_time,\n",
    "        'prior_question_had_explanation' : prior_question_had_explanation,\n",
    "        'lag_time' : lag_time\n",
    "    }\n",
    "    return dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "b = 0\n",
    "for elt in tqdm(dico_user.keys()):\n",
    "    c = dico_user[elt]\n",
    "    if len(c['exercise_id']) <= 1:\n",
    "        a += 1\n",
    "    else:\n",
    "        b+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ameliorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add context on lecture and tasks\n",
    "\n",
    "cluster lecture and tasks\n",
    "\n",
    "give average score of a given task\n",
    "\n",
    "enhance test set with train set (optimization constraint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
