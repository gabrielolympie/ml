{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import _pickle as pickle\n",
    "\n",
    "## Some useful functions to ease the processings\n",
    "def save(file,name, folder = \"\"):\n",
    "    if folder != \"\":\n",
    "        outfile = open('./'+folder+'/'+name+'.pickle', 'wb')\n",
    "    else:\n",
    "        outfile = open(name+'.pickle', 'wb')\n",
    "    pickle.dump(file, outfile)\n",
    "    outfile.close\n",
    "    \n",
    "def load(name, folder = \"\"):\n",
    "    if folder != \"\":\n",
    "        outfile = open('./'+folder+'/'+name+'.pickle', 'rb')\n",
    "    else:\n",
    "        outfile = open(name+'.pickle', 'rb')\n",
    "    file = pickle.load(outfile)\n",
    "    outfile.close\n",
    "    return file\n",
    "\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, TFBertForQuestionAnswering, TFBertModel, TFBertForNextSentencePrediction\n",
    "\n",
    "from tf_transformers import *\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.metrics import sparse_categorical_accuracy\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tf_transformers import *\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, TimeDistributed, LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "END_TOKENS = ['.', '!', '?', '...', \"'\", \"`\", '\"', \")\"]\n",
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "def read_text_file(text_file):\n",
    "    lines = []\n",
    "    with open(text_file, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            lines.append(line.strip())\n",
    "    return lines\n",
    "\n",
    "\n",
    "def hashhex(s):\n",
    "    \"\"\"Returns a heximal formated SHA1 hash of the input string.\"\"\"\n",
    "    h = hashlib.sha1()\n",
    "    h.update(s)\n",
    "    return h.hexdigest()\n",
    "\n",
    "\n",
    "def get_url_hashes(url_list):\n",
    "    return [hashhex(url) for url in url_list]\n",
    "\n",
    "\n",
    "def fix_missing_period(line):\n",
    "    \"\"\"Adds a period to a line that is missing a period\"\"\"\n",
    "    if \"@highlight\" in line: \n",
    "        return line\n",
    "    if line==\"\": \n",
    "        return line\n",
    "    if line[-1] in END_TOKENS: \n",
    "        return line\n",
    "  # print line[-1]\n",
    "    return line + \" .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_art_abs(story_file):\n",
    "    lines = read_text_file(story_file)\n",
    "\n",
    "  # Lowercase everything\n",
    "    lines = [line.lower() for line in lines]\n",
    "\n",
    "  # Put periods on the ends of lines that are missing them (this is a problem in the dataset because many image captions don't end in periods; consequently they end up in the body of the article as run-on sentences)\n",
    "    lines = [fix_missing_period(line) for line in lines]\n",
    "\n",
    "  # Separate out article and abstract sentences\n",
    "    article_lines = []\n",
    "    highlights = []\n",
    "    next_is_highlight = False\n",
    "    for idx,line in enumerate(lines):\n",
    "        if line == \"\":\n",
    "            continue # empty line\n",
    "        elif line.startswith(\"@highlight\"):\n",
    "            next_is_highlight = True\n",
    "        elif next_is_highlight:\n",
    "            highlights.append(line)\n",
    "        else:\n",
    "            article_lines.append(line)\n",
    "\n",
    "  # Make article into a single string\n",
    "    article = ' '.join(article_lines)\n",
    "\n",
    "  # Make abstract into a signle string, putting <s> and </s> tags around the sentences\n",
    "    abstract = ' '.join([\"%s %s %s\" % (SENTENCE_START, sent, SENTENCE_END) for sent in highlights])\n",
    "\n",
    "    return article, abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir1 = 'cnn_stories_tokenized'\n",
    "\n",
    "ar = list(np.zeros(len(os.listdir(dir1))))\n",
    "ab = list(np.zeros(len(os.listdir(dir1))))\n",
    "for i, elt in tqdm(enumerate(os.listdir(dir1)), total = len(os.listdir(dir1))):\n",
    "    x, abstract = get_art_abs('./'+dir1+'/'+elt)\n",
    "    \n",
    "    x = x.replace('-lrb- cnn -rrb-','')\n",
    "    x = x.replace('-lrb-','')\n",
    "    x = x.replace('-rrb-','')\n",
    "    \n",
    "    ar[i] = x\n",
    "    ab[i] = abstract.replace('<s>', '').replace('</s>', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir1 = 'dm_stories_tokenized'\n",
    "\n",
    "ar1 = list(np.zeros(len(os.listdir(dir1))))\n",
    "ab1 = list(np.zeros(len(os.listdir(dir1))))\n",
    "for i, elt in tqdm(enumerate(os.listdir(dir1)), total = len(os.listdir(dir1))):\n",
    "    x, abstract = get_art_abs('./'+dir1+'/'+elt)\n",
    "    \n",
    "    x = x.replace('-lrb- cnn -rrb-','')\n",
    "    x = x.replace('-lrb-','')\n",
    "    x = x.replace('-rrb-','')\n",
    "    \n",
    "    ar1[i] = x\n",
    "    ab1[i] = abstract.replace('<s>', '').replace('</s>', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar.extend(ar1)\n",
    "ab.extend(ab1)\n",
    "\n",
    "df = pd.DataFrame({'article' : ar, 'abstract' : ab})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(df, 'cnn_dm_raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load('cnn_dm_raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "max_length_in = 512\n",
    "max_length_out = 129\n",
    "\n",
    "X = list(np.zeros(df.shape[0]))\n",
    "X_masks = list(np.zeros(df.shape[0]))\n",
    "               \n",
    "Y_in = list(np.zeros(df.shape[0]))\n",
    "Y_in_masks = list(np.zeros(df.shape[0]))\n",
    "\n",
    "Y_out = list(np.zeros(df.shape[0]))\n",
    "Y_out_masks = list(np.zeros(df.shape[0]))\n",
    "               \n",
    "text_pairs = []\n",
    "for index, line in tqdm(df.iterrows(), total = df.shape[0]):\n",
    "    s1 = line['article']\n",
    "    s2 = line['abstract']\n",
    "    \n",
    "    s1 = s1.replace('.', '[SEP] [CLS]')\n",
    "    \n",
    "    tokenized = tokenizer.encode_plus(str(s1), add_special_tokens = True, max_length = max_length_in, pad_to_max_length = True)\n",
    "    answer = tokenizer.encode_plus(str(s2), add_special_tokens = True, max_length = max_length_out, pad_to_max_length = True)\n",
    "    \n",
    "    X[index] = tokenized['input_ids']\n",
    "    X_masks[index] = tokenized['attention_mask']\n",
    "               \n",
    "    Y_in[index] = answer['input_ids'][:max_length_out]\n",
    "    Y_in_masks[index] = answer['attention_mask'][:max_length_out]\n",
    "    \n",
    "    Y_out[index] = answer['input_ids'][1:]\n",
    "    Y_out_masks[index] = answer['attention_mask'][1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['art'] = X\n",
    "df['art_mask'] = X_masks\n",
    "\n",
    "df['input'] = Y_in\n",
    "df['input_masks'] = Y_in_masks\n",
    "\n",
    "df['output'] = Y_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(df, 'cnn_dm_refined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load('cnn_dm_refined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([list(elt) for elt in df['art'].values]).astype(int)\n",
    "# X_masks = np.array([list(elt) for elt in df['art_mask'].values]).astype(int)\n",
    "\n",
    "Y_in = np.array([list(elt) for elt in df['input'].values]).astype(int)\n",
    "# Y_in_masks = np.array([list(elt) for elt in df['input_masks'].values]).astype(int)\n",
    "\n",
    "Y_out = np.array([list(elt) for elt in df['output'].values]).astype(int)\n",
    "\n",
    "Y_out = np.concatenate([Y_out, np.zeros((Y_out.shape[0], 1))], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:,:128]\n",
    "Y_in = Y_in[:,:32]\n",
    "Y_out = Y_out[:,:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_in[:, -1] = 102\n",
    "Y_out[:,-1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc, X_test_enc, y_train, y_test = train_test_split(X, Y_out, random_state=42, test_size=0.1)\n",
    "X_train_dec, X_test_dec, _, _ = train_test_split(Y_in, Y_out, random_state=42, test_size=0.1)\n",
    "\n",
    "X_train = [X_train_enc, X_train_dec]\n",
    "X_test = [X_test_enc, X_test_dec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoder\n",
    "def build_encoder(max_length_in = 512, vocab_size = 30522):\n",
    "\n",
    "    encoder_input = Input(shape = (None,), dtype = 'int32')\n",
    "    \n",
    "    attention_mask = create_padding_mask(encoder_input)\n",
    "    \n",
    "    attention_bert = create_padding_mask(encoder_input, add_dimension = False)\n",
    "    \n",
    "    sentence_encoder = TFBertModel.from_pretrained(\n",
    "        \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.  \n",
    "        output_attentions = False, # Whether the model returns attentions weights.\n",
    "        output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "    )\n",
    "\n",
    "    encoded = sentence_encoder(encoder_input, attention_mask = 1 - attention_bert)\n",
    "\n",
    "    encoded = encoded[0]\n",
    "\n",
    "    encoder = Model(encoder_input, [encoded,attention_mask] )\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = build_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder(d_model = 768, max_length_out = 128, vocab_size = 30522):\n",
    "\n",
    "    decoder_input = Input(shape = (None,))\n",
    "    \n",
    "    encoder_output = Input(shape = (None, 768))\n",
    "    encoder_mask = Input(shape = (1,1,None))\n",
    "    \n",
    "    inputs_decoder = [decoder_input, encoder_output, encoder_mask]\n",
    "    \n",
    "    dec = Decoder(num_layers = 6, d_model = d_model, num_heads = 8, dff = 512, target_vocab_size = vocab_size,\n",
    "               maximum_position_encoding = max_length_out, rate=0.1, bidirectional_decoder = False)\n",
    "\n",
    "    decoded, _ = dec( decoder_input, encoder_output, training = True, padding_mask = encoder_mask)\n",
    "\n",
    "    decoded = tf.keras.layers.Dense(vocab_size)(decoded)\n",
    "\n",
    "    decoder = Model(inputs_decoder, decoded)\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = build_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoder decoder architecture\n",
    "\n",
    "max_length_in = 512\n",
    "max_length_out = 128\n",
    "\n",
    "vocab_size = 30522\n",
    "\n",
    "encoder_inputs = Input(shape = (None,), dtype = 'int32')\n",
    "decoder_inputs = Input(shape = (None,))\n",
    "\n",
    "\n",
    "inputs = [encoder_inputs,  decoder_inputs]\n",
    "\n",
    "encoder = build_encoder(max_length_in = max_length_in, vocab_size = 30522)\n",
    "\n",
    "decoder = build_decoder(d_model = 768, max_length_out = max_length_out, vocab_size = 30522)\n",
    "\n",
    "enc = encoder(encoder_inputs)\n",
    "encoder_output = enc[0]\n",
    "encoder_mask = enc[1] \n",
    "\n",
    "decoded = decoder([decoder_inputs, encoder_output, encoder_mask])\n",
    "\n",
    "model = Model(inputs, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[2:3]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.config.optimizer.set_jit(True)\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "  \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "                    name='train_accuracy')\n",
    "\n",
    "loss_classif     =  loss_function# find the right loss for multi-class classification\n",
    "optimizer        =  Adam(3e-5, 1e-8) # find the right optimizer\n",
    "metrics_classif  =  [train_accuracy]\n",
    "\n",
    "\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "epochs = 4\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    history = model.fit(X_train, y_train, batch_size=batch_size,\n",
    "                                  epochs=epochs, validation_data=(X_test,  y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the full encoder decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[2:3]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.config.optimizer.set_jit(False)\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "  \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "                    name='train_accuracy')\n",
    "\n",
    "loss_classif     =  loss_function# find the right loss for multi-class classification\n",
    "optimizer        =  Adam(3e-5, 1e-8) # find the right optimizer\n",
    "metrics_classif  =  [train_accuracy]\n",
    "\n",
    "\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 4\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test,  y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
