{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import _pickle as pickle\n",
    "\n",
    "## Some useful functions to ease the processings\n",
    "def save(file,name, folder = \"\"):\n",
    "    if folder != \"\":\n",
    "        outfile = open('./'+folder+'/'+name+'.pickle', 'wb')\n",
    "    else:\n",
    "        outfile = open(name+'.pickle', 'wb')\n",
    "    pickle.dump(file, outfile)\n",
    "    outfile.close\n",
    "    \n",
    "def load(name, folder = \"\"):\n",
    "    if folder != \"\":\n",
    "        outfile = open('./'+folder+'/'+name+'.pickle', 'rb')\n",
    "    else:\n",
    "        outfile = open(name+'.pickle', 'rb')\n",
    "    file = pickle.load(outfile)\n",
    "    outfile.close\n",
    "    return file\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('fr_dedup.txt', 'r', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_batch = 500000\n",
    "\n",
    "\n",
    "for i, line in enumerate(f):\n",
    "    \n",
    "    if i % size_batch == 0:\n",
    "        if i != 0:\n",
    "            save(batch, 'batch_'+str(i//size_batch), 'batch')\n",
    "        batch = list(np.zeros(size_batch))\n",
    "        \n",
    "    batch[i%size_batch] = line\n",
    "    \n",
    "    if i == 40000000:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = load('batch_1', 'batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(x):\n",
    "    import unidecode\n",
    "    x = str(x)\n",
    "    x = x.lower()\n",
    "    x = \" \".join(x.split())\n",
    "    x = unidecode.unidecode(x)\n",
    "    x = x.replace('\\n', '')\n",
    "    x = '[CLS] ' + x + ' [SEP]'\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocess import Pool\n",
    "\n",
    "p = Pool(8)\n",
    "x = p.map(clean, test_file)\n",
    "p.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer(num_words = 30000, lower = False, filters = '', oov_token = '[UNK]')\n",
    "\n",
    "\n",
    "x0 = []\n",
    "for i in range(20000):\n",
    "    x1 = \" \".join(\"[CLS]\" for elt in range(60))\n",
    "    x2 = \" \".join(\"[SEP]\" for elt in range(50))\n",
    "    x0.append(x1)\n",
    "    x0.append(x2)\n",
    "tokenizer.fit_on_texts(x0 + x)\n",
    "\n",
    "tokenizer.word_index['[PAD]'] = 0\n",
    "tokenizer.index_word[0] = '[PAD]'\n",
    "tokenizer.word_counts['[PAD]'] = 30000\n",
    "tokenizer.word_counts['[UNK]'] = 30000\n",
    "wi = {}\n",
    "wc = {}\n",
    "iw = {}\n",
    "for key in tokenizer.word_index.keys():\n",
    "    if tokenizer.word_counts[key] >= 40:\n",
    "        wi[key] = tokenizer.word_index[key]\n",
    "        iw[tokenizer.word_index[key]] = key\n",
    "        wc[key] = tokenizer.word_counts[key]\n",
    "tokenizer.word_index = wi\n",
    "tokenizer.index_word = iw\n",
    "tokenizer.word_counts = wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(tokenizer, 'IFart_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load('IFart_tokenizer')\n",
    "\n",
    "for i in tqdm(range(1,81)):\n",
    "    test_file = load('batch_'+str(i), 'batch')\n",
    "    \n",
    "    p = Pool(8)\n",
    "    file_clean = p.map(clean, test_file)\n",
    "    p.close()\n",
    "    \n",
    "    save(file_clean,'batch_'+str(i), 'batch_clean')\n",
    "    \n",
    "    tok = tokenizer.texts_to_sequences(file_clean)\n",
    "    \n",
    "    pad = pad_sequences(tok, maxlen=512, dtype='int32', padding='post', truncating='post',\n",
    "    value=0.0)\n",
    "    \n",
    "    pad[pad[:,-1] != 0, -1] = 2\n",
    "    \n",
    "    save(pad,'batch_'+str(i), 'batch_tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_transformers2 import *\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, TimeDistributed, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load('IFart_tokenizer')\n",
    "\n",
    "inputs_ids = Input(shape = (None,))\n",
    "# inputs_types = Input(shape = (None,))\n",
    "\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "\n",
    "# inputs = [inputs_ids, inputs_types]\n",
    "\n",
    "inputs = inputs_ids\n",
    "\n",
    "# GPT = GPTDecoder(num_layers = 12, d_model = 768, num_heads = 12, dff = 3072, target_vocab_size = vocab_size,\n",
    "#                maximum_position_encoding = 512, rate=0.1, bidirectional_decoder = False)\n",
    "\n",
    "GPT = GPTDecoder(num_layers = 12, d_model = 512, num_heads = 8, dff = 1024, target_vocab_size = vocab_size,\n",
    "               maximum_position_encoding = 512, rate=0.1, bidirectional_decoder = False)\n",
    "\n",
    "outputs, _ = GPT(inputs_ids, training = True, token_types_ids = None)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(vocab_size)(outputs)\n",
    "\n",
    "model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000, factor = 1):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.factor = factor\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2) / self.factor\n",
    "    \n",
    "def sparse_k_acc(a,b):\n",
    "    \n",
    "    if a.shape[1] is None:\n",
    "        return 1\n",
    "    else:\n",
    "        a = tf.split(a, a.shape[1], axis = 1)\n",
    "        b = tf.split(b, b.shape[1],axis = 1)\n",
    "        c = [tf.keras.metrics.sparse_top_k_categorical_accuracy(tf.reshape(a[i],(-1,1))[:,0],\n",
    "                                                               b[i][:,0], k = 10) for i in range(len(a))]\n",
    "        return tf.reduce_mean(c)\n",
    "    \n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "  \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "                    name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "gpt_decoder (GPTDecoder)     ((None, None, 512), {'dec 42409472  \n",
      "_________________________________________________________________\n",
      "dense_120 (Dense)            (None, None, 33547)       17209611  \n",
      "=================================================================\n",
      "Total params: 59,619,083\n",
      "Trainable params: 59,619,083\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x18b520af348>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep_save = 2\n",
    "batch_save = 29\n",
    "model.load_weights('./checkpoints/IFart_epoch_'+str(ep_save)+'_batch_'+str(batch_save)+'/checkpoint.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ep_save = 5\n",
    "batch_save = 13\n",
    "model.load_weights('./checkpoints/IFart_epoch_'+str(ep_save)+'_batch_'+str(batch_save)+'/checkpoint.h5py')\n",
    "\n",
    "EPOCHS = 6\n",
    "# ep_save = 5\n",
    "for i in range(ep_save, EPOCHS):\n",
    "    print('**********************      EPOCH '+str(i)+'        **************************')\n",
    "    \n",
    "    learning_rate = CustomSchedule(512, factor = 1)\n",
    "    loss_classif     =  loss_function\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "                    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "    metrics_classif  =  [train_accuracy, sparse_k_acc]\n",
    "    \n",
    "    model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "    \n",
    "    lens = [32, 64, 128, 256, 350, 512]\n",
    "    bs = [256, 128, 64, 32, 20, 8]\n",
    "    e = [20,20,30,40,50,80]\n",
    "    start = batch_save+1\n",
    "#     start = 1\n",
    "\n",
    "    for batch in range(start,e[i]):\n",
    "        print('*********** BATCH '+str(batch))\n",
    "        \n",
    "        X = load('batch_'+str(batch), 'batch_tokenized')\n",
    "#         y = np.concatenate([X[:,1:], np.zeros((X.shape[0], 1))], axis = 1)\n",
    "    \n",
    "        max_len = lens[i]\n",
    "        \n",
    "        X = X[(X != 0).sum(axis = 1) >= max_len-int(max_len/10)]\n",
    "        \n",
    "        X = X[:, :max_len]\n",
    "        X[X[:,-1] != 0, -1] = 2\n",
    "        y = np.concatenate([X[:,1:], np.zeros((X.shape[0], 1))], axis = 1)\n",
    "        \n",
    "        print('batch_loaded')\n",
    "        \n",
    "        batch_size = bs[i]\n",
    "        epochs = 1\n",
    "        history = model.fit(X, y, batch_size=batch_size, epochs=epochs)\n",
    "        \n",
    "        os.mkdir('./checkpoints/IFart_epoch_'+str(i)+'_batch_'+str(batch))\n",
    "        model.save_weights('./checkpoints/IFart_epoch_'+str(i)+'_batch_'+str(batch)+'/checkpoint.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(x):\n",
    "    import unidecode\n",
    "    x = str(x)\n",
    "    x = x.lower()\n",
    "    x = \" \".join(x.split())\n",
    "    x = unidecode.unidecode(x)\n",
    "    x = x.replace('\\n', '')\n",
    "    x = '[CLS] ' + x \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = \"j'ai toujours eu envie de\"\n",
    "# inp = 'i always thought that'\n",
    "inp = clean(inp)\n",
    "\n",
    "# inp1 = \"j'ai toujours eu envie de\"\n",
    "# inp1 = clean(inp1)\n",
    "\n",
    "inp_seq = tokenizer.texts_to_sequences([inp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 73, 112, 237, 614, 4]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'generation' from 'D:\\\\fr_dedup.txt\\\\generation.py'>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import generation\n",
    "reload(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c7d3109626a43fd815642750d57c232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=59.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "a = generation.generate(\n",
    "        input_ids=inp_seq,\n",
    "        model = model, \n",
    "        max_length=60,\n",
    "        do_sample=True,\n",
    "        num_beams=1,\n",
    "        temperature=0.7,\n",
    "        top_k=5,\n",
    "        top_p=0.3,\n",
    "        repetition_penalty=4,\n",
    "        bos_token_id=2,\n",
    "        pad_token_id=0,\n",
    "        eos_token_ids=3,\n",
    "        unk_token_ids = 1,\n",
    "        length_penalty=1,\n",
    "        vocab_size = 33547,\n",
    "        remove_unk_tokens = True,\n",
    "        num_return_sequences = 5\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[CLS] j'ai toujours eu envie de vous parler des choses que je ne connais pas, mais il faut savoir qu'il y a beaucoup d'autres personnes qui ont un peu plus ou meme une fois par jour pour se rendre compte qu'ils sont en train d'etre en retard et que les gens qui ne savent pas trop comment ils vont les\",\n",
       " \"[CLS] j'ai toujours eu envie de vous parler des choses que je ne sais pas encore comment faire pour me rendre compte du pourquoi et si ca est vrai, c'est parce qu'on a fait un grand effort sur le plan technique mais aussi en tant qu'il y aura bien d'autres choses. il faut savoir ce qui se passe dans les\",\n",
       " \"[CLS] j'ai toujours eu envie de faire un petit tour sur le site et je me suis fait plaisir, en effet, a la fin du mois d'octobre, pour une fois que vous avez bien compris ce qui est arrive au debut des annees 90, mais pas n'importe quoi ! c'est d'ailleurs avec les memes raisons : il y avait deja\",\n",
       " \"[CLS] j'ai toujours eu envie de vous parler des choses que je ne peux pas dire a ma mere et aux enfants qui me suivent dans la vie en tant qu'elle est tres heureuse avec moi. elle m'a dit qu'il n'y avait rien d'autre qu'une seule personne pour lui faire plaisir. mais il y aurait aussi beaucoup d'autres personnes comme\",\n",
       " \"[CLS] j'ai toujours eu envie de vous parler du sujet que je vais avoir a la tete d'un grand nombre d'autres personnes qui ont deja fait le tour des choses et qui me font un peu plus mal au niveau d'une personne ou encore une autre personne. mais pour moi il y aura beaucoup d'amis dans les autres pays en\"]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
