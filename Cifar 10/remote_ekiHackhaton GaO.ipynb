{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ScVmE2VcrcBn"
   },
   "source": [
    "#Introduction\n",
    "Welcome to the first remote Eki hackaton. The aim of the hackaton is to build your first neural network and a learning scheme in a efficient way.The task is image classification on CIFAR10\n",
    "\n",
    "**************\n",
    "The code contains :\n",
    "\n",
    "1. Load and normalizing the CIFAR10 training and test datasets using\n",
    "   ``torchvision``\n",
    "2. Define a Convolutional Neural Network\n",
    "3. Define a loss function\n",
    "4. Train the network on the training data\n",
    "5. Test the network on the test data\n",
    "\n",
    "**************\n",
    "Some **questions** too keep in mind :\n",
    "\n",
    "    - which kind of data transformation and augmentation?\n",
    "    - How building my neural network?\n",
    "    - which kind of optimization?\n",
    "\n",
    "Some tips here :) : [ML Lunch](https://bizoffice61.sharepoint.com/:p:/r/sites/Ekimetrics51/_layouts/15/Doc.aspx?sourcedoc=%7BA44BD8EB-39E2-4113-9861-A817680A7F55%7D&file=20200304%20-%20ML%20Lunch%20-%20Understanding%20Deep%20Learning.pptx&action=edit&mobileredirect=true)\n",
    "\n",
    "**************\n",
    "How to win?\n",
    "\n",
    "The winner will propose a script which:\n",
    "\n",
    "reachs at least 60% of accuracy on the dataset\n",
    "builds a network with a minimum number of parameters.\n",
    "Example :\n",
    "\n",
    "People 1 : Accuracy : 58%, number of parameter : 10K\n",
    "\n",
    "People 2 : Accuracy : 61%, number of parameters : 15K\n",
    "\n",
    "People 3: Accuracy : 67%, number of paremeters : 16K\n",
    "\n",
    "People 2 wins !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nzpZHfSepk12"
   },
   "source": [
    "#Import all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 15 23:09:29 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 456.71       Driver Version: 456.71       CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 2070   WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   56C    P0    43W /  N/A |    857MiB /  8192MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1416    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      2096    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      3152    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      4212    C+G   ...app-2.2041.6\\WhatsApp.exe    N/A      |\n",
      "|    0   N/A  N/A      4648    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A      8920    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      9480    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     13088      C   ...abri\\Anaconda3\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     14688    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     15536    C+G   ...nputApp\\TextInputHost.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(tf.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_built_with_xla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yyKYHGjTrBec"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "import requests\n",
    "\n",
    "# Put the token you received in a mail here ! \n",
    "token = '5e9ed8b402f6580017600bca'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import own library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wuxlxnRsqedU"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jw1NDFrosjX6"
   },
   "source": [
    "# Load and normalizing the CIFAR10 training and test datasets using ``torchvision``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nm4gnJAurikT"
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# The output of torchvision datasets are PILImage images of range [0, 1].\n",
    "# We transform them to Tensors of normalized range [-1, 1].\n",
    "\n",
    "transform = transforms.Compose(\n",
    "     [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=0)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pickle\n",
    "def unpickle(file):\n",
    "\n",
    "    fo = open(file, 'rb')\n",
    "    dict = pickle.load(fo, encoding ='bytes')\n",
    "    X = dict[b'data']\n",
    "    Y = dict[b'labels']\n",
    "    fo.close()\n",
    "    return X, Y\n",
    "\n",
    "X, Y = unpickle('data/cifar-10-batches-py/test_batch')\n",
    "\n",
    "tr = [\n",
    "    'data_batch_1',\n",
    "    'data_batch_2',\n",
    "    'data_batch_3',\n",
    "    'data_batch_4',\n",
    "    'data_batch_5', \n",
    "]\n",
    "\n",
    "te = [\n",
    "    'test_batch'\n",
    "]\n",
    "\n",
    "X, Y = unpickle('data/cifar-10-batches-py/data_batch_1')\n",
    "\n",
    "for elt in tr[1:]:\n",
    "    X_temp, Y_temp = unpickle('data/cifar-10-batches-py/'+str(elt))\n",
    "    X = np.concatenate([X, X_temp], axis = 0)\n",
    "    Y+=Y_temp\n",
    "\n",
    "X_test, Y_test = unpickle('data/cifar-10-batches-py/test_batch')\n",
    "\n",
    "def reshape(x):\n",
    "    red = x[:1024].reshape(32,32,1)\n",
    "    green = x[1024:2048].reshape(32,32,1)\n",
    "    blue = x[2048:3072].reshape(32,32,1)\n",
    "    return np.concatenate([red, green, blue], axis = -1)\n",
    "\n",
    "def batch_reshape(X):\n",
    "    X1 = list(np.zeros(X.shape[0]))\n",
    "    for i, elt in enumerate(X):\n",
    "        X1[i] = reshape(elt)\n",
    "    return X1\n",
    "\n",
    "X = np.array(batch_reshape(X))\n",
    "X_test = np.array(batch_reshape(X_test))\n",
    "\n",
    "plt.imshow(X[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting into train and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X/255 - 0.5\n",
    "X_test = X_test/255 -0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np_utils.to_categorical(Y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train at first a big model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input,Concatenate, Conv2D, Lambda,Flatten, Activation, Dense, BatchNormalization, MaxPooling2D,AveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    inputs = Input(shape = (32,32,3))\n",
    "\n",
    "    x = Conv2D(32, kernel_size=(3, 3), activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    x = Conv2D(128, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dense(10)(x)\n",
    "\n",
    "    outputs = Activation('softmax')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n",
    "                         width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,\n",
    "                         horizontal_flip=True, fill_mode=\"nearest\")\n",
    "\n",
    "optimizer = SGD(0.1)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0.001, patience=6, verbose=1, mode='auto',\n",
    "    baseline=None, restore_best_weights=True\n",
    ")\n",
    "reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, \n",
    "                                                     mode='auto', min_delta=0.0001, cooldown=0, min_lr=0.00001)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 40\n",
    "\n",
    "history = model.fit_generator(aug.flow(X_train, y_train, batch_size=batch_size),\n",
    "    validation_data=(X_val, y_val), steps_per_epoch=len(X_train) // batch_size,\n",
    "    epochs=epochs, callbacks = [stop, reduce])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model_big_normalized.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = len(Y_test)\n",
    "\n",
    "pred = np.argmax(model.predict(X_test), axis = 1).astype(int)\n",
    "\n",
    "for i, elt in enumerate(pred):\n",
    "    if elt == Y_test[i]:\n",
    "        correct+=1\n",
    "\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "def num_weights(model):\n",
    "    weights = 0\n",
    "    for elt in model.trainable_weights:\n",
    "        a = 1\n",
    "        for elt1 in elt.shape:\n",
    "            a*=elt1\n",
    "        weights+=a\n",
    "    return weights\n",
    "\n",
    "trainable_parameters = num_weights(model)\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    accuracy))\n",
    "\n",
    "print(\"number of trainable parameters : %d\"%trainable_parameters)\n",
    "\n",
    "score = accuracy if accuracy <= 60. else 60. + 40 * np.tanh(10000/trainable_parameters)\n",
    "print(score)\n",
    "\n",
    "# requests.put(\n",
    "#   'http://eki-hackathon.herokuapp.com/submitScoreAndMetrics', \n",
    "#   json={\n",
    "#     'id': token,\n",
    "#     'score': score,\n",
    "#     'metrics': {\"trainable parameters\": trainable_parameters,\n",
    "#                 \"accuracy\" : accuracy}\n",
    "#   })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some attempt at distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_big = build_model()\n",
    "model_big.load_weights('model_big_normalized.h5')\n",
    "inputs = model_big.input\n",
    "outputs = model_big.layers[-2].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher = Model(inputs, outputs)\n",
    "\n",
    "train_logit = teacher.predict(X_train)\n",
    "test_logit = teacher.predict(X_val)\n",
    "\n",
    "y_traind = np.concatenate([y_train, train_logit], axis = 1)\n",
    "y_vald = np.concatenate([y_val, test_logit], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Best distilled model stable 8826 weights int 255 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Best stable\n",
    "# temperature =  5\n",
    "\n",
    "# inputs = Input(shape = (32,32,3))\n",
    "\n",
    "# x = Conv2D(16, kernel_size=(3, 3), activation='relu')(inputs)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# x = Conv2D(16, kernel_size=(3, 3), activation='relu')(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# x = Conv2D(32, kernel_size=(3, 3), activation='relu')(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# x = Flatten()(x)\n",
    "# # x = Dense(256, activation='relu')(x)\n",
    "# logits = Dense(10)(x)\n",
    "\n",
    "# probabilities = Activation('softmax')(logits)\n",
    "# # softed probabilities\n",
    "# logits_T = Lambda(lambda x: x/temperature)(logits)\n",
    "# probabilities_T = Activation('softmax')(logits_T)\n",
    "\n",
    "# output = Concatenate()([probabilities, probabilities_T])\n",
    "\n",
    "\n",
    "# distilled = Model(inputs, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Best Model unstable 6268 normalized images (might depend on training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature =  5\n",
    "\n",
    "inputs = Input(shape = (32,32,3))\n",
    "\n",
    "x = Conv2D(15, kernel_size=(3, 3),activation='relu')(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(15, kernel_size=(3, 3),activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(21, kernel_size=(3, 3),activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# x = Conv2D(22, kernel_size=(3, 3),padding = 'same', activation='relu')(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "# x = Dense(256, activation='relu')(x)\n",
    "logits = Dense(10)(x)\n",
    "\n",
    "probabilities = Activation('softmax')(logits)\n",
    "# softed probabilities\n",
    "logits_T = Lambda(lambda x: x/temperature)(logits)\n",
    "probabilities_T = Activation('softmax')(logits_T)\n",
    "\n",
    "output = Concatenate()([probabilities, probabilities_T])\n",
    "\n",
    "\n",
    "distilled = Model(inputs, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distilled.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import categorical_crossentropy as logloss\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "def knowledge_distillation_loss(y_true, y_pred, lambda_const):    \n",
    "    \n",
    "    # split in \n",
    "    #    onehot hard true targets\n",
    "    #    logits from xception\n",
    "    y_true, logits = y_true[:, :10], y_true[:, 10:]\n",
    "    \n",
    "    # convert logits to soft targets\n",
    "    y_soft = K.softmax(logits/temperature)\n",
    "    \n",
    "    # split in \n",
    "    #    usual output probabilities\n",
    "    #    probabilities made softer with temperature\n",
    "    y_pred, y_pred_soft = y_pred[:, :10], y_pred[:, 10:]    \n",
    "    \n",
    "    return lambda_const*logloss(y_true, y_pred) + logloss(y_soft, y_pred_soft)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = y_true[:, :10]\n",
    "    y_pred = y_pred[:, :10]\n",
    "    return categorical_accuracy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_const = 0.2\n",
    "\n",
    "distilled.compile(\n",
    "    optimizer=SGD(lr=0.1, momentum=0.9, nesterov=True), \n",
    "    loss=lambda y_true, y_pred: knowledge_distillation_loss(y_true, y_pred, lambda_const), \n",
    "    metrics=[accuracy]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0.001, patience=6, verbose=1, mode='auto',\n",
    "    baseline=None, restore_best_weights=True\n",
    ")\n",
    "reduce =tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, \n",
    "                                                     mode='auto', min_delta=0.0001, cooldown=0, min_lr=0.00001)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 40\n",
    "\n",
    "history = distilled.fit_generator(aug.flow(X_train, y_traind, batch_size=batch_size),\n",
    "    validation_data=(X_val, y_vald), steps_per_epoch=len(X_train) // batch_size,\n",
    "    epochs=epochs, callbacks = [stop, reduce])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = len(Y_test)\n",
    "\n",
    "pred = np.argmax(distilled.predict(X_test)[:,:10], axis = 1).astype(int)\n",
    "\n",
    "for i, elt in enumerate(pred):\n",
    "    if elt == Y_test[i]:\n",
    "        correct+=1\n",
    "\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "def num_weights(model):\n",
    "    weights = 0\n",
    "    for elt in model.trainable_weights:\n",
    "        a = 1\n",
    "        for elt1 in elt.shape:\n",
    "            a*=elt1\n",
    "        weights+=a\n",
    "    return weights\n",
    "\n",
    "trainable_parameters = num_weights(distilled)\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    accuracy))\n",
    "\n",
    "print(\"number of trainable parameters : %d\"%trainable_parameters)\n",
    "\n",
    "score = accuracy if accuracy <= 60. else 60. + 40 * np.tanh(10000/trainable_parameters)\n",
    "print(score)\n",
    "\n",
    "# requests.put(\n",
    "#   'http://eki-hackathon.herokuapp.com/submitScoreAndMetrics', \n",
    "#   json={\n",
    "#     'id': token,\n",
    "#     'score': score,\n",
    "#     'metrics': {\"trainable parameters\": trainable_parameters,\n",
    "#                 \"accuracy\" : accuracy}\n",
    "#   })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "remote_ekiHackhaton.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
