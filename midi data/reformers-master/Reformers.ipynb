{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.layers import Dropout, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_unit_length(x, epsilon=1e-6):\n",
    "    norm = tf.norm(x,  ord=2, axis=-1, keepdims=True)\n",
    "    return tf.math.truediv(x, norm + epsilon)\n",
    "\n",
    "def sort_key_val(t1, t2, dim=-1):\n",
    "    values = tf.sort(t1, axis=dim)\n",
    "    t2 = tf.broadcast_to(t2, t1.shape)\n",
    "    return values, tf.gather(t2, tf.argsort(t1, axis=dim), axis=dim)\n",
    "\n",
    "def batched_index_select(values, indices):\n",
    "    last_dim = values.shape[-1]\n",
    "    return tf.squeeze(tf.gather(values, indices[:, :, None], axis=1))\n",
    "\n",
    "def process_inputs_chunk(fn, *args, chunks=1):\n",
    "    chunked_inputs = list(map(lambda x: tf.split(x, chunks, axis=0), args))\n",
    "    outputs = [fn(*input_pair) for input_pair in zip(*chunked_inputs)]\n",
    "    return outputs\n",
    "\n",
    "def chunked_sum(tensor, chunks=1):\n",
    "    *orig_size, last_dim = tensor.shape\n",
    "    tensor = tf.reshape(tensor,  [-1, last_dim])\n",
    "    summed_tensors = [c.sum(axis=-1) for c in tf.chunk(tensor, chunks, axis=0)]\n",
    "    return tf.reshape(torch.concat(summed_tensors, axis=0), orig_size)\n",
    "\n",
    "def cache_fn(f):\n",
    "    cache = None\n",
    "    def cached_fn(*args, **kwargs):\n",
    "        nonlocal cache\n",
    "        if cache is not None:\n",
    "            return cache\n",
    "        cache = f(*args, **kwargs)\n",
    "        return cache\n",
    "    return cached_fn\n",
    "\n",
    "class ScaleNorm(layers.Layer):\n",
    "    def __init__(self, emb, eps):\n",
    "        super(ScaleNorm, self).__init__()\n",
    "        self.g = tf.Variable(initial_value=w_init(shape=(1,),\n",
    "                        dtype='float32'),\n",
    "                        trainable=True)\n",
    "        self.eps = eps\n",
    "\n",
    "    def call(self, inputs):\n",
    "        n = tf.norm(inputs, axis=-1, keepdims=True).clip_by_value(min=self.eps)\n",
    "        return x / n * self.g\n",
    "\n",
    "class WithNorm(layers.Layer):\n",
    "    def __init__(self, norm_class, emb, fn):\n",
    "        super(WithNorm, self).__init__()\n",
    "        self.emb = emb\n",
    "        if isinstance(norm_class, ScaleNorm):\n",
    "            self.norm = norm_class(emb)\n",
    "        else:\n",
    "            self.norm = norm_class()\n",
    "\n",
    "        self.fn = fn\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = self.norm(inputs)\n",
    "        return self.fn(inputs)\n",
    "\n",
    "class Chunk(layers.Layer):\n",
    "    def __init__(self, chunks, fn, along_axis = -1):\n",
    "        super(Chunk, self).__init__()\n",
    "        self.axis = along_axis\n",
    "        self.chunks = chunks\n",
    "        self.fn = fn\n",
    "\n",
    "    def call(self, inputs):\n",
    "        chunks = tf.split(inputs, self.chunks, axis= self.axis)\n",
    "        return tf.concat([self.fn(c) for c in chunks], axis = self.axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFLSHAttention(tf.keras.Model):\n",
    "    def __init__( self,\n",
    "                  dropout = 0.,\n",
    "                  bucket_size = 64,\n",
    "                  n_hashes = 8,\n",
    "                  causal = False,\n",
    "                  allow_duplicate_attention = True,\n",
    "                  attend_across_buckets = True,\n",
    "                  rehash_each_round = True,\n",
    "                  drop_for_hash_rate = 0.0,\n",
    "                  random_rotations_per_head = False):\n",
    "        super(TFLSHAttention, self).__init__()\n",
    "        if dropout >= 1.0:\n",
    "            raise ValueError('Dropout rates must be lower than 1.')\n",
    "\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.dropout_for_hash = Dropout(dropout)\n",
    "\n",
    "        assert rehash_each_round or allow_duplicate_attention, (\n",
    "            'The setting {allow_duplicate_attention=False, rehash_each_round=False}'\n",
    "            ' is not implemented.')\n",
    "\n",
    "        self.causal = causal\n",
    "        self.n_hashes = n_hashes\n",
    "        self.bucket_size = bucket_size\n",
    "\n",
    "        self._allow_duplicate_attention = allow_duplicate_attention\n",
    "        self._attend_across_buckets = attend_across_buckets\n",
    "        self._rehash_each_round = rehash_each_round\n",
    "        self._random_rotations_per_head = random_rotations_per_head\n",
    "\n",
    "    def hash_vectors(self, n_buckets, vecs):\n",
    "        batch_size = vecs.shape[0]\n",
    "        device = vecs.device\n",
    "\n",
    "        # See https://arxiv.org/pdf/1509.02897.pdf\n",
    "        # We sample a different random rotation for each round of hashing to\n",
    "        # decrease the probability of hash misses.\n",
    "        assert n_buckets % 2 == 0\n",
    "\n",
    "        rot_size = n_buckets\n",
    "\n",
    "        rotations_shape = (\n",
    "            batch_size if self._random_rotations_per_head else 1,\n",
    "            vecs.shape[-1],\n",
    "            self.n_hashes if self._rehash_each_round else 1,\n",
    "            rot_size // 2)\n",
    "\n",
    "        random_rotations = tf.broadcast_to(tf.random.normal(rotations_shape), (batch_size, vecs.shape[-1], self.n_hashes if self._rehash_each_round else 1, rot_size // 2))\n",
    "\n",
    "        dropped_vecs = self.dropout_for_hash(vecs)\n",
    "        rotated_vecs = tf.einsum('btf,bfhi->bhti', dropped_vecs, random_rotations)\n",
    "\n",
    "        if self._rehash_each_round:\n",
    "            rotated_vecs = tf.concat([rotated_vecs, -rotated_vecs], axis=-1)\n",
    "            buckets = tf.math.argmax(rotated_vecs, axis=-1)\n",
    "            # buckets is now (self.n_hashes, seqlen). Next we add offsets so that\n",
    "            # bucket numbers from different hashing rounds don't overlap.\n",
    "            offsets = tf.range(self.n_hashes)\n",
    "            offsets = tf.reshape(offsets * n_buckets, (1, -1, 1))\n",
    "            offsets = tf.cast(offsets, tf.int64)\n",
    "            buckets = tf.reshape(buckets + offsets, (batch_size, -1,))\n",
    "        else:\n",
    "            rotated_vecs = tf.concat([rotated_vecs, -rotated_vecs], axis=-1)\n",
    "            # In this configuration, we map each item to the top self.n_hashes buckets\n",
    "            rotated_vecs = tf.squeeze(rotated_vecs, axis=0)\n",
    "            bucket_range = tf.range(rotated_vecs.shape[-1])\n",
    "            bucket_range = tf.reshape(bucket_range, (1, -1))\n",
    "            bucket_range = tf.broadcast_to(bucket_range, rotated_vecs.shape)\n",
    "\n",
    "            _, buckets = sort_key_val(rotated_vecs, bucket_range, axis=-1)\n",
    "            buckets = buckets[:, -self.n_hashes:]\n",
    "\n",
    "            h, *_ = buckets.shape \n",
    "            buckets = tf.reshape(buckets.permute((*_, h)), (-1,))\n",
    "\n",
    "        return buckets\n",
    "\n",
    "    def call(self, qk, v):\n",
    "        batch_size, seqlen, _ = qk.shape\n",
    "        device = qk.device\n",
    "\n",
    "        n_buckets = seqlen // self.bucket_size\n",
    "        n_bins = n_buckets\n",
    "\n",
    "        buckets = self.hash_vectors(n_buckets, qk)\n",
    "        # We use the same vector as both a query and a key.\n",
    "        assert int(buckets.shape[1]) == self.n_hashes * seqlen\n",
    "\n",
    "        ticker = tf.expand_dims(tf.range(self.n_hashes * seqlen), axis=0)\n",
    "        buckets_and_t = seqlen * buckets + tf.cast((ticker % seqlen), tf.int64)\n",
    "        buckets_and_t = tf.stop_gradient(buckets_and_t)\n",
    "\n",
    "        # Hash-based sort (\"s\" at the start of variable names means \"sorted\")\n",
    "        sbuckets_and_t, sticker = sort_key_val(buckets_and_t, ticker, dim=-1)\n",
    "        _, undo_sort = sort_key_val(sticker, ticker, dim=-1)\n",
    "        del ticker\n",
    "\n",
    "        sbuckets_and_t = tf.stop_gradient(sbuckets_and_t)\n",
    "        sticker = tf.stop_gradient(sticker)\n",
    "        undo_sort = tf.stop_gradient(undo_sort)\n",
    "\n",
    "        st = (sticker % seqlen)\n",
    "        sqk = batched_index_select(qk, st)\n",
    "        sv = batched_index_select(v, st)\n",
    "\n",
    "        # Split off a \"bin\" axis so that attention only occurs within chunks.\n",
    "        bq_t = bkv_t = tf.reshape(st, (batch_size, self.n_hashes * n_bins, -1))\n",
    "        bqk = tf.reshape(sqk, (batch_size, self.n_hashes * n_bins, -1, sqk.shape[-1]))\n",
    "        bv = tf.reshape(sv, (batch_size, self.n_hashes * n_bins, -1, sv.shape[-1]))\n",
    "        bq_buckets = bkv_buckets = tf.reshape(sbuckets_and_t // seqlen, (batch_size, self.n_hashes * n_bins, -1))\n",
    "\n",
    "        # Hashing operates on unit-length vectors. Unnormalized query vectors are\n",
    "        # fine because they effectively provide a learnable temperature for the\n",
    "        # attention softmax, but normalizing keys is needed so that similarity for\n",
    "        # the purposes of attention correctly corresponds to hash locality.\n",
    "        bq = bqk\n",
    "        bk = make_unit_length(bqk)\n",
    "\n",
    "        # Allow each chunk to attend within itself, and also one chunk back. Chunk\n",
    "        # boundaries might occur in the middle of a sequence of items from the\n",
    "        # same bucket, so this increases the chances of attending to relevant items.\n",
    "        def look_one_back(x):\n",
    "            x_extra = tf.concat([x[:, -1:, ...], x[:, :-1, ...]], axis=1)\n",
    "            return tf.concat([x, x_extra], axis=2)\n",
    "        \n",
    "        print(bq.shape)\n",
    "        print(bk.shape)\n",
    "        \n",
    "        bk = look_one_back(bk)\n",
    "        bv = look_one_back(bv)\n",
    "        bkv_t = look_one_back(bkv_t)\n",
    "        bkv_buckets = look_one_back(bkv_buckets)\n",
    "        \n",
    "        print(bq.shape)\n",
    "        print(bk.shape)\n",
    "        \n",
    "        # Dot-product attention.\n",
    "        dots = tf.einsum('bhie,bhje->bhij', bq, bk) * (bq.shape[-1] ** -0.5)\n",
    "\n",
    "        # Causal masking\n",
    "        if self.causal:\n",
    "            mask = bq_t[:, :, :, None] < bkv_t[:, :, None, :] \n",
    "            dots = tf.math.multiply(dots, tf.cast(mask, tf.float32)) + (1-tf.cast(mask, tf.float32)) * float('-inf')\n",
    "            del mask\n",
    "\n",
    "        # Mask out attention to self except when no other targets are available.\n",
    "        self_mask = bq_t[:, :, :, None] == bkv_t[:, :, None, :]\n",
    "        \n",
    "        print(dots.shape)\n",
    "        print(self_mask.shape)\n",
    "        dots = tf.math.multiply(dots, tf.cast(self_mask, tf.float32)) + (1-tf.cast(self_mask, tf.float32)) * (- 1e5)\n",
    "        del self_mask\n",
    "\n",
    "        # Mask out attention to other hash buckets.\n",
    "        if not self._attend_across_buckets:\n",
    "            bucket_mask = bq_buckets[:, :, :, None] != bkv_buckets[:, :, None, :]\n",
    "            dots = tf.math.multiply(dots, tf.cast(bucket_mask, tf.float32)) + (1-tf.cast(bucket_mask, tf.float32)) * float('-inf')\n",
    "            del bucket_mask\n",
    "\n",
    "        # Don't double-count query-key pairs across multiple rounds of hashing.\n",
    "        # There are two possible strategies here. (1) The default is to count how\n",
    "        # many times a query-key pair is repeated, and to lower its log-prob\n",
    "        # correspondingly at each repetition. (2) When hard_k is set, the code\n",
    "        # instead masks all but the first occurence of each query-key pair.\n",
    "        if not self._allow_duplicate_attention:\n",
    "            locs1 = undo_sort // bq_t.shape[-1]\n",
    "            locs2 = (locs1 + 1) % (self.n_hashes * n_bins)\n",
    "            if not self._attend_across_buckets:\n",
    "                locs1 = buckets * (self.n_hashes * n_bins) + locs1\n",
    "                locs2 = buckets * (self.n_hashes * n_bins) + locs2\n",
    "            locs = tf.transpose(\n",
    "                tf.concat([\n",
    "                    tf.reshape(locs1, (batch_size, self.n_hashes, seqlen)),\n",
    "                    tf.reshape(locs2, (batch_size, self.n_hashes, seqlen)),\n",
    "                ], 1),\n",
    "            perm=[0, 2, 1]) \n",
    "\n",
    "            slocs = batched_index_select(locs, st)\n",
    "            b_locs = tf.reshape(slocs, (batch_size, self.n_hashes * n_bins, -1, 2 * self.n_hashes))\n",
    "\n",
    "            b_locs1 = b_locs[:, :, :, None, :self.n_hashes]\n",
    "\n",
    "            bq_locs = b_locs1.expand(b_locs.shape[:3] + (2, self.n_hashes))\n",
    "            bq_locs = tf.reshape(bq_locs, b_locs.shape)\n",
    "            bkv_locs = look_one_back(b_locs)\n",
    "\n",
    "            dup_counts = (bq_locs[:, :, :, None, :] == bkv_locs[:, :, None, :, :])\n",
    "            # for memory considerations, chunk summation of last dimension for counting duplicates\n",
    "            dup_counts = chunked_sum(dup_counts, chunks=(self.n_hashes * batch_size))\n",
    "            dup_counts = tf.stop_gradient(dup_counts)\n",
    "            assert dup_counts.shape == dots.shape\n",
    "            dots = dots - tf.log(dup_counts + 1e-9)\n",
    "            del dup_counts\n",
    "\n",
    "        # Softmax.\n",
    "        dots_logsumexp = tf.math.reduce_logsumexp(dots, axis=-1, keepdims=True)\n",
    "        dots = tf.exp(dots - dots_logsumexp)\n",
    "        dots = self.dropout(dots)\n",
    "\n",
    "        bo = tf.einsum('buij,buje->buie', dots, bv)\n",
    "        so = tf.reshape(bo, (batch_size, -1, bo.shape[-1]))\n",
    "        slogits = tf.reshape(dots_logsumexp, (batch_size, -1,))\n",
    "\n",
    "        class UnsortLogits(tf.keras.layers.Layer):\n",
    "            def __init__(self):\n",
    "                super(UnsortLogits, self).__init__()\n",
    "            \n",
    "            def call(self, so, slogits):\n",
    "                so, slogits = tf.stop_gradient(so), tf.stop_gradient(slogits)\n",
    "                o = batched_index_select(so, undo_sort)\n",
    "                _, logits = sort_key_val(sticker, slogits, dim=-1)\n",
    "                return o, logits\n",
    "\n",
    "            \n",
    "        unsortlogits = UnsortLogits()\n",
    "        o, logits = unsortlogits(so, slogits)\n",
    "\n",
    "        if self.n_hashes == 1:\n",
    "            out = o\n",
    "        else:\n",
    "            o = tf.reshape(o, (batch_size, self.n_hashes, seqlen, o.shape[-1]))\n",
    "            logits = tf.reshape(logits, (batch_size, self.n_hashes, seqlen, 1))\n",
    "            probs = tf.exp(logits - tf.math.reduce_logsumexp(logits, axis=1, keepdims=True))\n",
    "            out = tf.reduce_sum(o * probs, axis=1)\n",
    "\n",
    "        assert out.shape == v.shape\n",
    "        return out, buckets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFLSHSelfAttention(tf.keras.Model):\n",
    "    def __init__(self, emb, heads = 8, bucket_size = 64, n_hashes = 8, causal = False, attn_chunks = None, random_rotations_per_head = False, attend_across_buckets = True, allow_duplicate_attention = True, **kwargs):\n",
    "        super(TFLSHSelfAttention, self).__init__()\n",
    "        assert emb % heads == 0, 'dimensions must be divisible by number of heads'\n",
    "\n",
    "        self.emb = emb\n",
    "        self.heads = heads\n",
    "        self.attn_chunks = heads if attn_chunks is None else attn_chunks\n",
    "\n",
    "        self.toqk = Dense(emb, use_bias = False)\n",
    "        self.tov = Dense(emb, use_bias = False)\n",
    "        self.to_out = Dense(emb)\n",
    "\n",
    "        self.bucket_size = bucket_size\n",
    "        self.lsh_attn = TFLSHAttention(bucket_size=bucket_size, causal=causal, random_rotations_per_head=random_rotations_per_head, attend_across_buckets = attend_across_buckets,  allow_duplicate_attention = allow_duplicate_attention, **kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        b, t, e, h = *inputs.shape, self.heads\n",
    "        assert t % self.bucket_size == 0, f'Sequence length needs to be divisible by target bucket size - {self.bucket_size}'\n",
    "\n",
    "        qk = self.toqk(inputs)\n",
    "        v = self.tov(inputs)\n",
    "\n",
    "        def merge_heads(v):\n",
    "            return tf.reshape(tf.transpose(tf.reshape(v, (b, t, h, -1)), perm=[0, 2, 1, 3]), (b * h, t, -1)) \n",
    "\n",
    "        def split_heads(v):\n",
    "            return tf.transpose(tf.reshape(v, (b, t, h, -1)), perm=[0, 2, 1, 3])\n",
    "\n",
    "        qk = merge_heads(qk)\n",
    "        v = merge_heads(v)\n",
    "\n",
    "        outputs = process_inputs_chunk(self.lsh_attn, qk, v, chunks=self.attn_chunks)\n",
    "        attn_out = tf.concat([output for (output, _) in outputs], axis=0)\n",
    "\n",
    "        out = tf.reshape(split_heads(attn_out), (b, t, e))\n",
    "\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    layer = TFLSHSelfAttention(emb = 512, heads = 8, \n",
    "                               bucket_size = 64, n_hashes = 8, \n",
    "                               causal = False, attn_chunks = None, \n",
    "                               random_rotations_per_head = False, \n",
    "                               attend_across_buckets = True, \n",
    "                               allow_duplicate_attention = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(0,1,(2, 2048, 512)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 256, 256, 64)\n",
      "(2, 256, 256, 64)\n",
      "(2, 256, 256, 64)\n",
      "(2, 256, 512, 64)\n",
      "(2, 256, 256, 512)\n",
      "(2, 256, 128, 256)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [2,256,256,512] vs. [2,256,128,256] [Op:Mul]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-50693e84b27d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/CPU:0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuckets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    821\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 822\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-1988db2f6cf8>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerge_heads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_inputs_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlsh_attn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattn_chunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mattn_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-bd64afade1eb>\u001b[0m in \u001b[0;36mprocess_inputs_chunk\u001b[1;34m(fn, chunks, *args)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprocess_inputs_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mchunked_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput_pair\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0minput_pair\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mchunked_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-bd64afade1eb>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprocess_inputs_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mchunked_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput_pair\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0minput_pair\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mchunked_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    821\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 822\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-8104a8b02b9d>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, qk, v)\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdots\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[0mdots\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdots\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m \u001b[1;36m1e5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mmultiply\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_dispatch_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   6120\u001b[0m         \u001b[1;32mpass\u001b[0m  \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6121\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6122\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6123\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6124\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6604\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6605\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6606\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6607\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [2,256,256,512] vs. [2,256,128,256] [Op:Mul]"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    y, buckets = layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
