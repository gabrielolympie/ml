{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.8389592410982"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "60. + 40 * np.tanh(10000/6268)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General librairies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "## Cosine Distance\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "## Keras utilities\n",
    "import tensorflow.keras\n",
    "import tensorflow\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Conv2D, Lambda,  Dense, Flatten,MaxPooling2D,Dropout, UpSampling2D\n",
    "# from tensorflow.keras.engine.input_layer import Input\n",
    "# from tensorflow.keras.layers import merge\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import numpy.random as rng\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "## Sklearn utilities\n",
    "#### Models\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#### Scores\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#### Feature transformations\n",
    "import umap\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input,Concatenate, Conv2D, Lambda,Flatten, Activation, Dense, BatchNormalization, MaxPooling2D,AveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.activations import softmax\n",
    "\n",
    "\n",
    "## Removing some of the useless warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful functions to ease the processings\n",
    "def save(file,name, folder = \"\"):\n",
    "    if folder != \"\":\n",
    "        outfile = open('./'+folder+'/'+name+'.pickle', 'wb')\n",
    "    else:\n",
    "        outfile = open(name+'.pickle', 'wb')\n",
    "    pickle.dump(file, outfile)\n",
    "    outfile.close\n",
    "    \n",
    "def load(name, folder = \"\"):\n",
    "    if folder != \"\":\n",
    "        outfile = open('./'+folder+'/'+name+'.pickle', 'rb')\n",
    "    else:\n",
    "        outfile = open(name+'.pickle', 'rb')\n",
    "    file = pickle.load(outfile)\n",
    "    outfile.close\n",
    "    return file\n",
    "\n",
    "def plot(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    x = list(range(len(acc)))\n",
    "    plt.plot(x,acc)\n",
    "    plt.plot(x,val_acc)\n",
    "    \n",
    "def cat_to_num(y):\n",
    "    y1 = []\n",
    "    for i in range(y.shape[0]):\n",
    "        a = 0\n",
    "        for j in range(y.shape[1]):\n",
    "            a += j*y[i,j]\n",
    "        y1.append(int(a))\n",
    "    return y1\n",
    "\n",
    "def generate_integer():\n",
    "    \n",
    "    ints = []\n",
    "    \n",
    "    while len(ints)<5:\n",
    "        r = random.randint(0,63)\n",
    "        if not(r in ints):\n",
    "            ints.append(r)\n",
    "    return ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OT = load('training_set')\n",
    "\n",
    "for i in range(5):\n",
    "    plt.figure(i)\n",
    "    fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1,5)\n",
    "    ax1.imshow(OT['data'][600*i+1])\n",
    "    ax2.imshow(OT['data'][600*i+2])    \n",
    "    ax3.imshow(OT['data'][600*i+3])\n",
    "    ax4.imshow(OT['data'][600*i+4])\n",
    "    ax5.imshow(OT['data'][600*i+5])\n",
    "    \n",
    "Y_meta = OT['label']\n",
    "X_meta = OT['data']\n",
    "\n",
    "y_meta = np_utils.to_categorical(Y_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_meta, y_meta, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import resnet\n",
    "build = resnet.ResnetBuilder()\n",
    "model = build.build_resnet_12((32,32,3),64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n",
    "                         width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,\n",
    "                         horizontal_flip=True, fill_mode=\"nearest\")\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "\n",
    "# score = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print('Test loss:', score[0])\n",
    "# print('Test accuracy:', score[1])\n",
    "\n",
    "# save(history, '2.resnet2 with augmentation', 'results')\n",
    "# save(model, '2.resnet2 with augmentation', 'model')\n",
    "# plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(0.1)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0.001, patience=6, verbose=1, mode='auto',\n",
    "    baseline=None, restore_best_weights=True\n",
    ")\n",
    "reduce = tensorflow.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, \n",
    "                                                     mode='auto', min_delta=0.0001, cooldown=0, min_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0.001, patience=6, verbose=1, mode='auto',\n",
    "    baseline=None, restore_best_weights=True\n",
    ")\n",
    "reduce = tensorflow.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, \n",
    "                                                     mode='auto', min_delta=0.0001, cooldown=0, min_lr=0.001)\n",
    "\n",
    "import time\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "t0 = time.time()\n",
    "history = model.fit_generator(aug.flow(X_train, y_train, batch_size=batch_size),\n",
    "    validation_data=(X_test, y_test), steps_per_epoch=len(X_train) // batch_size,\n",
    "    epochs=epochs, callbacks = [stop, reduce])\n",
    "\n",
    "t1 = time.time()\n",
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(model, 'resnet12', 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('resnet12_big.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best score : train 0.7184 test 0.6374"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline distilled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## distilled model\n",
    "\n",
    "\n",
    "inputs = Input(shape = (32,32,3))\n",
    "\n",
    "x = Conv2D(32, kernel_size=(3, 3), activation='relu')(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(64, kernel_size=(3, 3), activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(128, kernel_size=(3, 3), activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "dense = Dense(64)(x)\n",
    "\n",
    "outputs = softmax(dense, axis = -1)\n",
    "\n",
    "distilled_base = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## distilled model\n",
    "\n",
    "\n",
    "inputs = Input(shape = (32,32,3))\n",
    "\n",
    "### Block 1\n",
    "x = block(inputs, 12)\n",
    "\n",
    "### Block 2\n",
    "x = block(x, 12)\n",
    "\n",
    "### Block 3\n",
    "x = block(x, 12)\n",
    "\n",
    "# ### Block 4\n",
    "# x = block(x, 12)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "dense = Dense(64)(x)\n",
    "\n",
    "outputs = softmax(dense, axis = -1)\n",
    "\n",
    "distilled_base = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distilled_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(0.1)\n",
    "\n",
    "distilled_base.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0.001, patience=8, verbose=1, mode='auto',\n",
    "    baseline=None, restore_best_weights=True\n",
    ")\n",
    "reduce = tensorflow.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, \n",
    "                                                     mode='auto', min_delta=0.0001, cooldown=0, min_lr=0.00001)\n",
    "\n",
    "import time\n",
    "batch_size = 32\n",
    "epochs = 40\n",
    "t0 = time.time()\n",
    "history = distilled_base.fit_generator(aug.flow(X_train, y_train, batch_size=batch_size),\n",
    "    validation_data=(X_test, y_test), steps_per_epoch=len(X_train) // batch_size,\n",
    "    epochs=epochs, callbacks = [stop, reduce])\n",
    "\n",
    "t1 = time.time()\n",
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best_score : train : 0.2936 test : 0.2823"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import resnet\n",
    "build = resnet.ResnetBuilder()\n",
    "model_big = build.build_resnet_12((32,32,3),64)\n",
    "model_big.load_weights('resnet12_big.h5')\n",
    "\n",
    "inputs = model_big.input\n",
    "outputs = model_big.layers[-2].output\n",
    "\n",
    "teacher = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logit = teacher.predict(X_train)\n",
    "test_logit = teacher.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_traind = np.concatenate([y_train, train_logit], axis = 1)\n",
    "y_testd = np.concatenate([y_test, test_logit], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature =  5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block(x, nn):\n",
    "    \n",
    "    x = Conv2D(filters=nn, kernel_size=(3, 3), strides=1, padding=\"same\", kernel_initializer=\"he_normal\", \n",
    "           kernel_regularizer=l2(1e-4), activation = 'relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Conv2D(filters=nn, kernel_size=(3, 3), strides=1, padding=\"same\", kernel_initializer=\"he_normal\", \n",
    "           kernel_regularizer=l2(1e-4), activation = 'relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "#     x = Conv2D(filters=nn, kernel_size=(3, 3), strides=1, padding=\"same\", kernel_initializer=\"he_normal\", \n",
    "#            kernel_regularizer=l2(1e-4))(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "    \n",
    "    x = MaxPooling2D()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape = (32,32,3))\n",
    "\n",
    "x = Conv2D(32, kernel_size=(3, 3), activation='relu')(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(64, kernel_size=(3, 3), activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(128, kernel_size=(3, 3), activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "# x = Dense(256, activation='relu')(x)\n",
    "logits = Dense(64)(x)\n",
    "\n",
    "# x = Dense(64, activation = 'relu')(x)\n",
    "# logits = Dense(64)(x)\n",
    "# logits = model.layers[-1].output\n",
    "probabilities = Activation('softmax')(logits)\n",
    "# softed probabilities\n",
    "logits_T = Lambda(lambda x: x/temperature)(logits)\n",
    "probabilities_T = Activation('softmax')(logits_T)\n",
    "\n",
    "output = Concatenate()([probabilities, probabilities_T])\n",
    "distilled = Model(inputs, output)\n",
    "\n",
    "# outputs = softmax(dense, axis = -1)\n",
    "\n",
    "# distilled_base = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape = (32,32,3))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Block 1\n",
    "x = block(inputs, 12)\n",
    "\n",
    "### Block 2\n",
    "x = block(x, 12)\n",
    "\n",
    "### Block 3\n",
    "x = block(x, 12)\n",
    "\n",
    "# ### Block 4\n",
    "# x = block(x, 12)\n",
    "\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(64, activation = 'relu')(x)\n",
    "logits = Dense(64)(x)\n",
    "# logits = model.layers[-1].output\n",
    "probabilities = Activation('softmax')(logits)\n",
    "# softed probabilities\n",
    "logits_T = Lambda(lambda x: x/temperature)(logits)\n",
    "probabilities_T = Activation('softmax')(logits_T)\n",
    "\n",
    "output = Concatenate()([probabilities, probabilities_T])\n",
    "distilled = Model(inputs, output)\n",
    "\n",
    "# outputs = softmax(dense, axis = -1)\n",
    "\n",
    "# distilled_base = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distilled.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import categorical_crossentropy as logloss\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "def knowledge_distillation_loss(y_true, y_pred, lambda_const):    \n",
    "    \n",
    "    # split in \n",
    "    #    onehot hard true targets\n",
    "    #    logits from xception\n",
    "    y_true, logits = y_true[:, :64], y_true[:, 64:]\n",
    "    \n",
    "    # convert logits to soft targets\n",
    "    y_soft = K.softmax(logits/temperature)\n",
    "    \n",
    "    # split in \n",
    "    #    usual output probabilities\n",
    "    #    probabilities made softer with temperature\n",
    "    y_pred, y_pred_soft = y_pred[:, :64], y_pred[:, 64:]    \n",
    "    \n",
    "    return lambda_const*logloss(y_true, y_pred) + logloss(y_soft, y_pred_soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    y_true = y_true[:, :64]\n",
    "    y_pred = y_pred[:, :64]\n",
    "    return categorical_accuracy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lambda_const = 0.8\n",
    "\n",
    "distilled.compile(\n",
    "    optimizer=SGD(lr=0.1, momentum=0.9, nesterov=True), \n",
    "    loss=lambda y_true, y_pred: knowledge_distillation_loss(y_true, y_pred, lambda_const), \n",
    "    metrics=[accuracy]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0.001, patience=8, verbose=1, mode='auto',\n",
    "    baseline=None, restore_best_weights=True\n",
    ")\n",
    "reduce = tensorflow.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, \n",
    "                                                     mode='auto', min_delta=0.0001, cooldown=0, min_lr=0.00001)\n",
    "\n",
    "import time\n",
    "batch_size = 64\n",
    "epochs = 40\n",
    "t0 = time.time()\n",
    "history = distilled.fit_generator(aug.flow(X_train, y_traind, batch_size=batch_size),\n",
    "    validation_data=(X_test, y_testd), steps_per_epoch=len(X_train) // batch_size,\n",
    "    epochs=epochs, callbacks = [stop, reduce])\n",
    "\n",
    "t1 = time.time()\n",
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.3863"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.4160"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
