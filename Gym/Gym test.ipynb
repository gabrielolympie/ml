{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.backend import clear_session\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNCartPoleSolver():\n",
    "    def __init__(self, gamma=1.0, \n",
    "                 epsilon=1.0, epsilon_min=0.01, epsilon_log_decay=0.995, \n",
    "                 lr=0.01, lr_decay=0.01, dense_layers = [24,48], \n",
    "                 batch_size=64, monitor=False, quiet=False,\n",
    "                env_shape = 4, action_space_shape = 2):\n",
    "        self.memory = deque(maxlen=1000000)\n",
    "\n",
    "        self.gamma = gamma\n",
    "    \n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_log_decay\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.quiet = quiet\n",
    "        self.env_shape = env_shape\n",
    "        self.action_space_shape = action_space_shape\n",
    "\n",
    "        # Init model\n",
    "        with tf.device('/device:GPU:0'):\n",
    "            self.model = self.build_model(dense_layers = dense_layers,output_shape = action_space_shape, lr = lr, lr_decay = lr_decay)\n",
    "\n",
    "    \n",
    "    def build_model(self, dense_layers = [24,48],output_shape = 2, lr = 0.01, lr_decay = 0.01):\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Dense(dense_layers[0], input_shape=(4,),activation = 'tanh'))\n",
    "        \n",
    "        for layer in dense_layers[1:]:\n",
    "            model.add(Dense(layer, activation = 'tanh'))\n",
    "        \n",
    "        model.add(Dense(output_shape, activation = 'linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=lr, decay=lr_decay))\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def choose_action(self, state, epsilon, env):\n",
    "        return env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.model.predict_on_batch(state).numpy())\n",
    "\n",
    "    def get_epsilon(self, t):\n",
    "        return max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((t + 1) * self.epsilon_decay)))\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        return np.reshape(state, [1, 4])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        \n",
    "        if len(self.memory) < batch_size:\n",
    "            return 1\n",
    "        \n",
    "        x_batch, y_batch = [], []\n",
    "        \n",
    "        minibatch = random.sample(\n",
    "                self.memory, min(len(self.memory), batch_size))\n",
    "\n",
    "        state_batch = np.array([elt[0][0] for elt in minibatch ])\n",
    "        action_batch = np.array([elt[1] for elt in minibatch])\n",
    "        reward_batch = np.array([elt[2] for elt in minibatch])\n",
    "        next_state_batch = np.array([elt[3][0] for elt in minibatch ])\n",
    "        done_batch = np.array([elt[4] for elt in minibatch])\n",
    "        \n",
    "        with tf.device('/device:GPU:0'):\n",
    "            y_target = self.model.predict_on_batch(state_batch).numpy()\n",
    "            q_target = np.max(self.model.predict_on_batch(next_state_batch).numpy(), axis = 1)\n",
    "\n",
    "        for i in range(y_target.shape[0]):\n",
    "            y_target[i, action_batch[i]] = reward_batch[i] if done_batch[i] else reward_batch[i] + self.gamma * q_target[i]\n",
    "        \n",
    "        with tf.device('/device:GPU:0'):\n",
    "            self.model.fit(state_batch, y_target, batch_size = 32, epochs = 2, verbose = 0)\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "        clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "agent = DQNCartPoleSolver(batch_size = 512, dense_layers = [16,32,64,128,256])\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "scores = deque(maxlen=100)\n",
    "\n",
    "# env.render()\n",
    "for e in range(15000):\n",
    "#     print(e)\n",
    "    env.render()\n",
    "    state = agent.preprocess_state(env.reset())\n",
    "    done = False\n",
    "    i = 0\n",
    "    while not done:\n",
    "        with tf.device('/device:GPU:0'):\n",
    "            action = agent.choose_action(state, agent.get_epsilon(e), env)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = agent.preprocess_state(next_state)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        i += 1\n",
    "\n",
    "    scores.append(i)\n",
    "    mean_score = np.mean(scores)\n",
    "    if mean_score >= 500 and e >= 100:\n",
    "        if not agent.quiet: print('Ran {} episodes. Solved after {} trials âœ”'.format(e, e - 100))\n",
    "        print(e - 100)\n",
    "        break\n",
    "\n",
    "    if e % 100 == 0 and not agent.quiet:\n",
    "        print('[Episode {}] - Mean survival time over last 100 episodes was {} ticks.'.format(e, mean_score))\n",
    "\n",
    "    agent.replay(agent.batch_size)\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
