{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autosklearn.classification\n",
    "import sklearn.model_selection\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "\n",
    "X, y = sklearn.datasets.load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        sklearn.model_selection.train_test_split(X, y, random_state=1)\n",
    "automl = autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task=31)\n",
    "automl.fit(X_train, y_train)\n",
    "y_hat = automl.predict(X_test)\n",
    "\n",
    "print(sklearn.metrics.accuracy_score(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import _pickle as pickle\n",
    "# import tensorflow as tf\n",
    "def save(file,name, folder = \"\"):\n",
    "    if folder != \"\":\n",
    "        outfile = open('./'+folder+'/'+name+'.pickle', 'wb')\n",
    "    else:\n",
    "        outfile = open(name+'.pickle', 'wb')\n",
    "    pickle.dump(file, outfile, protocol=4)\n",
    "    outfile.close\n",
    "    \n",
    "def load(name, folder = \"\"):\n",
    "    if folder != \"\":\n",
    "        outfile = open('./'+folder+'/'+name+'.pickle', 'rb')\n",
    "    else:\n",
    "        outfile = open(name+'.pickle', 'rb')\n",
    "    file = pickle.load(outfile)\n",
    "    outfile.close\n",
    "    return file\n",
    "\n",
    "pd.set_option('display.max_columns', 150)\n",
    "pd.set_option('display.max_rows', 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autosklearn.classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Time series version (date < 400 et > 400)\n",
    "2000000 train values, 323000 test values\n",
    "Max possible 29847\n",
    "\n",
    "xgboost bce loss 500 trees  utility : 2184 / sw : 2389\n",
    "catboost bce loss 2000 trees utility : 2435 / sw : 1845\n",
    "lgb bce loss 29 trees utility : 2277 / sw : 2645"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = pd.read_csv('example_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f = pd.read_csv('features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_feats_tag = {}\n",
    "\n",
    "for i, line in df_f.iterrows():\n",
    "    f = line['feature']\n",
    "    \n",
    "    t = \" \".join(list(df_f.columns[1:][line[df_f.columns[1:]].values]))\n",
    "    \n",
    "    dico_feats_tag[f] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utility_metric(date,weights, resp, action):\n",
    "    import numpy as np\n",
    "    p = []\n",
    "    for i in np.unique(date):\n",
    "        wi = weights[date == i]\n",
    "        ri = resp[date == i]\n",
    "        ai = action[date == i]\n",
    "        pi = np.sum(wi * ri * ai)\n",
    "        p.append(pi)\n",
    "    p = np.array(p)\n",
    "    \n",
    "    nt = np.unique(date).shape[0]\n",
    "#     print(nt)\n",
    "    sp = np.sum(p)\n",
    "    normp = np.sqrt(np.sum(np.square(p)))\n",
    "    t = (sp / normp) * np.sqrt(250/nt)\n",
    "    u = min(max(t,0), 6) * sp\n",
    "    return u\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['weight'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[df['date']<440]\n",
    "test = df[df['date']>=440]\n",
    "\n",
    "train = train.sample(n = train.shape[0])\n",
    "test = test.sample(n = test.shape[0])\n",
    "\n",
    "X_train = train[['weight'] + ['feature_'+str(i) for i in range(130)]]\n",
    "X_test = test[['weight'] + ['feature_'+str(i) for i in range(130)]]\n",
    "\n",
    "date_train = train['date'].values\n",
    "date_test = test['date'].values\n",
    "\n",
    "weights_train = train['weight'].values\n",
    "weights_test = test['weight'].values\n",
    "\n",
    "y_train = train['resp'].values\n",
    "y_test = test['resp'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = (y_test> 0)*1\n",
    "import random\n",
    "utility_metric(date_test,weights_test,y_test, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save((X_train, X_test, y_train, y_test, date_train, date_test, weights_train, weights_test), 'splitted_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, y_train, y_test, date_train, date_test, weights_train, weights_test) = load('splitted_dataset')\n",
    "SAMPLE_WEIGHTS = abs((y_train * weights_train)) + 1\n",
    "sample_weights = abs((np.concatenate([y_train, y_test]) * np.concatenate([weights_train, weights_test]))) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train\n",
    "# X_train['label'] = (y_train > 0)*1\n",
    "y_train1 = (y_train > 0)*1\n",
    "\n",
    "# X_val = X_train[date_train >= 420]\n",
    "# X = X_train[date_train <420]\n",
    "\n",
    "X_test = X_test#[:100]\n",
    "y_test1 = (y_test > 0)*1\n",
    "\n",
    "# X_test['label'] = y_test1\n",
    "\n",
    "# X_test = X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del X_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Params on https://auto.gluon.ai/stable/api/autogluon.task.html#autogluon.task.TabularPrediction\n",
    "\n",
    "import autogluon as ag\n",
    "from autogluon import TabularPrediction as task\n",
    "\n",
    "# dir1 = 'agModels-predictClass'\n",
    "# predictor = task.fit(train_data=X, label='label', output_directory=dir1)\n",
    "\n",
    "presets = ['best_quality',\n",
    "           'best_quality_with_high_quality_refit', #x10 vs best\n",
    "           'high_quality_fast_inference_only_refit', #x10-100 vs best\n",
    "           \n",
    "           'good_quality_faster_inference_only_refit', #x4 vs prec\n",
    "           \n",
    "           'medium_quality_faster_train', #x10 prec\n",
    "           'optimize_for_deployment',\n",
    "    \n",
    "    \n",
    "    \n",
    "           'good_quality_faster_inference_only_refit', \n",
    "           'optimize_for_deployment', \n",
    "           'medium_quality_faster_train',           \n",
    "          ]\n",
    "\n",
    "# hyperparameters = {\n",
    "#     'NN': {}, \n",
    "#     'GBM': [\n",
    "#          {'early_stopping_round' : 20},\n",
    "        \n",
    "#         {'extra_trees': True, 'AG_args': {'name_suffix': 'XT'}},\n",
    "#             ], \n",
    "#     'CAT': {}, \n",
    "#     'RF': [\n",
    "#         {'criterion': 'gini', 'AG_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, \n",
    "#         {'criterion': 'entropy', 'AG_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, \n",
    "#         {'criterion': 'mse', 'AG_args': {'name_suffix': 'MSE', 'problem_types': ['regression']}},\n",
    "#             ], \n",
    "#     'XT': [\n",
    "#         {'criterion': 'gini', 'AG_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, \n",
    "#         {'criterion': 'entropy', 'AG_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, \n",
    "#         {'criterion': 'mse', 'AG_args': {'name_suffix': 'MSE', 'problem_types': ['regression']}},\n",
    "#             ], \n",
    "#     'KNN': [\n",
    "#         {'weights': 'uniform', 'AG_args': {'name_suffix': 'Unif'}}, \n",
    "#         {'weights': 'distance', 'AG_args': {'name_suffix': 'Dist'}},\n",
    "#             ], \n",
    "#     'custom': ['GBM' for elt in range(30)]\n",
    "#     }\n",
    "\n",
    "lgb_params =  {'early_stopping_round' : [20],\n",
    "               'max_depth' : [-1,8,12],\n",
    "             'n_estimators' : [50,100,200,500],\n",
    "             'num_leaves': [6,15,25,45],\n",
    "             'min_child_samples': [100,300,500], \n",
    "             'min_child_weight': [1e-5, 1e-2,  1,  1e2],\n",
    "             'subsample': [0.2,0.4,0.6,0.8], \n",
    "             'colsample_bytree': [0.4,0.5,0.6],\n",
    "             'reg_alpha': [0, 1e-1, 1, 2 ],\n",
    "             'reg_lambda': [0, 1e-1, 1, 5]}\n",
    "\n",
    "n_ensemble = 400\n",
    "hyperparameters = {\n",
    "    'GBM' : []\n",
    "}\n",
    "\n",
    "for i in range(n_ensemble):\n",
    "    hyperparameters['GBM'].append(\n",
    "        {key : np.random.choice(lgb_params[key]) for key in lgb_params}\n",
    "    )\n",
    "# print(hyperparameters)\n",
    "    \n",
    "time_limits = int(3600*3)\n",
    "metric = 'roc_auc'\n",
    "dir1 = 'agModels-predictClass'\n",
    "predictor = task.fit(train_data=X, label='label', \n",
    "                     time_limits=time_limits,\n",
    "                     tuning_data = X_val, \n",
    "                     eval_metric=metric, \n",
    "#                      presets='best_quality', \n",
    "                     output_directory=dir1,\n",
    "                     auto_stack = False,   ## Create stacking\n",
    "                     stack_ensemble_levels = 3,\n",
    "                     hyperparameter_tune = False,  ## Tune the hyperparameters (longer)\n",
    "                     hyperparameters = hyperparameters, # 'default', 'light', 'very_light', 'toy' ot dico\n",
    "                     num_bagging_folds = 0,\n",
    "                     search_strategy = 'skopt',\n",
    "                     num_trials = 50,\n",
    "#                      keep_only_best = False, ## Keep only one model\n",
    "#                      refit_full = False,  ## Refit all models on the whole data\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = task.load(dir1)\n",
    "predictor.persist_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.leaderboard(silent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pred = predictor.predict(X_test.drop(columns = ['label']))\n",
    "print(1)\n",
    "proba = predictor.predict_proba(X_test.drop(columns = ['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.metrics.accuracy_score(y_test1, pred))\n",
    "print(sklearn.metrics.roc_auc_score(y_test1, proba))\n",
    "print(sklearn.metrics.roc_auc_score(y_test1, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action = (pred>0.5)*1\n",
    "action = pred\n",
    "print(utility_metric(date_test,weights_test, y_test, pred))\n",
    "print(utility_metric(date_test,weights_test, y_test, (y_test>0)*1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(predictor.distill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_models = predictor.distill(time_limits=180,\n",
    "                                  hyperparameters = {'GBM': {}},\n",
    "                                   augment_method = None,\n",
    "                                   \n",
    "                                  )  # specify much longer time-limits in real applications\n",
    "print(student_models)\n",
    "preds_student = predictor.predict(X_test.drop(columns = ['label']), model=student_models[0])\n",
    "print(f\"predictions from {student_models[0]}:\", preds_student)\n",
    "predictor.leaderboard(X_test, silent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = './agModels-predictClass/models/LightGBMClassifier/model.pkl'\n",
    "model = './agModels-predictClass/models/weighted_ensemble_k0_l1/model.pkl'\n",
    "model = './agModels-predictClass/models/LightGBMRegressor_DSTL/model.pkl'\n",
    "\n",
    "with open(model, 'rb') as f:\n",
    "    distil = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.drop(columns = ['label']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(distil.preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "for row in range(100):\n",
    "#     a = predictor.predict(X_test.iloc[row:row+1])#, model=student_models[0])\n",
    "    a = distil.predict(X_test.iloc[row:row+1])\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pred = (distil.predict(X_test.drop(columns = ['label'])) >= 0.5)*1\n",
    "proba = distil.predict_proba(X_test.drop(columns = ['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.metrics.accuracy_score(y_test1, pred))\n",
    "print(sklearn.metrics.roc_auc_score(y_test1, proba))\n",
    "print(sklearn.metrics.roc_auc_score(y_test1, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action = (pred>0.5)*1\n",
    "action = pred\n",
    "utility_metric(date_test,weights_test, y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = pred\n",
    "utility_metric(date_test,weights_test, y_test, (y_test>0)*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb bag : full 2671, distilled 2504.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autosklearn.classification\n",
    "help(autosklearn.classification.AutoSklearnClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r ./autosklearn_tmp\n",
    "!rm -r ./autosklearn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autosklearn.classification\n",
    "import autosklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)\n",
    "# X, y = sklearn.datasets.load_digits(return_X_y=True)\n",
    "# X_train, X_test, y_train, y_test = \\\n",
    "#         sklearn.model_selection.train_test_split(X, y, random_state=1)\n",
    "\n",
    "accuracy_scorer = autosklearn.metrics.make_scorer(\n",
    "        name=\"accuracy\",\n",
    "        score_func=autosklearn.metrics.accuracy,\n",
    "        optimum=1,\n",
    "        greater_is_better=True,\n",
    "        needs_proba=False,\n",
    "        needs_threshold=False,\n",
    "    )\n",
    "\n",
    "auc_scorer = autosklearn.metrics.make_scorer(\n",
    "        name=\"roc_auc\",\n",
    "        score_func=autosklearn.metrics.roc_auc,\n",
    "        optimum=1,\n",
    "        greater_is_better=True,\n",
    "        needs_proba=True,\n",
    "        needs_threshold=False,\n",
    "    )\n",
    "\n",
    "\n",
    "## Start\n",
    "automl = autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task=3600*5,\n",
    "                                                          n_jobs = 2,\n",
    "                                                          memory_limit = 12000,\n",
    "                                                          tmp_folder  = './autosklearn_tmp',\n",
    "                                                        output_folder = './autosklearn_output',\n",
    "                                                          metric = auc_scorer,\n",
    "                                                          scoring_functions  = [accuracy_scorer, auc_scorer],\n",
    "                                                         )\n",
    "\n",
    "automl.fit(X_train, y_train1, X_test = X_test, y_test = y_test1)\n",
    "\n",
    "y_hat = automl.predict(X_test)\n",
    "\n",
    "print(sklearn.metrics.accuracy_score(y_test1, y_hat))\n",
    "print(sklearn.metrics.roc_auc_score(y_test1, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(au)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = automl.predict(X_test)\n",
    "y_proba = automl.predict_proba(X_test)[:,1]\n",
    "print(sklearn.metrics.accuracy_score(y_test1, y_hat))\n",
    "print(sklearn.metrics.roc_auc_score(y_test1, y_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action = (pred>0.5)*1\n",
    "# action = pred\n",
    "utility_metric(date_test,weights_test, y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in tqdm(range(10)):\n",
    "    a = automl.predict(X_test[i:i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "20000/14.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "14.6/20000*2000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "clf = lgb.LGBMClassifier(max_depth = -1, n_estimators = 20000, n_jobs = 12, silent = False, early_stopping_rounds = 20)\n",
    "clf.fit(X_train, (y_train>0)*1, eval_set =(X_test, (y_test>0)*1), eval_metric = 'auc', sample_weight=SAMPLE_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lightgbm as lgb\n",
    "# temp = 100\n",
    "# clf = lgb.LGBMRegressor(max_depth = -1, n_estimators = 2000, n_jobs = 12, silent = False, early_stoping_rounds = 20)\n",
    "# clf.fit(X_train, y_train*temp, eval_set =(X_test, y_test*temp), eval_metric = 'mse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "clf = xgb.XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=11,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.7,\n",
    "    random_state=2020,\n",
    "    tree_method='gpu_hist',\n",
    "    verbosity = 2\n",
    "    \n",
    ")\n",
    "\n",
    "clf.fit(X_train.values, (y_train>0)*1, eval_metric=\"auc\", eval_set=[(X_test.values,(y_test>0)*1)], early_stopping_rounds=10, sample_weight=SAMPLE_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "clf = CatBoostClassifier(learning_rate=0.1, n_estimators=2000, early_stopping_rounds = 15, task_type=\"GPU\", eval_metric = 'AUC')\n",
    "clf.fit(X_train, (y_train>0)*1, eval_set =(X_test, (y_test>0)*1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "\n",
    "kind = np.random.choice(range(X_train.shape[0]), size = 100000)\n",
    "kind_test = np.random.choice(range(X_test.shape[0]), size = 5000)\n",
    "\n",
    "X_train1 = X_train[kind]\n",
    "y_train1 = y_train[kind]\n",
    "\n",
    "X_test1 = X_test[kind_test]\n",
    "y_test1 = y_test[kind_test]\n",
    "\n",
    "\n",
    "X = np.concatenate([X_train1, X_test1])\n",
    "y = np.concatenate([y_train1, y_test1])\n",
    "\n",
    "train_idx = np.array(range(X_train1.shape[0]))\n",
    "test_idx = np.array(range(X_test1.shape[0])) + train_idx.max()\n",
    "\n",
    "split = [(train_idx, test_idx)]\n",
    "\n",
    "sample_weights = abs((np.concatenate([y_train1, y_test1]) * np.concatenate([X_train1[:,0], X_test1[:,0]]))) + 1\n",
    "sample_weights1 = abs(y_train1 * X_train1[:,0]) + 1\n",
    "# def cv():\n",
    "#     train_idx = np.array(range(X_train.shape[0]))\n",
    "#     test_idx = np.array(range(X_test.shape[0])) + train_idx.max()\n",
    "#     yield train_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save((X, y, sample_weights, split), 'tpot_bench_data')\n",
    "# (X, y, sample_weights, split)=load('tpot_bench_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Check the TPOT documentation for information on the structure of config dicts\n",
    "classifier_config_dict_light = {\n",
    "\n",
    "    \n",
    "    # Classifiers\n",
    "#     'sklearn.naive_bayes.GaussianNB': {\n",
    "#     },\n",
    "\n",
    "#     'sklearn.naive_bayes.BernoulliNB': {\n",
    "#         'alpha': [1e-3, 1e-2, 1e-1, 1., 10., 100.],\n",
    "#         'fit_prior': [True, False]\n",
    "#     },\n",
    "\n",
    "#     'sklearn.naive_bayes.MultinomialNB': {\n",
    "#         'alpha': [1e-3, 1e-2, 1e-1, 1., 10., 100.],\n",
    "#         'fit_prior': [True, False]\n",
    "#     },\n",
    "\n",
    "#     'sklearn.tree.DecisionTreeClassifier': {\n",
    "#         'criterion': [\"gini\", \"entropy\"],\n",
    "#         'max_depth': range(1, 11),\n",
    "#         'min_samples_split': range(2, 21),\n",
    "#         'min_samples_leaf': range(1, 21)\n",
    "#     },\n",
    "\n",
    "\n",
    "#     'sklearn.neighbors.KNeighborsClassifier': {\n",
    "#         'n_neighbors': range(1, 101),\n",
    "#         'weights': [\"uniform\", \"distance\"],\n",
    "#         'p': [1, 2]\n",
    "#     },\n",
    "\n",
    "\n",
    "#     'sklearn.linear_model.LogisticRegression': {\n",
    "#         'penalty': [\"l1\", \"l2\"],\n",
    "#         'C': [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.],\n",
    "#         'dual': [True, False]\n",
    "#     },\n",
    "\n",
    "#     'sklearn.ensemble.ExtraTreesClassifier': {\n",
    "#         'n_estimators': [100],\n",
    "#         'criterion': [\"gini\", \"entropy\"],\n",
    "#         'max_features': np.arange(0.05, 1.01, 0.05),\n",
    "#         'min_samples_split': range(2, 21),\n",
    "#         'min_samples_leaf': range(1, 21),\n",
    "#         'bootstrap': [True, False]\n",
    "#     },\n",
    "\n",
    "#     'sklearn.ensemble.RandomForestClassifier': {\n",
    "#         'n_estimators': [100],\n",
    "#         'criterion': [\"gini\", \"entropy\"],\n",
    "#         'max_features': np.arange(0.05, 1.01, 0.05),\n",
    "#         'min_samples_split': range(2, 21),\n",
    "#         'min_samples_leaf':  range(1, 21),\n",
    "#         'bootstrap': [True, False]\n",
    "#     },\n",
    "\n",
    "#     'sklearn.ensemble.GradientBoostingClassifier': {\n",
    "#         'n_estimators': [100],\n",
    "#         'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],\n",
    "#         'max_depth': range(1, 11),\n",
    "#         'min_samples_split': range(2, 21),\n",
    "#         'min_samples_leaf': range(1, 21),\n",
    "#         'subsample': np.arange(0.05, 1.01, 0.05),\n",
    "#         'max_features': np.arange(0.05, 1.01, 0.05)\n",
    "#     },\n",
    "    \n",
    "    'xgboost.XGBClassifier': {\n",
    "        'n_estimators': [100,300,500],\n",
    "        'max_depth': range(1, 11),\n",
    "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],\n",
    "        'subsample': np.arange(0.05, 1.01, 0.05),\n",
    "        'min_child_weight': range(1, 21),\n",
    "        'n_jobs': [1],\n",
    "        'verbosity': [0]\n",
    "    },\n",
    "\n",
    "#     'sklearn.linear_model.SGDClassifier': {\n",
    "#         'loss': ['log', 'hinge', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "#         'penalty': ['elasticnet'],\n",
    "#         'alpha': [0.0, 0.01, 0.001],\n",
    "#         'learning_rate': ['invscaling', 'constant'],\n",
    "#         'fit_intercept': [True, False],\n",
    "#         'l1_ratio': [0.25, 0.0, 1.0, 0.75, 0.5],\n",
    "#         'eta0': [0.1, 1.0, 0.01],\n",
    "#         'power_t': [0.5, 0.0, 1.0, 0.1, 100.0, 10.0, 50.0]\n",
    "#     },\n",
    "\n",
    "#     'sklearn.neural_network.MLPClassifier': {\n",
    "#         'alpha': [1e-4, 1e-3, 1e-2, 1e-1],\n",
    "#         'learning_rate_init': [1e-3, 1e-2, 1e-1, 0.5, 1.]\n",
    "#     },\n",
    "\n",
    "    'lightgbm.LGBMClassifier':\n",
    "        {\n",
    "            'n_estimators': [100,300,500,700],\n",
    "            'num_leaves' : [10,30,60],\n",
    "            'min_child_samples' : [100,200,300,400,500],\n",
    "            'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "            'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "            'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100],\n",
    "            'early_stopping_rounds' : [None, 50,100,200]\n",
    "    },\n",
    "       \n",
    "    # Preprocesssors\n",
    "    'sklearn.preprocessing.Binarizer': {\n",
    "        'threshold': np.arange(0.0, 1.01, 0.05)\n",
    "    },\n",
    "\n",
    "    'sklearn.cluster.FeatureAgglomeration': {\n",
    "        'linkage': ['ward', 'complete', 'average'],\n",
    "        'affinity': ['euclidean', 'l1', 'l2', 'manhattan', 'cosine']\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.MaxAbsScaler': {\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.MinMaxScaler': {\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.Normalizer': {\n",
    "        'norm': ['l1', 'l2', 'max']\n",
    "    },\n",
    "\n",
    "    'sklearn.decomposition.PCA': {\n",
    "        'svd_solver': ['randomized'],\n",
    "        'iterated_power': range(1, 11)\n",
    "    },\n",
    "\n",
    "    'sklearn.kernel_approximation.RBFSampler': {\n",
    "        'gamma': np.arange(0.0, 1.01, 0.05)\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.RobustScaler': {\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.StandardScaler': {\n",
    "    },\n",
    "\n",
    "    'tpot.builtins.ZeroCount': {\n",
    "    },\n",
    "\n",
    "    # Selectors\n",
    "    'sklearn.feature_selection.SelectFwe': {\n",
    "        'alpha': np.arange(0, 0.05, 0.001),\n",
    "        'score_func': {\n",
    "            'sklearn.feature_selection.f_classif': None\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'sklearn.feature_selection.SelectPercentile': {\n",
    "        'percentile': range(1, 100),\n",
    "        'score_func': {\n",
    "            'sklearn.feature_selection.f_classif': None\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'sklearn.feature_selection.VarianceThreshold': {\n",
    "        'threshold': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2]\n",
    "    }\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Time bench gen 5 pop size 10 job 1 \n",
    "## Test 3 000 \n",
    "\n",
    "nb : 0.495 / 0.03  0.355\n",
    "bernoullinb : 1.49 / 0.047  10\n",
    "multinomialnb: fail\n",
    "decisiontree: 13.7s / 0.003 7.58\n",
    "knn: 27.4s / 1.05s 76\n",
    "logreg: 6 / 0.016  25\n",
    "extratree : 37 / 1.12\n",
    "randforest : slow\n",
    "gradientboost : 78s/1.78\n",
    "xgbclass : 60 / 0.8\n",
    "sgdclass :2.71 / 0.3\n",
    "mlpclass : 38.8 / 0.7\n",
    "lgbclass : 7.47 /0.1\n",
    "    \n",
    "( with nb)\n",
    "binarizer:  0.852 / 57.3    7\n",
    "feature_agglom: 0.750 / 0.014    25\n",
    "maxabs: 0.35\n",
    "minmax:  0.35\n",
    "normalizer: 20.4\n",
    "pca: 0.035\n",
    "rbfsampler: 24.18\n",
    "robustscaler: 0.35\n",
    "stdscaler: 0.35\n",
    "zerocount: 0.35\n",
    "selectfwe: 0.35\n",
    "selectpercentile: 3.75\n",
    "variancetres : 0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "clf = TPOTClassifier(generations=15, population_size=50, \n",
    "                     offspring_size=None, mutation_rate=0.9, \n",
    "                     crossover_rate=0.1, scoring='roc_auc', cv=split, \n",
    "                     subsample=1.0, n_jobs=6, \n",
    "                     max_time_mins=200, max_eval_time_mins=2, \n",
    "                     random_state=None, \n",
    "                     config_dict=classifier_config_dict_light, \n",
    "                     template=None, warm_start=True, \n",
    "                     memory=None, use_dask=False, \n",
    "                     periodic_checkpoint_folder=None, \n",
    "                     early_stop=20, verbosity=2, \n",
    "                     disable_update_check=False, log_file=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf.fit(X, (y>0)*1, sample_weight=sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(clf, 'tpot_end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fitted_pipeline_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "end = X_test.shape[0]\n",
    "pred = clf.predict(X_test[:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action = (pred>0.5)*1\n",
    "action = pred\n",
    "utility_metric(date_test[:end],weights_test[:end], y_test[:end], action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.export('best pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_metric(date_test[:end],weights_test[:end], y_test[:end], ( y_test[:end]>0)*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tpot.builtins import StackingEstimator\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from copy import copy\n",
    "\n",
    "exported_pipeline = make_pipeline(\n",
    "    make_union(\n",
    "        FunctionTransformer(copy),\n",
    "        make_pipeline(\n",
    "            PCA(iterated_power=7, svd_solver=\"randomized\"),\n",
    "            RobustScaler()\n",
    "        )\n",
    "    ),\n",
    "    LGBMClassifier(early_stopping_rounds=None, min_child_samples=100, \n",
    "                   min_child_weight=0.001, n_estimators=100, \n",
    "                   num_leaves=10, reg_alpha=1, reg_lambda=50, \n",
    "                   silent = False, n_jobs = 12)\n",
    ")\n",
    "\n",
    "exported_pipeline.fit(X_train, (y_train>0)*1, lgbmclassifier__sample_weight=SAMPLE_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "clf = BaggingClassifier(base_estimator=make_pipeline(\n",
    "                    make_union(\n",
    "                        FunctionTransformer(copy),\n",
    "                        make_pipeline(\n",
    "                            PCA(iterated_power=7, svd_solver=\"randomized\"),\n",
    "                            RobustScaler()\n",
    "                        )\n",
    "                    ),\n",
    "                    LGBMClassifier(early_stopping_rounds=None, min_child_samples=100, \n",
    "                                   min_child_weight=0.001, n_estimators=100, \n",
    "                                   num_leaves=10, reg_alpha=1, reg_lambda=50, \n",
    "                                   silent = False, n_jobs = 12)), \n",
    "            n_estimators=20, random_state=0)\n",
    "\n",
    "clf.fit(X_train, (y_train>0)*1)#, sample_weight=SAMPLE_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "end = X_test.shape[0]\n",
    "pred = automl.predict(X_test[:end])\n",
    "\n",
    "action = pred\n",
    "utility_metric(date_test[:end],weights_test[:end], y_test[:end], action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(automl, 'autosklearn_4_h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "clf = lgb.LGBMClassifier(n_estimators=1000, n_jobs = 12, silent = False, \n",
    "                         min_child_samples=500, min_child_weight=10.0,\n",
    "                         num_leaves=30, reg_alpha=0, reg_lambda=100,\n",
    "                         early_stopping_rounds = 200)\n",
    "clf.fit(X_train1, (y_train1>0)*1, eval_set =(X_test1, (y_test1>0)*1), eval_metric = 'auc', sample_weight=sample_weights1)\n",
    "\n",
    "pred = clf.predict(X_test[:end])\n",
    "\n",
    "action = pred\n",
    "utility_metric(date_test[:end],weights_test[:end], y_test[:end], action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "# policy = mixed_precision.Policy('mixed_float16')\n",
    "policy = mixed_precision.Policy('float32')\n",
    "mixed_precision.set_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, y_train, y_test, date_train, date_test, weights_train, weights_test) = load('splitted_dataset')\n",
    "SAMPLE_WEIGHTS = abs((y_train * weights_train)) + 1\n",
    "sample_weights = abs((np.concatenate([y_train, y_test]) * np.concatenate([weights_train, weights_test]))) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "# X_train['label'] = (y_train > 0)*1\n",
    "y_train1 = (y_train > 0)*1\n",
    "\n",
    "# X_val = X_train[date_train >= 420]\n",
    "# X = X_train[date_train <420]\n",
    "\n",
    "X_test = X_test.values#[:100]\n",
    "y_test1 = (y_test > 0)*1\n",
    "\n",
    "# X_test['label'] = y_test1\n",
    "\n",
    "# X_test = X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Masking and scaling\n",
    "import tensorflow as tf\n",
    "with tf.device('/CPU:0'):\n",
    "    X_train = tf.keras.activations.sigmoid(X_train).numpy()\n",
    "    X_test = tf.keras.activations.sigmoid(X_test).numpy()\n",
    "\n",
    "# mask_train = np.random.choice([0,1], p = [0.15,0.85],size = (X_train.shape[0], X_train.shape[1]))\n",
    "# mask_test = np.random.choice([0,1], p = [0.15,0.85],size = (X_test.shape[0], X_test.shape[1]))\n",
    "\n",
    "# X_train, X_train_tar = X_train*mask_train, (1-mask_train)*X_train\n",
    "# X_test, X_test_tar = X_test*mask_test, (1-mask_test)*X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabnet2 import *\n",
    "import tensorflow as tf\n",
    "\n",
    "size = X_train.shape[1]\n",
    "\n",
    "# node = NODE(n_layers=1, units=1, depth=7, n_trees=80, link=tf.keras.activations.sigmoid)\n",
    "\n",
    "tabnet_encoder = TabNet(num_features = size,  feature_dim = 256,   output_dim = 128,\n",
    "        feature_columns = None,  n_step = 4,   n_total = 4,  n_shared = 2,\n",
    "        relaxation_factor = 0.82,  bn_epsilon = 1e-5,  bn_momentum = 0.7,  bn_virtual_divider = 32\n",
    "    )\n",
    "\n",
    "tabnet_decoder = TabNet(num_features = 128,  feature_dim = 256,   output_dim = 128,\n",
    "        feature_columns = None,  n_step = 4,   n_total = 4,  n_shared = 2,\n",
    "        relaxation_factor = 0.82,  bn_epsilon = 1e-5,  bn_momentum = 0.7,  bn_virtual_divider = 32\n",
    "    )\n",
    "\n",
    "inputs_num = tf.keras.Input(shape = (size,), dtype = 'float32')#, batch_size = 1)\n",
    "\n",
    "encoded, masks, loss_enc = tabnet_encoder(inputs_num, training = True)\n",
    "decoded, masks_dec, loss_dec = tabnet_decoder(encoded, training = True)\n",
    "\n",
    "pred = tf.keras.layers.Dense(size, activation = 'sigmoid')(decoded)\n",
    "model = tf.keras.Model(inputs_num, pred)\n",
    "model.add_loss(0.001*loss_enc)\n",
    "model.add_loss(0.001*loss_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 131)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tab_net (TabNet)                ((None, 128), [(1, N 3542844     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tab_net_1 (TabNet)              ((None, 128), [(1, N 3537408     tab_net[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 131)          16899       tab_net_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul (TensorFlowOpLa [()]                 0           tab_net[0][5]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_loss (AddLoss)              ()                   0           tf_op_layer_Mul[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_1 (TensorFlowOp [()]                 0           tab_net_1[0][5]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_loss_1 (AddLoss)            ()                   0           tf_op_layer_Mul_1[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 7,097,151\n",
      "Trainable params: 7,033,121\n",
      "Non-trainable params: 64,030\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.CategoricalCrossentropy(\n",
    "                        from_logits=False, label_smoothing=0, reduction=tf.keras.losses.Reduction.NONE,\n",
    "                        name='categorical_crossentropy'\n",
    "                    )\n",
    "\n",
    "def masked_cat_loss(y_true, y_pred):\n",
    "    mask = tf.cast(tf.math.logical_not(tf.math.equal(y_true, 0)),dtype = y_pred.dtype)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    \n",
    "    y_pred *= mask\n",
    "    \n",
    "#     print(y_true)\n",
    "#     print(y_pred)\n",
    "    loss = loss_object(y_true, y_pred)\n",
    "    loss = tf.reduce_sum( - tf.math.log(1 - tf.math.minimum(tf.math.abs(y_true - y_pred), 0.99)), axis = -1)\n",
    "#     print(loss)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def masked_mse_loss(y_true, y_pred):\n",
    "    mask = tf.cast(tf.math.logical_not(tf.math.equal(y_true, 0)),dtype = y_pred.dtype)\n",
    "    \n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    \n",
    "    loss = tf.reduce_sum(tf.pow((y_pred - y_true)*mask, 2), axis = -1)\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "208/208 [==============================] - 276s 1s/step - loss: 335.9805 - val_loss: 317.5090\n",
      "Epoch 2/1000\n",
      "208/208 [==============================] - 230s 1s/step - loss: 332.7187 - val_loss: 316.0152\n",
      "Epoch 3/1000\n",
      "208/208 [==============================] - 216s 1s/step - loss: 331.6372 - val_loss: 315.2913\n",
      "Epoch 4/1000\n",
      "208/208 [==============================] - 208s 1s/step - loss: 331.0882 - val_loss: 315.0257\n",
      "Epoch 5/1000\n",
      "208/208 [==============================] - 214s 1s/step - loss: 330.8979 - val_loss: 314.6793\n",
      "Epoch 6/1000\n",
      "208/208 [==============================] - 220s 1s/step - loss: 330.6208 - val_loss: 314.5102\n",
      "Epoch 7/1000\n",
      "208/208 [==============================] - 213s 1s/step - loss: 330.4488 - val_loss: 314.4377\n",
      "Epoch 8/1000\n",
      "208/208 [==============================] - 212s 1s/step - loss: 330.4380 - val_loss: 314.2663\n",
      "Epoch 9/1000\n",
      "208/208 [==============================] - 221s 1s/step - loss: 330.3317 - val_loss: 314.2026\n",
      "Epoch 10/1000\n",
      "208/208 [==============================] - 222s 1s/step - loss: 330.0868 - val_loss: 314.0133\n",
      "Epoch 11/1000\n",
      "208/208 [==============================] - 207s 996ms/step - loss: 329.9642 - val_loss: 313.9308\n",
      "Epoch 12/1000\n",
      "208/208 [==============================] - 214s 1s/step - loss: 329.9080 - val_loss: 313.8877\n",
      "Epoch 13/1000\n",
      "208/208 [==============================] - 227s 1s/step - loss: 329.8261 - val_loss: 313.8214\n",
      "Epoch 14/1000\n",
      "208/208 [==============================] - 217s 1s/step - loss: 329.8705 - val_loss: 313.8647\n",
      "Epoch 15/1000\n",
      "208/208 [==============================] - 221s 1s/step - loss: 329.7873 - val_loss: 313.8049\n",
      "Epoch 16/1000\n",
      "208/208 [==============================] - 222s 1s/step - loss: 329.7946 - val_loss: 313.9000\n",
      "Epoch 17/1000\n",
      "208/208 [==============================] - 209s 1s/step - loss: 329.8354 - val_loss: 313.8940\n",
      "Epoch 18/1000\n",
      "208/208 [==============================] - 220s 1s/step - loss: 329.7532 - val_loss: 313.7388\n",
      "Epoch 19/1000\n",
      "208/208 [==============================] - 216s 1s/step - loss: 329.7590 - val_loss: 313.8231\n",
      "Epoch 20/1000\n",
      "208/208 [==============================] - 216s 1s/step - loss: 329.7520 - val_loss: 313.7487\n",
      "Epoch 21/1000\n",
      "208/208 [==============================] - 222s 1s/step - loss: 329.7267 - val_loss: 313.7236\n",
      "Epoch 22/1000\n",
      "208/208 [==============================] - 222s 1s/step - loss: 330.0449 - val_loss: 313.9685\n",
      "Epoch 23/1000\n",
      "208/208 [==============================] - 223s 1s/step - loss: 329.9862 - val_loss: 313.8431\n",
      "Epoch 24/1000\n",
      "208/208 [==============================] - ETA: 0s - loss: 329.9597\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "208/208 [==============================] - 223s 1s/step - loss: 329.9597 - val_loss: 313.8476\n",
      "Epoch 25/1000\n",
      " 90/208 [===========>..................] - ETA: 2:01 - loss: 329.7427"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "model.compile(\n",
    "        loss = 'categorical_crossentropy',\n",
    "#         loss = masked_cat_loss, #'mse',\n",
    "        optimizer = Adam(0.01),\n",
    "#         metrics = ['accuracy', 'AUC'],\n",
    ")\n",
    "\n",
    "batch_size = 8192\n",
    "epochs = 1000\n",
    "\n",
    "ls = X_train.shape[0] // batch_size * batch_size\n",
    "lt = X_test.shape[0] // batch_size * batch_size\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "datas = [date_test[:lt], weights_test[:lt], y_test[:lt]]\n",
    "\n",
    "\n",
    "early = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=8, verbose=1, \n",
    "                                                mode='min', restore_best_weights=True)\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, \n",
    "                           mode='min', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "# util = UtilityCallback(validation = (X_test[:lt].values, (y_test[:lt]>0)*1),batch_size = batch_size, datas = datas)\n",
    "callbacks =[early, reduce]\n",
    "# callbacks = []\n",
    "\n",
    "\n",
    "\n",
    "model.fit(X_train[:ls],X_train[:ls], validation_data = (X_test[:lt], X_test[:lt]), \n",
    "          batch_size=batch_size, epochs=epochs, callbacks = callbacks,\n",
    "         sample_weight = SAMPLE_WEIGHTS[:ls],\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabnet_encoder.save_weights('./tabencoder_unmasked/weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabnet2 import *\n",
    "import tensorflow as tf\n",
    "\n",
    "size = X_train.shape[1]\n",
    "\n",
    "# node = NODE(n_layers=1, units=1, depth=7, n_trees=80, link=tf.keras.activations.sigmoid)\n",
    "\n",
    "tabnet_encoder = TabNet(num_features = size,  feature_dim = 256,   output_dim = 128,\n",
    "        feature_columns = None,  n_step = 4,   n_total = 4,  n_shared = 2,\n",
    "        relaxation_factor = 0.82,  bn_epsilon = 1e-5,  bn_momentum = 0.7,  bn_virtual_divider = 32\n",
    "        )\n",
    "\n",
    "tabnet_encoder.load_weights('./tabencoder/weights')\n",
    "\n",
    "# for layer in tabnet_encoder.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "inputs_num = tf.keras.Input(shape = (size,), dtype = 'float32')#, batch_size = 1)\n",
    "\n",
    "encoded, masks, loss_enc = tabnet_encoder(inputs_num, training = False)\n",
    "\n",
    "pred = tf.keras.layers.Dense(1, activation = 'sigmoid')(encoded)\n",
    "\n",
    "model = tf.keras.Model(inputs_num, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./tabnetcl/weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "model.compile(\n",
    "        loss = 'binary_crossentropy',\n",
    "#         loss = masked_cat_loss, #'mse',\n",
    "        optimizer = Adam(0.01),\n",
    "        metrics = ['accuracy', 'AUC'],\n",
    ")\n",
    "\n",
    "batch_size = 8192\n",
    "epochs = 1000\n",
    "\n",
    "ls = X_train.shape[0] // batch_size * batch_size\n",
    "lt = X_test.shape[0] // batch_size * batch_size\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "early = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=8, verbose=1, \n",
    "                                                mode='min', restore_best_weights=True)\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, \n",
    "                           mode='min', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "# util = UtilityCallback(validation = (X_test[:lt].values, (y_test[:lt]>0)*1),batch_size = batch_size, datas = datas)\n",
    "callbacks =[early, reduce]\n",
    "# callbacks = []\n",
    "\n",
    "\n",
    "\n",
    "model.fit(X_train[:ls],y_train1[:ls], validation_data = (X_test[:lt], y_test1[:lt]), \n",
    "          batch_size=batch_size, epochs=epochs, callbacks = callbacks,\n",
    "         sample_weight = SAMPLE_WEIGHTS[:ls],\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./tabnetcl/weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(tabnet_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test, verbose = 1, batch_size = 8192)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pred, bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "fpr, tpr, thresholds = roc_curve((y_test>0)*1, pred)\n",
    "plt.plot(fpr, tpr, marker='.', label='Logistic')\n",
    "gmeans = np.sqrt(tpr * (1-fpr))\n",
    "ix = np.argmax(gmeans)\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "roc_auc_score((y_test>0)*1, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = (pred >= thresholds[ix])*1\n",
    "utility_metric(date_test,weights_test, y_test, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, y_train, y_test, date_train, date_test, weights_train, weights_test, train_feats, test_feats) =load('dataset_with_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.optimizer.set_jit(True)\n",
    "size = 132# X_train.shape[1]\n",
    "inputs = tf.keras.Input(shape = (size,), dtype = 'float32')\n",
    "training = False\n",
    "x = tf.keras.layers.Dense(1024, activation = 'relu')(inputs)\n",
    "x = tf.keras.layers.Dense(1024, activation = 'relu')(x)\n",
    "# x = tf.keras.layers.BatchNormalization()(x, training = training)\n",
    "x = tf.keras.layers.Dense(512, activation = 'relu')(x)\n",
    "x = tf.keras.layers.Dense(512, activation = 'relu')(x)\n",
    "# x = tf.keras.layers.BatchNormalization()(x, training = training)\n",
    "x = tf.keras.layers.Dense(256, activation = 'relu')(x)\n",
    "x = tf.keras.layers.Dense(256, activation = 'relu')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x, training = training)\n",
    "x = tf.keras.layers.Dense(128, activation = 'sigmoid')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./emb_model/weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "model.compile(\n",
    "        loss = 'categorical_crossentropy',\n",
    "#         loss = 'binary_crossentropy',\n",
    "#         loss = 'mse',\n",
    "        optimizer = Adam(0.01),\n",
    "        metrics = ['accuracy', \"mae\"],\n",
    ")\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "early = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=8, verbose=1, \n",
    "                                                mode='min', restore_best_weights=True)\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, \n",
    "                           mode='min', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "callbacks = [early, reduce]\n",
    "# callbacks = []\n",
    "\n",
    "batch_size = 512\n",
    "epochs = 1000\n",
    "\n",
    "history = model.fit(X_train.values, train_feats, validation_data = (X_test.values, test_feats), epochs= epochs, batch_size=batch_size, callbacks = callbacks)\n",
    "# history = model.fit(X_train.values, (y_train>0)*1, validation_data = (X_test.values, (y_test>0)*1), epochs= epochs, batch_size=batch_size, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./emb_model/weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = model.predict(X_train.values, verbose = 1, batch_size = 512)\n",
    "vf = model.predict(X_test.values, verbose = 1, batch_size = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save((tf, vf), 'nn_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tf, vf) = load('nn_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(vf[21], bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "clf = CatBoostClassifier(learning_rate=0.1, n_estimators=20000, early_stopping_rounds = 15, task_type=\"GPU\", eval_metric = 'AUC')\n",
    "clf.fit(tf, (y_train>0)*1, eval_set =(vf, (y_test>0)*1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "clf = lgb.LGBMClassifier(max_depth = -1, n_estimators = 30000, n_jobs = 12, silent = False, early_stopping_rounds = 20)\n",
    "clf.fit(tf, (y_train>0)*1, eval_set =(vf, (y_test>0)*1), eval_metric = 'auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(clf, 'lightgbm_sup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = load('lightgbm_sup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_compile=True)\n",
    "def model_fn(x):\n",
    "    return model(x, training = False)\n",
    "\n",
    "# def create_and_run_graph():\n",
    "#     x = tf.placeholder(tf.float32, name='x')\n",
    "#     result = tf.xla.experimental.compile(computation=model_fn, inputs=(x))[0]\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn(X_test[0:1][cols].values.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "cols = ['date', 'weight'] + ['feature_'+str(i) for i in range(130)]\n",
    "actions = []\n",
    "lt = 20000\n",
    "for i in tqdm(range(lt)):\n",
    "    x = X_test[i:i+1]\n",
    "    x = x[cols].values.astype('float32')\n",
    "    x[np.isnan(x)] = -100\n",
    "    with tf.device('/GPU:0'):\n",
    "        e = model(x, training = False)\n",
    "    p = clf.predict(e)\n",
    "    actions.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = np.array(actions)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = x[cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1[np.isnan(x1)] = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action = (pred>0.505679)*1\n",
    "# action = clf.predict(vf)\n",
    "# action = (y_test[:lt]>0)*1\n",
    "# lt = test_feats.shape[0]\n",
    "utility_metric(date_test[:lt],weights_test[:lt], y_test[:lt], action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = X_test.values.astype('float32')\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    for i in tqdm(range(500)):\n",
    "        p = model(x[i:i+1, :size], training = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('./checkpoints/tabnet1/tabnet_val_auc_0.8131')\n",
    "model.load_weights('./checkpoints/int/weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve((y_test[:lt]>0)*1, pred)\n",
    "plt.plot(fpr, tpr, marker='.', label='Logistic')\n",
    "gmeans = np.sqrt(tpr * (1-fpr))\n",
    "ix = np.argmax(gmeans)\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "roc_auc_score((y_test[:lt]>0)*1, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(y_test[:lt]>0)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action = (pred>0.505679)*1\n",
    "action = clf.predict(test_feats)\n",
    "# action = (y_test[:lt]>0)*1\n",
    "lt = test_feats.shape[0]\n",
    "utility_metric(date_test[:lt],weights_test[:lt], y_test[:lt], action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pred, bins = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "def objective(trial):\n",
    "    x = trial.suggest_uniform('x', 0, 1)\n",
    "    action = (pred>x)*1\n",
    "    utility = accuracy_score((y_test[:lt]>0), (pred >x)*1)     #utility_metric(date_test[:lt],weights_test[:lt], y_test[:lt], action)\n",
    "    print(utility)\n",
    "    return -utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "def objective(trial):\n",
    "    x = trial.suggest_uniform('x', 0, 1)\n",
    "    action = action = (pred>x)*1\n",
    "    utility = utility_metric(date_test[:lt],weights_test[:lt], y_test[:lt], action)\n",
    "    print(utility)\n",
    "    return -utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tres = study.best_params['x']\n",
    "print(tres)\n",
    "action = (pred>tres)*1\n",
    "utility_metric(date_test[:lt],weights_test[:lt], y_test[:lt], action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark\n",
    "max possible : 45093.72\n",
    "lgb  mse est : 2000 t = 100 tres = -0.004808  utility : 13721  mse : 4.976 not stopped\n",
    "\n",
    "lgb bce est : 2000 auc : 0.6912     utility : 17016   tres : 0.5005077   tres_utility : 17115 not stopped\n",
    "\n",
    "catboost bce num est : 2000 auc : 0.6465   utility : 13191.73  tres : 0.5029   tres_utility : 13242 not stopped\n",
    "\n",
    "catboost bce cat est : 2000 auc : 0.6814  utility : 17045  not stopped\n",
    "\n",
    "catboost bce cat est : 7500 auc : 0.7125  utility : 20257 stopped\n",
    "\n",
    "catboost mse num est : 2000 RMSE : 26.75  utility : 2653\n",
    "\n",
    "xgboost bce num est : 500  utility : 19857\n",
    "\n",
    "tabnet bce num nstep : 3, fd 256 od 128 , AUC : 0.81, utility : 26964, tres : 0.508\n",
    "\n",
    "tabnet bce num nstep : 4, fd 256 od 128 , AUC : 0.79, utility : 24369, tres :\n",
    "\n",
    "tabnet fast nstep : 2, fd 256 of 128, AUC : 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
