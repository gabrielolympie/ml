{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    'Elevation',\n",
    "    'Aspect',\n",
    "    'Slope',\n",
    "    'Horizontal_Distance_To_Hydrology',\n",
    "    'Vertical_Distance_To_Hydrology',\n",
    "    'Horizontal_Distance_To_Roadways',\n",
    "    'Hillshade_9am',\n",
    "    'Hillshade_Noon',\n",
    "    'Hillshade_3pm',\n",
    "    'Horizontal_Distance_To_Fire_Points',\n",
    "    'Wilderness_Area1',\n",
    "    'Wilderness_Area2',\n",
    "    'Wilderness_Area3',\n",
    "    'Wilderness_Area4',\n",
    "    'Soil_Type1',\n",
    "    'Soil_Type2',\n",
    "    'Soil_Type3',\n",
    "    'Soil_Type4',\n",
    "    'Soil_Type5',\n",
    "    'Soil_Type6',\n",
    "    'Soil_Type7',\n",
    "    'Soil_Type8',\n",
    "    'Soil_Type9',\n",
    "    'Soil_Type10',\n",
    "    'Soil_Type11',\n",
    "    'Soil_Type12',\n",
    "    'Soil_Type13',\n",
    "    'Soil_Type14',\n",
    "    'Soil_Type15',\n",
    "    'Soil_Type16',\n",
    "    'Soil_Type17',\n",
    "    'Soil_Type18',\n",
    "    'Soil_Type19',\n",
    "    'Soil_Type20',\n",
    "    'Soil_Type21',\n",
    "    'Soil_Type22',\n",
    "    'Soil_Type23',\n",
    "    'Soil_Type24',\n",
    "    'Soil_Type25',\n",
    "    'Soil_Type26',\n",
    "    'Soil_Type27',\n",
    "    'Soil_Type28',\n",
    "    'Soil_Type29',\n",
    "    'Soil_Type30',\n",
    "    'Soil_Type31',\n",
    "    'Soil_Type32',\n",
    "    'Soil_Type33',\n",
    "    'Soil_Type34',\n",
    "    'Soil_Type35',\n",
    "    'Soil_Type36',\n",
    "    'Soil_Type37',\n",
    "    'Soil_Type38',\n",
    "    'Soil_Type39',\n",
    "    'Soil_Type40',\n",
    "    'Cover_Type',\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('covtype.csv', names  = cols)\n",
    "soils = ['Soil_Type'+str(i) for i in range(1,41)]\n",
    "df['Soil_Type'] = np.argmax(df[soils].values, axis = 1)\n",
    "df = df.drop(columns = soils)\n",
    "y = df['Cover_Type'].values\n",
    "df = df.drop(columns = 'Cover_Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgba\n",
    "clf = lgb.LGBMClassifier(max_depth = -1, n_estimators = 1000, n_jobs = 12, silent = False, \n",
    "                         )\n",
    "\n",
    "clf.fit(X_train, y_train, eval_set =(X_test, y_test), categorical_feature = ['Soil_Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "clf = CatBoostClassifier(learning_rate=0.1, n_estimators=5000, early_stopping_rounds = 15, task_type=\"GPU\")\n",
    "clf.fit(X_train, y_train, eval_set =(X_test, y_test), cat_features = ['Soil_Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = tf.keras.utils.to_categorical(y_train)\n",
    "Y_test = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tabular(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_dimension, output_dimension, batch_momentum, rate = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = tf.keras.layers.Dense(hidden_dimension, activation = 'relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(hidden_dimension, activation = 'relu')\n",
    "        self.fc3 = tf.keras.layers.Dense(output_dimension, activation = 'relu')\n",
    "        \n",
    "        self.bn1 = tf.keras.layers.BatchNormalization(momentum=batch_momentum)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization(momentum=batch_momentum)\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization(momentum=batch_momentum)\n",
    "        \n",
    "        self.drop = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, features, training = True):\n",
    "        x = self.fc1(features)\n",
    "        x = self.bn1(x)\n",
    "        if training:\n",
    "            x = self.drop(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        if training:\n",
    "            x = self.drop(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        if training:\n",
    "            x = self.drop(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "def find_tensor_mask(tensor, n_keep_features, temperature = 100):\n",
    "    k_best, _ = tf.math.top_k(tensor, k=n_keep_features+1, sorted=False)\n",
    "    treshold =(k_best[:,-1] + k_best[:,-2]) / 2\n",
    "    reshaped_tres = tf.repeat(tf.expand_dims(treshold, axis = 1), tf.shape(tensor)[1], axis = 1)\n",
    "    tensor_tres = (tensor - reshaped_tres)\n",
    "    reduced_mask = tf.reduce_mean(tensor_tres, axis = 0)\n",
    "    mask = tf.repeat(tf.expand_dims(reduced_mask, axis = 0), tf.shape(tensor)[0], axis = 0)*temperature\n",
    "    mask = tf.keras.activations.sigmoid(mask)\n",
    "    return mask\n",
    "\n",
    "class TreeTabular(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_steps = 4, hidden_dim = 128, input_dim = 64,\n",
    "                 output_dim = 64, n_feature_per_steps = 20, \n",
    "                 temperature = 100, batch_momentum = 0.7, dropout_rate = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert output_dim % num_steps == 0 , 'the output dimension must be a multiple of number of steps' \n",
    "        \n",
    "        self.solo_output_dim = int(output_dim / num_steps)\n",
    "        self.num_steps = num_steps\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_feature_per_steps = n_feature_per_steps\n",
    "        self.temperature = temperature\n",
    "        self.batch_momentum = batch_momentum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.mask_classif1 = [Tabular(hidden_dim, input_dim, batch_momentum, rate = dropout_rate) for _ in range(num_steps)]\n",
    "#         self.mask_classif2 = [Tabular(hidden_dim, input_dim, batch_momentum, rate = dropout_rate) for _ in range(num_steps)]\n",
    "        \n",
    "        self.feature_builder1 = [Tabular(hidden_dim, self.solo_output_dim, batch_momentum, rate = dropout_rate) for _ in range(num_steps)]\n",
    "        self.feature_builder2 = [Tabular(hidden_dim, self.solo_output_dim, batch_momentum, rate = dropout_rate) for _ in range(num_steps)]\n",
    "        \n",
    "        self.smooth_layer = Tabular(hidden_dim, output_dim, batch_momentum, rate = dropout_rate)\n",
    "        \n",
    "    def call(self, features, training = True):\n",
    "        \n",
    "        output = None\n",
    "        \n",
    "        for step in range(self.num_steps):\n",
    "            \n",
    "            mask = self.mask_classif1[step](features, training = training)\n",
    "#             mask = self.mask_classif2[step](mask, training = training)\n",
    "            mask = find_tensor_mask(mask, self.n_feature_per_steps, temperature = self.temperature)\n",
    "            \n",
    "            masked_features = features * mask\n",
    "            \n",
    "            out = self.feature_builder1[step](masked_features, training = training)\n",
    "            out = self.feature_builder2[step](out, training = training)\n",
    "            \n",
    "            if output is None:\n",
    "                output = out\n",
    "            else:\n",
    "                output = tf.keras.layers.Concatenate(axis = -1)([output, out])\n",
    "                \n",
    "        output = self.smooth_layer(output)    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_r = [X_train.values[:,:-1], X_train.values[:,-1].reshape(-1,1)]\n",
    "X_test_r = [X_test.values[:,:-1], X_test.values[:,-1].reshape(-1,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabularnn import *\n",
    "inputs_num = tf.keras.Input(shape = (14,))\n",
    "inputs_cat = tf.keras.Input(shape = (1,))\n",
    "\n",
    "inputs = [inputs_num,inputs_cat]\n",
    "\n",
    "cat_emb = tf.keras.layers.Embedding(40, 10)(inputs_cat)[:,0,:]\n",
    "\n",
    "agg = tf.keras.layers.Concatenate(axis = -1)([cat_emb, inputs_num])\n",
    "\n",
    "# encoder = TreeTabular(num_steps = 4, hidden_dim = 128, input_dim = 24,\n",
    "#                  output_dim = 64, n_feature_per_steps = 10, \n",
    "#                  temperature = 100, batch_momentum = 0.7, dropout_rate = 0.1)\n",
    "\n",
    "\n",
    "node = NODE(n_layers=1, units=7, depth=6, n_trees=512, link=tf.keras.activations.softmax)\n",
    "encoded = node(agg)\n",
    "# encoder = Tabular(hidden_dimension = 128, output_dimension = 64, batch_momentum = 0.7, rate = 0.1)\n",
    "\n",
    "# encoded = encoder(agg, training = True)\n",
    "\n",
    "pred = tf.keras.layers.Dense(8, activation = 'softmax')(encoded)\n",
    "model = tf.keras.Model(inputs, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabnet2 import *\n",
    "import tensorflow as tf\n",
    "tabnet = TabNet(\n",
    "        num_features = 24,\n",
    "        feature_dim = 64,\n",
    "        output_dim = 64,\n",
    "        feature_columns = None,\n",
    "        n_step = 5,\n",
    "        n_total = 5,\n",
    "        n_shared = 2,\n",
    "        relaxation_factor = 1.5,\n",
    "        bn_epsilon = 1e-5,\n",
    "        bn_momentum = 0.7,\n",
    "        bn_virtual_divider = 10,\n",
    "    )\n",
    "# tabnet.build((None, 24))\n",
    "inputs_num = tf.keras.Input(shape = (14,))\n",
    "inputs_cat = tf.keras.Input(shape = (1,))\n",
    "inputs = [inputs_num,inputs_cat]\n",
    "cat_emb = tf.keras.layers.Embedding(40, 10)(inputs_cat, training = True)[:,0,:]\n",
    "agg = tf.keras.layers.Concatenate(axis = -1)([cat_emb, inputs_num])\n",
    "encoded, masks = tabnet(agg)\n",
    "pred = tf.keras.layers.Dense(8, activation = 'softmax')(encoded)\n",
    "model = tf.keras.Model(inputs, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "25000/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "model.compile(\n",
    "        loss = 'categorical_crossentropy',\n",
    "        optimizer = Adam(0.01),\n",
    "        metrics = ['accuracy'])\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "early = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=9, verbose=1, \n",
    "                                                mode='auto', restore_best_weights=True)\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, verbose=1, \n",
    "                           mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "callbacks =[early, reduce]\n",
    "batch_size = 25000\n",
    "epochs = 100\n",
    "\n",
    "ls = X_train_r[0].shape[0] // batch_size * batch_size\n",
    "lt = X_test_r[0].shape[0] // batch_size * batch_size\n",
    "\n",
    "model.fit([elt[:ls] for elt in X_train_r], Y_train[:ls], validation_data = ([elt[:lt] for elt in X_test_r], Y_test[:lt]), batch_size=batch_size, epochs=epochs, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pred = model.predict(X_test_r, verbose = 1, batch_size = 6400)\n",
    "\n",
    "\n",
    "\n",
    "pred = np.argmax(pred, axis = -1)\n",
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Baseline LGB 0.8628\n",
    "## Baseline CatBoost 0.9424\n",
    "\n",
    "## Baseline NN sans embeddings  0.818\n",
    "## Baseline NN avec embeddings  0.8569\n",
    "## Baseline TreeTab avec embeddings 0.8784\n",
    "## Baseline Tabnet avec embeddings 0.9575\n",
    "## Baseline Node avec embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "\n",
    "clf = TabNetClassifier(cat_idxs = [14])  #TabNetRegressor()\n",
    "clf.fit(\n",
    "  X_train.values, y_train,\n",
    "  eval_set=[(X_test.values, y_test)]\n",
    ")\n",
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
