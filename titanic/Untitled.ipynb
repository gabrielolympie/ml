{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y = df['Survived'].values\n",
    "\n",
    "# train = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']].copy().fillna(0)\n",
    "# sdico = {'male' : 0, 'female': 1}\n",
    "\n",
    "# train['Sex'] = train['Sex'].replace(sdico).copy()\n",
    "\n",
    "# edico = {'S':0, 'C':1, 'Q':2}\n",
    "# def app(x):\n",
    "#     try:\n",
    "#         return edico[x]\n",
    "#     except:\n",
    "#         return 3\n",
    "# train['Embarked'] = train['Embarked'].apply(app).copy()\n",
    "\n",
    "# X = train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['target'].values\n",
    "X = df[['var_'+str(i) for i in range(200)]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = tf.keras.utils.to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabnet_layer import *\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabnet = TabNet(\n",
    "#            columns = None,\n",
    "           num_features = 200,\n",
    "           feature_dim = 512,\n",
    "           output_dim = 256,\n",
    "           num_decision_steps = 3,\n",
    "           relaxation_factor = 1.5,\n",
    "           batch_momentum = 0.7,\n",
    "           virtual_batch_size = None,\n",
    "#            num_classes = 0,\n",
    "           epsilon=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape = (200,))\n",
    "\n",
    "encoded, entropy = tabnet(inputs)\n",
    "# encoded = tf.keras.layers.Dense(128,activation = 'relu')(inputs)\n",
    "# encoded = tf.keras.layers.Dense(128,activation = 'relu')(encoded)\n",
    "# encoded = tf.keras.layers.Dense(128,activation = 'relu')(encoded)\n",
    "pred = tf.keras.layers.Dense(1, activation = 'sigmoid')(encoded)\n",
    "model = tf.keras.Model(inputs, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.1\n",
    "def loss_auc(true, pred, batch_size = batch_size):\n",
    "        \n",
    "    pred = tf.repeat(pred, batch_size, axis = -1)\n",
    "    \n",
    "    diff1 = tf.math.exp(- temperature * (pred - tf.transpose(pred)))\n",
    "    true = tf.repeat(true, batch_size, axis = -1)\n",
    "    \n",
    "    zero_un_comp = tf.cast(tf.math.maximum(true - tf.transpose(true), 0), dtype = diff1.dtype)\n",
    "    diff1 *= zero_un_comp\n",
    "    return tf.math.reduce_sum(diff1)/tf.math.reduce_sum(zero_un_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                    from_logits=True, reduction='none')\n",
    "\n",
    "acc_object = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "def acc(true, pred):\n",
    "    return tf.reduce_mean(acc_object(true, pred))\n",
    "\n",
    "def loss(true, pred):\n",
    "    return tf.reduce_mean(loss_object(true, pred))\n",
    "\n",
    "model.compile(\n",
    "        loss = 'categorical_crossentropy',\n",
    "        optimizer = Adam(0.02),\n",
    "        metrics = ['accuracy', 'AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "\n",
    "class AUCCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, validation=None, logs = {}, dico_params = {}, from_path = None):\n",
    "        super(AUCCallback, self).__init__()\n",
    "        self.validation = validation\n",
    "        self.epoch = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        ## Roc auc calculation on test set\n",
    "        x_val, y_val = self.validation[0], self.validation[1]\n",
    "        pred = model.predict(x_val, verbose = 0)\n",
    "        \n",
    "        pred = pred[:,1]\n",
    "        true = y_val[:,1]\n",
    "        \n",
    "        roc_auc = roc_auc_score(y_val, pred)\n",
    "        \n",
    "        logs['roc_auc'] = roc_auc\n",
    "        print(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 6400\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "early = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=9, verbose=1, \n",
    "                                                mode='auto', restore_best_weights=True)\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, verbose=1, \n",
    "                           mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "\n",
    "\n",
    "st = X_train.shape[0] // batch_size * batch_size\n",
    "sv = X_test.shape[0] // batch_size * batch_size\n",
    "\n",
    "callbacks =[early, reduce, AUCCallback(validation = (X_test[:sv], y_test[:sv]))]\n",
    "\n",
    "model.fit(X_train[:st], y_train[:st],  validation_data = (X_test[:sv], y_test[:sv]), batch_size = batch_size, epochs = epochs, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "print(accuracy_score(np.argmax(y_test, axis = -1), np.argmax(pred, axis = -1)))\n",
    "print(roc_auc_score(np.argmax(y_test, axis = -1), pred[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "clf = lgb.LGBMClassifier(max_depth = -1, n_estimators = 500, n_jobs = 12, silent = False)\n",
    "clf.fit(X_train, y_train[:,1], eval_set =(X_test, y_test[:,1]), eval_metric = 'auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adult Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('adult.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize(df, field):\n",
    "    x = df[field].unique()\n",
    "    dico = {}\n",
    "    for i, elt in enumerate(x):\n",
    "        dico[elt] = i\n",
    "    \n",
    "    def apply_dico(a):\n",
    "        try:\n",
    "            return dico[a]\n",
    "        except:\n",
    "            return len(x) + 1\n",
    "        \n",
    "    new_cols = df[field].apply(apply_dico)\n",
    "    \n",
    "#     if field !=  '<=50K':\n",
    "#         new_cols = tf.keras.utils.to_categorical(new_cols)\n",
    "\n",
    "#         df = df.drop(columns = field)\n",
    "\n",
    "#         for i in range(new_cols.shape[1]):\n",
    "#             df[field+'_'+str(i)] = new_cols[:,i]\n",
    "#     else:\n",
    "    df[field] = new_cols\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "cat_cols = ['workclass', 'education', 'education.num', 'marital.status',\n",
    "       'occupation', 'relationship', 'race', 'sex', 'native.country', '<=50K']\n",
    "\n",
    "num_cols = [elt for elt in df.columns if not(elt in cat_cols)]\n",
    "for field in cat_cols:\n",
    "    df = categorize(df, field)\n",
    "    \n",
    "cats_ids = [i for i in range(len(df.columns[:-1])) if df.columns[i] in cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['<=50K']\n",
    "df= df.drop(columns = ['<=50K'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = tf.keras.utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [X_train[elt].values.reshape(-1,1) for elt in cat_cols[:-1]] + [X_train[num_cols].values]\n",
    "X_test = [X_test[elt].values.reshape(-1,1) for elt in cat_cols[:-1]] + [X_test[num_cols].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_size = [9,10,10,6,12,6,5,3,25]\n",
    "np.sum(out_size) + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabnet2 import *\n",
    "import tensorflow as tf\n",
    "tabnet = TabNet(\n",
    "        num_features = 90,\n",
    "        feature_dim = 64,\n",
    "        output_dim = 64,\n",
    "        feature_columns = None,\n",
    "        n_step = 5,\n",
    "        n_total = 5,\n",
    "        n_shared = 2,\n",
    "        relaxation_factor = 1.5,\n",
    "        bn_epsilon = 1e-5,\n",
    "        bn_momentum = 0.7,\n",
    "        bn_virtual_divider = 10,\n",
    "    )\n",
    "\n",
    "inputs = [tf.keras.Input((1,)) for i in range(9)] + [tf.keras.Input((4,))]\n",
    "\n",
    "in_size = [9, 16, 16,7,15,6,5,3,42]\n",
    "out_size = [9,10,10,6,12,6,5,3,25]\n",
    "\n",
    "embed = [tf.squeeze(tf.keras.layers.Embedding(in_size[i], out_size[i])(inputs[i]), axis = 1) for i in range(len(in_size))] + [inputs[-1]]\n",
    "\n",
    "embed = tf.keras.layers.Concatenate(axis = -1)(embed)\n",
    "\n",
    "encoded, masks = tabnet(embed, training = True)\n",
    "\n",
    "pred = tf.keras.layers.Dense(1, activation = 'sigmoid')(encoded)\n",
    "model = tf.keras.Model(inputs, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "model.compile(\n",
    "        loss = 'binary_crossentropy',\n",
    "        optimizer = Adam(0.1),\n",
    "        metrics = ['accuracy', 'AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "early = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=9, verbose=1, \n",
    "                                                mode='auto', restore_best_weights=True)\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, verbose=1, \n",
    "                           mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "batch_size = 2600\n",
    "epochs = 100\n",
    "\n",
    "st = X_train[0].shape[0] // batch_size * batch_size\n",
    "sv = X_test[0].shape[0] // batch_size * batch_size\n",
    "\n",
    "callbacks =[early, reduce]\n",
    "\n",
    "model.fit([elt[:st] for elt in X_train], y_train[:st],  validation_data = ([elt[:sv] for elt in X_test[:sv]], y_test[:sv]), batch_size = batch_size, epochs = epochs, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.predict(X_test, verbose = 1, batch_size = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test[:,1], np.argmax(a, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test[:,1], a[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "clf = lgb.LGBMClassifier(max_depth = -1, n_estimators = 10, n_jobs = 12, silent = False)\n",
    "clf.fit(X_train, y_train, eval_set =(X_test, y_test), eval_metric = 'auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(X_train)\n",
    "accuracy_score(y_train, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
