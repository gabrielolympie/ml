{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "import _pickle as pickle\n",
    "def save(file,name, folder = \"\"):\n",
    "    if folder != \"\":\n",
    "        outfile = open('./'+folder+'/'+name+'.pickle', 'wb')\n",
    "    else:\n",
    "        outfile = open(name+'.pickle', 'wb')\n",
    "    pickle.dump(file, outfile)\n",
    "    outfile.close\n",
    "    \n",
    "def load(name, folder = \"\"):\n",
    "    if folder != \"\":\n",
    "        outfile = open('./'+folder+'/'+name+'.pickle', 'rb')\n",
    "    else:\n",
    "        outfile = open(name+'.pickle', 'rb')\n",
    "    file = pickle.load(outfile)\n",
    "    outfile.close\n",
    "    return file\n",
    "\n",
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata():\n",
    "    with open('arxiv-metadata-oai-snapshot.json', 'r') as f:\n",
    "        for line in f:\n",
    "            yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = get_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license', 'abstract', 'versions', 'update_date', 'authors_parsed']\n",
    "\n",
    "titles_tags_dict = {k:[] for k in keys}\n",
    "count = 0\n",
    "for paper in tqdm(metadata, total = 1700000):\n",
    "    parsed = json.loads(paper)\n",
    "    for k in keys:\n",
    "        titles_tags_dict[k].append(parsed[k])\n",
    "#     titles_tags_dict[\"title\"].append(parsed['title'])\n",
    "#     titles_tags_dict[\"tags\"].append(parsed['categories'])\n",
    "#     count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(titles_tags_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(df, 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load('data')\n",
    "df = df.sample(n = df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ = pd.read_csv('categ.csv', sep = ';', encoding = 'latin-1')\n",
    "\n",
    "dcat = {}\n",
    "for i, line in categ.iterrows():\n",
    "    dcat[line['category']] = line['description']\n",
    "    \n",
    "def apply_categ(x):\n",
    "    x = x.split(' ')\n",
    "    x1 = []\n",
    "    for elt in x:\n",
    "        try:\n",
    "            x1.append(dcat[elt])\n",
    "        except:\n",
    "            1\n",
    "#             x1.append('')\n",
    "    return \"; \".join(x1)\n",
    "df['category'] = df['categories'].apply(apply_categ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins = list(map(lambda x : \" | \".join(x), list(zip(df['category'], df['title']))))\n",
    "ous = df['abstract'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_tf_records(inputs, outputs, save_path):\n",
    "#     input_ids = inputs['input_ids']   \n",
    "#     token_type_ids = inputs['token_type_ids']   \n",
    "#     attention_mask = inputs['attention_mask']  \n",
    "#     outputs = outputs\n",
    "\n",
    "    # write to tfrecord\n",
    "    with tf.io.TFRecordWriter(save_path) as writer:\n",
    "        def create_float_feature(values):\n",
    "            return tf.train.Feature(float_list=tf.train.FloatList(value=values))\n",
    "\n",
    "        def create_int_feature(values):\n",
    "            return tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "\n",
    "        for (ids, ty, ma, out) in zip(inputs['input_ids'], inputs['token_type_ids'], inputs['attention_mask'], outputs):\n",
    "            features = {'input_ids': create_int_feature(ids), \n",
    "                        'token_type_ids': create_int_feature(ty),\n",
    "                        'attention_mask': create_int_feature(ma),\n",
    "                        'outputs': create_int_feature(out), \n",
    "            }\n",
    "            tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "            writer.write(tf_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 512\n",
    "bs = 10000\n",
    "nb = df.shape[0] // bs + 1\n",
    "for i in tqdm(range(23,nb)):\n",
    "    a = ins[bs*i:bs*(i+1)]\n",
    "    b = ous[bs*i:bs*(i+1)]\n",
    "    toks = tokenizer.batch_encode_plus(list(zip(a, b)),\n",
    "                                  add_special_tokens  = True, truncation = 'only_second',padding = 'max_length',\n",
    "                                  max_length = max_len + 1, return_token_type_ids = True, verbose = True)\n",
    "    \n",
    "#     print(toks['input_ids'][:50])\n",
    "    inputs = {\n",
    "        'input_ids' : np.array(toks['input_ids'])[:, :max_len].astype('int32'),\n",
    "        'token_type_ids' : np.array(toks['token_type_ids'])[:, :max_len].astype('int32'),\n",
    "        'attention_mask' : np.array(toks['attention_mask'])[:, :max_len].astype('int32')\n",
    "    }\n",
    "\n",
    "    outputs = np.array(toks['input_ids'])[:,1:].astype('int32')\n",
    "#     save((inputs, outputs), 'batch_'+str(i), 'tokenized')\n",
    "    \n",
    "    print(inputs['input_ids'].shape)\n",
    "    \n",
    "    write_to_tf_records(inputs, outputs, './tf_records/batch_'+str(i)+'.tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_function(record):\n",
    "    max_len = 512\n",
    "    feature_description = {'input_ids': tf.io.FixedLenFeature(shape = (max_len),dtype = tf.int64), \n",
    "            'token_type_ids': tf.io.FixedLenFeature(shape = (max_len),dtype = tf.int64), \n",
    "            'attention_mask': tf.io.FixedLenFeature(shape = (max_len),dtype = tf.int64), \n",
    "            'outputs': tf.io.FixedLenFeature(shape = (max_len),dtype = tf.int64), \n",
    "        }\n",
    "\n",
    "    new_sample = tf.io.parse_single_example(record, feature_description)\n",
    "\n",
    "    input_ids = new_sample['input_ids']\n",
    "    token_type_ids = new_sample['token_type_ids']\n",
    "    attention_mask = new_sample['attention_mask']\n",
    "\n",
    "    y = new_sample['outputs']\n",
    "\n",
    "    X = {\n",
    "        'input_ids' : input_ids,\n",
    "        'attention_mask' : attention_mask,\n",
    "        'token_type_ids' : token_type_ids\n",
    "    }\n",
    "\n",
    "    return X,y\n",
    "\n",
    "def get_dataset( gcs_pattern, batch_size = 256):\n",
    "    list_data = tf.io.gfile.glob(gcs_pattern)\n",
    "    dataset = tf.data.TFRecordDataset(list_data)\n",
    "    dataset = dataset.map(_parse_function, num_parallel_calls=8)\n",
    "    dataset = dataset.repeat().batch(batch_size)\n",
    "    return dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x0000020556109CF8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x0000020556109CF8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "# batch_size = 4\n",
    "max_len = 128\n",
    "lr = 0.000001\n",
    "\n",
    "inputs = {\n",
    "        'input_ids' : tf.keras.Input(shape = (max_len,), dtype = tf.int32),\n",
    "        'attention_mask' : tf.keras.Input(shape = (max_len,), dtype = tf.int32),\n",
    "        'token_type_ids' : tf.keras.Input(shape = (max_len,), dtype = tf.int32)\n",
    "    }\n",
    "decoder = TFGPT2LMHeadModel.from_pretrained('gpt2')\n",
    "x = decoder(input_ids = inputs['input_ids'], attention_mask = inputs['attention_mask'], token_type_ids = inputs['token_type_ids'])\n",
    "x = x[0]\n",
    "model = tf.keras.Model(inputs, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tfgp_t2lm_head_model (TFGPT2LMH TFCausalLMOutputWith 124439808   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 124,439,808\n",
      "Trainable params: 124,439,808\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x20551ce99c8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('./checkpoint/gpt_1947_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atomic Physics | We introduce a novel way to dissociate carbon dioxyde into dioxygen and carbon monnoxyde by stimulating the radial excitation of molecule.\n",
      "  This approach is able to produce high-temperatures, which can be used in\n",
      "quantum computing applications such as photovoltaics and nanoscale sensing.\n",
      "  In this paper we propose a new method to disentangle molecules from their chemical counterparts\n",
      "and show that it has significant advantages over conventional methods.\n",
      "  The main advantage of this technique is its ability to generate low temperatures at room temperature.\n",
      "We also demonstrate that it does not require an external magnetic field for\n",
      "dissipations to occur. Our results are consistent with previous work showing the\n",
      "effectiveness when applied to quantum computer systems.\n",
      "  It should also be pointed out that these findings do not necessarily mean that there\n",
      "is no physical connection between atoms or nucleotides; rather, they suggest\n",
      "that one may have different reactions depending only on how much time passes through them.\n",
      "  These observations could provide important insights into future research towards\n",
      "thermodynamical control of molecular dynamics.\n",
      "  For more information about our study please see: http://bit.ly/1eX3qYc\n",
      "http:\\www\\mathbb{Z}(Phys. Rev. Lett. 86 (2015) 102301 -102403\n",
      "https \\emph^{\\epsilon}\\simequo_2(\\textit p_{0},p_{1})^{+4}}$.\n",
      "This article was originally published online May 22, 2015.\n",
      "For further details visit:\n",
      "http\\\" www.sciencedirectory.com/article/pii/S0017086111214078<|endoftext|>\n",
      "\n",
      "\n",
      "Atomic Physics | We introduce a novel way to dissociate carbon dioxyde into dioxygen and carbon monnoxyde by stimulating the radial excitation of molecule.\n",
      "  The method is based on two-photon emission spectroscopy, which can be used\n",
      "in conjunction with electron microscopics (EMs). In this paper we show that in the presence or\n",
      "exhaustion of oxygen at room temperature, DIOXYDE undergoes an irreversible phase transition from\n",
      "diphenhydrodynamically active form to deuterated hydrophobic state where it becomes unstable\n",
      "at low temperatures. It has recently been demonstrated that DIOYD does not decay as quickly\n",
      "as previously thought due mainly to its non-trivial nature. Here, we demonstrate\n",
      "that DIOXYDE exhibits no significant change in chemical structure during\n",
      "reconstruction when exposed to high ambient CO2 concentrations. Our results are also\n",
      "consistently consistent with those reported earlier [1].\n",
      "  These findings provide new insights into the molecular dynamics of DIOXYDE.\n",
      "  This work may pave the way for further studies of these molecules.\n",
      "  Future experiments will allow us better understanding of their potential applications.\n",
      "  For more information, please see: http://www3.arXiv:1408.067\n",
      "http:\\/dx;9\\textbf{S010312}.\n",
      "This article was originally published online May 24, 2015.\n",
      "For any questions regarding the content of this publication, e-mail arxiv.org/abs/1310.076\n",
      "or follow arxiv.org on Twitter: https\"https\\\"@Arxiv.net\".<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = [\"Computation and Language (Computational Linguistics and Natural Language) | Topic Recognition and Understanding for zero Shot Transformers, a sentence pair classification model based on transformers language models to perform aspect based sentiment analysis.\"]\n",
    "# prompt = [\"Computation and Language (Computational Linguistics and Natural Language) | Transformers are a new technique in NLP. At Ekimetrics we use Transformers extensively and have pushed the boudaries of the state of the art NLP for business. We propose a new technoque called TRUST (Topic Recognition and Understanding with zero Shot Transformers) to recognize topics in social media texts using Zero Shot Learning and masked self supervised learning (BERT).\"]\n",
    "# prompt = [\"Computation and Language (Computational Linguistics and Natural Language) | We propose a new state of the art framework to perform topic based sentiment analysis tasks, based on a sentence pair classification Transformers architecture that we named TRUST (Topic Recognition and Understanding for zero Shot Transformers).\"]\n",
    "\n",
    "prompt = [\"Atomic Physics | We introduce a novel way to dissociate carbon dioxyde into dioxygen and carbon monnoxyde by stimulating the radial excitation of molecule.\"]\n",
    "\n",
    "prompt = tokenizer.batch_encode_plus(prompt, add_special_tokens = True, return_tensors=\"tf\")['input_ids']\n",
    "\n",
    "generated = decoder.generate(input_ids=prompt, max_length=400, min_length=12, \n",
    "                     do_sample=True, early_stopping=None, num_beams=5, \n",
    "                     temperature=3, top_k=3, top_p=None, repetition_penalty=1.1, \n",
    "                     bad_words_ids=None, bos_token_id=None, pad_token_id=None, \n",
    "                     eos_token_id=None, length_penalty=None, no_repeat_ngram_size=None, \n",
    "                     num_return_sequences=2, attention_mask=None, \n",
    "                     decoder_start_token_id=None, use_cache=None)\n",
    "\n",
    "decoded = tokenizer.batch_decode(generated)\n",
    "                             \n",
    "for elt in decoded:\n",
    "    print(elt)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
