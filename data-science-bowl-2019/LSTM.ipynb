{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## General librairies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import os\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import dateutil.parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs = 'specs.csv'\n",
    "test = 'test.csv'\n",
    "train = 'train.csv'\n",
    "train_labels = 'train_labels.csv'\n",
    "subs = 'sample_submission_exemple.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(file,name, folder = \"\"):\n",
    "    if folder != \"\":\n",
    "        outfile = open('./'+folder+'/'+name+'.pickle', 'wb')\n",
    "    else:\n",
    "        outfile = open(name+'.pickle', 'wb')\n",
    "    pickle.dump(file, outfile)\n",
    "    outfile.close\n",
    "    \n",
    "def load(name, folder = \"\"):\n",
    "    if folder != \"\":\n",
    "        outfile = open('./'+folder+'/'+name+'.pickle', 'rb')\n",
    "    else:\n",
    "        outfile = open(name+'.pickle', 'rb')\n",
    "    file = pickle.load(outfile)\n",
    "    outfile.close\n",
    "    return file\n",
    "\n",
    "def relative_time(x):\n",
    "    x1 = []\n",
    "    for elt in x:\n",
    "        x1.append((elt-x[0]).item()/1000000000)\n",
    "    return x1\n",
    "\n",
    "def categorise(x):\n",
    "    dico = {}\n",
    "    count = 0\n",
    "    for elt in x:\n",
    "        if not(elt in dico):\n",
    "            dico[elt] = count\n",
    "            count += 1\n",
    "    return dico\n",
    "\n",
    "def padding( dataset, n):\n",
    "    d = list(np.zeros(len(dataset)))\n",
    "    c = 0\n",
    "    count = 0\n",
    "    for elt in dataset:\n",
    "        \n",
    "        if count % 100 == 0:\n",
    "            print(count)\n",
    "        u = elt.shape[0]\n",
    "        \n",
    "        if u<200:\n",
    "            c += 1\n",
    "        \n",
    "        if u > n:\n",
    "            d[count] = elt[-n:]\n",
    "        else:\n",
    "            a = np.zeros(((n-u), elt.shape[1]))-1\n",
    "            elt = np.concatenate([a, elt], axis = 0)\n",
    "            d[count] = elt\n",
    "        count += 1\n",
    "    return d\n",
    "\n",
    "def build_line(df, installation_id, game_session):\n",
    "    \n",
    "    df1 = df[(df['installation_id']==installation_id)&(df['date']<=df[(df['installation_id']==installation_id)&(df['game_session']==game_session)]['date'].iloc[0])]\n",
    "    dico = categorise(df1['game_session'].unique())\n",
    "    df1 = df1.replace({'game_session' : dico})\n",
    "    x = relative_time(df1['date'].values)\n",
    "    \n",
    "    df1['time_delta'] = x\n",
    "    \n",
    "#     df1 = df1.drop_duplicates(subset = ['event_id'])\n",
    "    \n",
    "    cats_to_keep = ['game_session','time_delta','title','type', 'world',  'event_count', 'game_time', 'event_code']\n",
    "    df1 = df1[cats_to_keep]\n",
    "    return df1\n",
    "\n",
    "def format_input(X):\n",
    "    return [X[:,:,[0,1,5,6]], X[:,:,2].reshape((X.shape[0], X.shape[1],1)), X[:,:,3].reshape((X.shape[0], X.shape[1],1)), X[:,:,4].reshape((X.shape[0], X.shape[1],1)),X[:,:,7].reshape((X.shape[0], X.shape[1],1))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(train)\n",
    "labels = pd.read_csv(train_labels)\n",
    "dtitle, dtype, dworld, devent = load('dicos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding some formating to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['event_code']<4020)|(df['event_code']>4080)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = df[df['type']=='Assessment']['installation_id'].unique()\n",
    "df = df[df.installation_id.isin(ids)]\n",
    "df = df[df.installation_id.isin(labels.installation_id.unique())]\n",
    "\n",
    "dtitle, dtype, dworld, devent = load('dicos')\n",
    "\n",
    "dtitle = categorise(df['title'])\n",
    "dtype = categorise(df['type'])\n",
    "dworld = categorise(df['world'])\n",
    "devent = categorise(df['event_code'])\n",
    "\n",
    "save((dtitle, dtype, dworld, devent), 'dico')\n",
    "\n",
    "df = df.replace({'title' : dtitle})\n",
    "df = df.replace({'type' : dtype})\n",
    "df = df.replace({'world' : dworld})\n",
    "df = df.replace({'event_code' : devent})\n",
    "\n",
    "# df['date'] = df['timestamp'].apply(dateutil.parser.parse)\n",
    "# df = df.sort_values(by = ['date'], ascending =True)\n",
    "# df = df.reset_index()\n",
    "\n",
    "# df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocess import Pool\n",
    "def f(x):\n",
    "    import dateutil.parser\n",
    "    return dateutil.parser.parse(x)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    p = Pool(8)\n",
    "    vect = p.map(f, df['timestamp'].values)\n",
    "\n",
    "df['date'] = vect\n",
    "df = df.sort_values(by = ['date'], ascending =True)\n",
    "df = df.reset_index()\n",
    "\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(df, 'time train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load('time train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by = ['date'], ascending =True)\n",
    "df = df.sort_values(by = ['installation_id'], ascending =True)\n",
    "\n",
    "dico_id = {}\n",
    "\n",
    "vect = df['installation_id'].values\n",
    "count = 0\n",
    "for elt in vect:    \n",
    "    if count == 0:\n",
    "        val1 = 0\n",
    "        mem = elt\n",
    "    \n",
    "    if elt != mem:\n",
    "        val2 = count\n",
    "        dico_id[mem] = [val1, val2]\n",
    "        val1 = count\n",
    "    \n",
    "    if count == vect.shape[0]-1:\n",
    "        dico_id[mem] = [val1, vect.shape[0]]\n",
    "    \n",
    "    mem = elt\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "per_id = list(np.zeros(3614))\n",
    "\n",
    "count = 0\n",
    "for elt in labels['installation_id'].unique():\n",
    "    per_id[count] = labels[labels['installation_id']==elt]\n",
    "    count += 1\n",
    "\n",
    "\n",
    "\n",
    "dataset = list(np.zeros(labels['installation_id'].unique().shape[0]))\n",
    "\n",
    "count = 0\n",
    "\n",
    "for i in range(len(per_id)):\n",
    "    print(i)\n",
    "    temp = list(np.zeros(per_id[i].shape[0]))\n",
    "    rows = dico_id[per_id[i].iloc[0]['installation_id']]\n",
    "    df1 = df.iloc[rows[0]:rows[1]]\n",
    "    df1 = df1.sort_values(by = ['date'], ascending =True)\n",
    "#     df1 = df1.drop_duplicates(subset = ['event_id'])\n",
    "#     print(df1)\n",
    "#     df1 = df[df['installation_id'] == per_id[i].iloc[0]['installation_id']]\n",
    "#     print(df1.head())\n",
    "    for j in range(len(temp)):\n",
    "        installation_id = per_id[i].iloc[j]['installation_id']\n",
    "        game_session = per_id[i].iloc[j]['game_session']\n",
    "        df2 = build_line(df1, installation_id, game_session)\n",
    "\n",
    "#         temp[j] = df2.values\n",
    "        dataset[i] = df2.values\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(dataset, 'non_padded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 0\n",
    "for i in dataset:\n",
    "#     if i.shape[0] > M:\n",
    "        M += i.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M/17000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load('non padded')\n",
    "\n",
    "# pad = 2000\n",
    "\n",
    "# dataset = padding(dataset,pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = load('train_labels')\n",
    "X = np.array(dataset)\n",
    "Y = labels['accuracy_group'].values\n",
    "y = np_utils.to_categorical(Y)\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(Y),\n",
    "                                               Y)\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=43)\n",
    "\n",
    "#15102\n",
    "# X_train = X[:15102]\n",
    "# X_test = X[15102:]\n",
    "# y_train = y[:15102]\n",
    "# y_test = y[15102:]\n",
    "\n",
    "\n",
    "\n",
    "X_train = format_input(X_train)\n",
    "X_test = format_input(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dropout, Dense,CuDNNLSTM, Flatten, Embedding, TimeDistributed, Concatenate, LSTM, BatchNormalization, Lambda, Reshape\n",
    "from keras.regularizers import l2\n",
    "\n",
    "pad_size = pad\n",
    "\n",
    "num = Input(shape = (pad_size, 4))\n",
    "title = Input(shape = (pad_size, 1))\n",
    "type1 = Input(shape = (pad_size, 1))\n",
    "world = Input(shape = (pad_size, 1))\n",
    "event = Input(shape = (pad_size, 1))\n",
    "\n",
    "inputs = [num, title, type1, world, event]\n",
    "\n",
    "num_emb = TimeDistributed(Dense(16))(num)\n",
    "\n",
    "title_emb = TimeDistributed(Embedding( 44 ,8))(title)\n",
    "# title_emb = Lambda(lambda x: x, output_shape = lambda s:s)(title_emb)\n",
    "title_emb = Reshape((-1,8))(title_emb)\n",
    "\n",
    "type_emb = TimeDistributed(Embedding( 4 ,2))(type1)\n",
    "# type_emb = Lambda(lambda x: x, output_shape = lambda s:s)(type_emb)\n",
    "type_emb = Reshape((-1,2))(type_emb)\n",
    "\n",
    "world_emb = TimeDistributed(Embedding( 4 ,2))(world)\n",
    "# world_emb = Lambda(lambda x: x, output_shape = lambda s:s)(world_emb)\n",
    "world_emb = Reshape((-1,2))(world_emb)\n",
    "\n",
    "event_emb = TimeDistributed(Embedding( 42 ,8))(event)\n",
    "# event_emb = Lambda(lambda x: x, output_shape = lambda s:s)(event_emb)\n",
    "event_emb = Reshape((-1,8))(event_emb)\n",
    "\n",
    "embedded = [title_emb, type_emb, world_emb, event_emb]\n",
    "\n",
    "merged = Concatenate()(embedded + [num_emb])\n",
    "\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "merged = TimeDistributed(Dense(16))(num)\n",
    "\n",
    "lstm_out = CuDNNLSTM(64, return_sequences = False)(merged)\n",
    "\n",
    "# lstm_out = Flatten()(lstm_out)\n",
    "\n",
    "td = Dropout(0.2)(lstm_out)\n",
    "td = Dense(32, activation = 'relu')(td)\n",
    "td = Dropout(0.2)(td)\n",
    "\n",
    "td = Dense(4, activation = 'sigmoid')(td)\n",
    "\n",
    "model = Model(inputs , td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import keras.backend as K\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "optimizer = SGD(0.1)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 15\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, validation_data=(X_test, y_test), class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "optimizer = SGD(0.01)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, validation_data=(X_test, y_test), class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(0.001)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, validation_data=(X_test, y_test), class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohen_kappa_score(np.argmax(y_test, axis = 1),np.argmax(a, axis = 1), weights = 'quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(model, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load('test')\n",
    "ids = df['installation_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df['installation_id']=='0500e23b']\n",
    "\n",
    "df1['date'] = df1['timestamp'].apply(dateutil.parser.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.sort_values(by = ['date'], ascending =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dtitle, dtype, dworld, devent = load('dicos')\n",
    "df = df.replace({'title' : dtitle})\n",
    "df = df.replace({'type' : dtype})\n",
    "df = df.replace({'world' : dworld})\n",
    "df = df.replace({'event_code' : devent})\n",
    "# df['date'] = df['timestamp'].apply(dateutil.parser.parse)\n",
    "# df = df.sort_values(by = ['date'], ascending =True)\n",
    "# df = df.reset_index()\n",
    "# df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from multiprocess import Pool\n",
    "def f(x):\n",
    "    import dateutil.parser\n",
    "    return dateutil.parser.parse(x)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    p = Pool(8)\n",
    "    vect = p.map(f, df['timestamp'].values)\n",
    "    \n",
    "df['date'] = vect\n",
    "df = df.sort_values(by = ['date'], ascending =True)\n",
    "df = df.reset_index()\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by = ['date'], ascending =True)\n",
    "df = df.sort_values(by = ['installation_id'], ascending =True)\n",
    "\n",
    "dico_id = {}\n",
    "\n",
    "vect = df['installation_id'].values\n",
    "count = 0\n",
    "for elt in vect:    \n",
    "    if count == 0:\n",
    "        val1 = 0\n",
    "        mem = elt\n",
    "    \n",
    "    if elt != mem:\n",
    "        val2 = count\n",
    "        dico_id[mem] = [val1, val2]\n",
    "        val1 = count\n",
    "    \n",
    "    if count == vect.shape[0]-1:\n",
    "        dico_id[mem] = [val1, vect.shape[0]]\n",
    "    \n",
    "    mem = elt\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = list(np.zeros(ids.shape[0]))\n",
    "\n",
    "\n",
    "for i in range(ids.shape[0]):\n",
    "    print(i)\n",
    "    \n",
    "    installation_id = ids[i]\n",
    "    \n",
    "    rows = dico_id[installation_id]\n",
    "    df1 = df.iloc[rows[0]: rows[1]]\n",
    "    df1 = df[df['installation_id'] == installation_id]\n",
    "    df1 = df1.sort_values(by = ['date'], ascending =True)\n",
    "    \n",
    "    game_session = df1.iloc[-1]['game_session']\n",
    "#     print(df1.iloc[-1]['type'])\n",
    "    \n",
    "    df2 = build_line(df1, installation_id, game_session)\n",
    "        \n",
    "    dataset[i] = df2.values\n",
    "\n",
    "# for i in range(len(per_id)):\n",
    "#     print(i)\n",
    "#     temp = list(np.zeros(per_id[i].shape[0]))\n",
    "#     df1 = df[df['installation_id'] == per_id[i].iloc[0]['installation_id']]\n",
    "# #     print(df1.head())\n",
    "#     for j in range(len(temp)):\n",
    "#         installation_id = per_id[i].iloc[j]['installation_id']\n",
    "#         game_session = per_id[i].iloc[j]['game_session']\n",
    "#         df2 = build_line(df1, installation_id, game_session)\n",
    "\n",
    "#         temp[j] = df2.values\n",
    "#     dataset.extend(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "save(dataset, 'non_padded_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load('non_padded_test')\n",
    "dataset = padding(dataset,pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(dataset)\n",
    "X = format_input(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load('model')\n",
    "pred = model.predict(X)\n",
    "\n",
    "\n",
    "pred = np.argmax(pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = ids.reshape((ids.shape[0], 1))\n",
    "pred = pred.reshape((pred.shape[0], 1))\n",
    "\n",
    "df1 = pd.DataFrame(np.concatenate([ids, pred], axis = 1), index = ids.reshape(ids.shape[0]), columns = ['installation_id','accuracy_group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_csv(subs)\n",
    "sample_sub['accuracy_group'] = df1.loc[sample_sub['installation_id'].values]['accuracy_group'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub.to_csv('sample_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
