{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## General librairies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "import dateutil.parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs = 'specs.csv'\n",
    "test = 'test.csv'\n",
    "train = 'train.csv'\n",
    "train_labels = 'train_labels.csv'\n",
    "subs = 'sample_submission_exemple.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(file,name, folder = \"\"):\n",
    "    if folder != \"\":\n",
    "        outfile = open('./'+folder+'/'+name+'.pickle', 'wb')\n",
    "    else:\n",
    "        outfile = open(name+'.pickle', 'wb')\n",
    "    pickle.dump(file, outfile)\n",
    "    outfile.close\n",
    "    \n",
    "def load(name, folder = \"\"):\n",
    "    if folder != \"\":\n",
    "        outfile = open('./'+folder+'/'+name+'.pickle', 'rb')\n",
    "    else:\n",
    "        outfile = open(name+'.pickle', 'rb')\n",
    "    file = pickle.load(outfile)\n",
    "    outfile.close\n",
    "    return file\n",
    "\n",
    "def relative_time(x):\n",
    "    x1 = []\n",
    "    for elt in x:\n",
    "        x1.append((elt-x[0]).item()/1000000000)\n",
    "    return x1\n",
    "\n",
    "def categorise(x):\n",
    "    dico = {}\n",
    "    count = 0\n",
    "    for elt in x:\n",
    "        if not(elt in dico):\n",
    "            dico[elt] = count\n",
    "            count += 1\n",
    "    return dico\n",
    "\n",
    "def padding( dataset, n):\n",
    "    d = list(np.zeros(len(dataset)))\n",
    "    c = 0\n",
    "    count = 0\n",
    "    for elt in dataset:\n",
    "        elt = elt.values\n",
    "        if count % 100 == 0:\n",
    "            print(count)\n",
    "        u = elt.shape[0]\n",
    "        \n",
    "        if u<200:\n",
    "            c += 1\n",
    "        \n",
    "        if u > n:\n",
    "            d[count] = elt[-n:]\n",
    "        else:\n",
    "            a = np.zeros(((n-u), elt.shape[1])) -1\n",
    "            elt = np.concatenate([a, elt], axis = 0)\n",
    "            d[count] = elt.tolist()\n",
    "        count += 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(train)\n",
    "\n",
    "dtitle = categorise(df['title'])\n",
    "dtype = categorise(df['type'])\n",
    "dworld = categorise(df['world'])\n",
    "devent = categorise(df['event_code'])\n",
    "devent_id = categorise(df['event_id'])\n",
    "\n",
    "vect = df['event_code']\n",
    "vect[df['event_id'] == '17113b36'] = 4100\n",
    "df['event_code'] = vect\n",
    "\n",
    "# df['date'] = df['timestamp'].apply(dateutil.parser.parse)\n",
    "\n",
    "\n",
    "save((dtitle, dtype, dworld, devent, devent_id), 'dicos cats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assessments = df[df['type']=='Assessment']['title'].unique()\n",
    "games = df[df['type']=='Games']['title'].unique()\n",
    "\n",
    "def build_line(df):\n",
    "    vect = []\n",
    "    \n",
    "    ## Categorical\n",
    "    vect.append(df.iloc[0]['installation_id'])\n",
    "    vect.append(df.iloc[0]['game_session'])\n",
    "    vect.append(df.iloc[0]['title'])\n",
    "    vect.append(df.iloc[0]['type'])\n",
    "    vect.append(df.iloc[0]['world'])\n",
    "    vect.append(df.iloc[0]['timestamp'])\n",
    "    \n",
    "    ## Assessment_accuracy_group\n",
    "    \n",
    "    if df.iloc[0]['title'] in assessments:\n",
    "        df1 = df[df['event_code'] == 4100]\n",
    "        if df1.shape[0] == 0:\n",
    "            vect.append(0)\n",
    "            vect.append(0)\n",
    "        else:\n",
    "            n_corr = 0\n",
    "            n_inc = 0\n",
    "            for i in df1['event_data']:\n",
    "                a = json.loads(i)['correct']\n",
    "                if a:\n",
    "                    n_corr += 1\n",
    "                else:\n",
    "                    n_inc += 1\n",
    "            acc = n_corr/(n_corr+n_inc)    \n",
    "            if acc == 1:\n",
    "                acc_class = 3\n",
    "            elif acc == 0.5:\n",
    "                acc_class = 2\n",
    "            elif acc <=0:\n",
    "                acc_class = 0\n",
    "            else:\n",
    "                acc_class = 1\n",
    "            vect.append(acc_class)\n",
    "            vect.append(acc)\n",
    "    else:\n",
    "        vect.append(-1)\n",
    "        vect.append(-1)\n",
    "    \n",
    "    ## feedbacks_accuracy\n",
    "    if df.iloc[0]['title'] in assessments or df.iloc[0]['title'] in games:\n",
    "        vc = df['event_code'].value_counts()\n",
    "        if 3021 in vc.index:\n",
    "            n_corr = vc[3021]\n",
    "        else:\n",
    "            n_corr = 0\n",
    "        \n",
    "        if 3020 in vc.index:\n",
    "            n_inc = vc[3020]\n",
    "        else:\n",
    "            n_inc = 0\n",
    "        \n",
    "        if n_corr + n_inc == 0:\n",
    "            vect.append(0)\n",
    "        else:\n",
    "            vect.append(n_corr/(n_corr+n_inc))\n",
    "    else:\n",
    "        vect.append(0)\n",
    "    ## Time\n",
    "    vect.append(df['game_time'].max()-df['game_time'].min())\n",
    "    \n",
    "    ## Event_code\n",
    "    vc = df['event_code'].value_counts()\n",
    "    \n",
    "    for elt in devent:\n",
    "        if elt in vc.index:\n",
    "            vect.append(vc[elt])\n",
    "        else:\n",
    "            vect.append(0)\n",
    "    \n",
    "    ## Event_ID\n",
    "    vc = df['event_id'].value_counts()\n",
    "    for elt in devent_id:\n",
    "        if elt in vc.index:\n",
    "            vect.append(vc[elt])\n",
    "        else:\n",
    "            vect.append(0) \n",
    "            \n",
    "    return vect\n",
    "\n",
    "columns = ['installation_id', 'game_session', 'title', 'type', 'world', 'timestamp', 'accuracy_group', 'accuracy', 'feed_acc','time']\n",
    "\n",
    "for elt in devent:\n",
    "    columns.append('n_'+str(elt))\n",
    "for elt in devent_id:\n",
    "    columns.append('n_'+str(elt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = list(np.zeros(df['game_session'].unique().shape[0]))\n",
    "\n",
    "count = 0\n",
    "for i, session in df.groupby(['game_session']):\n",
    "#     if count >= 100:\n",
    "#         break\n",
    "    line =build_line(session)\n",
    "#     dataset.append(line)\n",
    "    dataset[count] = line\n",
    "    if count % 1000 == 0:\n",
    "        print(count)\n",
    "    count += 1\n",
    "\n",
    "dataset = pd.DataFrame(dataset, columns = columns)\n",
    "dataset.to_csv('data by session events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(dataset, 'data by session events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load('data by session events')\n",
    "labels = pd.read_csv(train_labels)\n",
    "\n",
    "ids = df[df['type']=='Assessment']['installation_id'].unique()\n",
    "df = df[df.installation_id.isin(ids)]\n",
    "df = df[df.installation_id.isin(labels.installation_id.unique())]\n",
    "\n",
    "df['date'] = df['timestamp'].apply(dateutil.parser.parse)\n",
    "\n",
    "dtitle, dtype, dworld, devent, devent_id = load('dicos cats')\n",
    "\n",
    "df = df.replace({'title' : dtitle})\n",
    "df = df.replace({'type' : dtype})\n",
    "df = df.replace({'world' : dworld})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = list(np.zeros(labels.shape[0]))\n",
    "\n",
    "for i in range(labels.shape[0]):\n",
    "    \n",
    "    if i%100 ==0:\n",
    "        print(i)\n",
    "    installation_id = labels.iloc[i]['installation_id']\n",
    "    game_session = labels.iloc[i]['game_session']\n",
    "    game_title = dtitle[labels.iloc[i]['title']]\n",
    "    \n",
    "    df1 = df[df['installation_id'] == installation_id]\n",
    "    \n",
    "    time = df1[df1['game_session'] == game_session].iloc[0]['date']\n",
    "    \n",
    "    df1 = df1[df1['date'] < time]\n",
    "    \n",
    "    df1 = df1.sort_values(by = ['date'])\n",
    "    \n",
    "    def sub(x, time = time):\n",
    "        return (time - x).total_seconds()/3600\n",
    "    \n",
    "    df1['date'] = df1['date'].apply(sub)\n",
    "    \n",
    "    df1['pred_title'] = game_title\n",
    "    \n",
    "    drop = ['installation_id', 'game_session', 'pred_title', 'timestamp']\n",
    "    cols = ['pred_title'] + [elt for elt in columns if not(elt in drop)] + ['date']\n",
    "    \n",
    "    df1 = df1[cols]\n",
    "    \n",
    "    dataset[i] = df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(dataset, 'train set non padded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(X, y, N):\n",
    "    y1 = []\n",
    "    X1 = list(np.zeros(N))\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(N):\n",
    "        if i%1000 == 0:\n",
    "            print(i)\n",
    "        ind = random.randint(0, len(X)-1)\n",
    "        df = X[ind]\n",
    "        \n",
    "        df = df.reset_index(drop = True)\n",
    "        indices = []\n",
    "        for j in df.index:\n",
    "            a = random.uniform(0,1)\n",
    "            if a < 0.2:\n",
    "#                 print(i)\n",
    "                indices.append(j)\n",
    "        df = df.drop(indices)\n",
    "#         df = df.sample(frac = 1)\n",
    "        \n",
    "        X1[i] = df\n",
    "        y1.append(y[ind])\n",
    "    return X1, y1\n",
    "    \n",
    "def format_input(X, pad):\n",
    "    i = X.shape[0]\n",
    "    j = X.shape[1]\n",
    "    return [X[:, :, 0].reshape((i,j,1)), X[:,:, 1].reshape((i,j,1)),X[:,:, 2].reshape((i,j,1)),X[:,:, 3].reshape((i,j,1)),X[:,:, 4].reshape((i,j,1)),X[:,:, 5:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = ['pred_title',\n",
    " 'title',\n",
    " 'type',\n",
    " 'world',\n",
    " 'accuracy_group',\n",
    " 'accuracy',\n",
    " 'feed_acc',\n",
    " 'time',\n",
    " 'n_2000',\n",
    " 'n_3010',\n",
    " 'n_3110',\n",
    " 'n_4070',\n",
    " 'n_4090',\n",
    " 'n_4030',\n",
    " 'n_4035',\n",
    " 'n_4021',\n",
    " 'n_4020',\n",
    " 'n_4010',\n",
    " 'n_2080',\n",
    " 'n_2083',\n",
    " 'n_2040',\n",
    " 'n_2020',\n",
    " 'n_2030',\n",
    " 'n_3021',\n",
    " 'n_3121',\n",
    " 'n_2050',\n",
    " 'n_3020',\n",
    " 'n_3120',\n",
    " 'n_2060',\n",
    " 'n_2070',\n",
    " 'n_4031',\n",
    " 'n_4025',\n",
    " 'n_5000',\n",
    " 'n_5010',\n",
    " 'n_2081',\n",
    " 'n_2025',\n",
    " 'n_4022',\n",
    " 'n_2035',\n",
    " 'n_4040',\n",
    " 'n_4100',\n",
    " 'n_2010',\n",
    " 'n_4110',\n",
    " 'n_4045',\n",
    " 'n_4095',\n",
    " 'n_4220',\n",
    " 'n_2075',\n",
    " 'n_4230',\n",
    " 'n_4235',\n",
    " 'n_4080',\n",
    " 'n_4050',\n",
    " 'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load('train set non padded light')\n",
    "\n",
    "# dataset1 = list(np.zeros(len(dataset)))\n",
    "# for i in range(len(dataset)):\n",
    "#     df = dataset[i]\n",
    "#     df = df[keep]\n",
    "#     dataset1[i] = df\n",
    "\n",
    "# dataset = dataset1\n",
    "\n",
    "# save(dataset, 'train set non padded light')\n",
    "\n",
    "labels = pd.read_csv(train_labels)\n",
    "\n",
    "Y = labels['accuracy_group'].values.astype('float64')\n",
    "\n",
    "y = np_utils.to_categorical(Y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, Y, test_size=0.2, random_state=43)\n",
    "# del(dataset)\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(Y),\n",
    "                                               Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n"
     ]
    }
   ],
   "source": [
    "# X_train, y_train = augment(X_train, y_train, 100000)\n",
    "\n",
    "pad = 100\n",
    "X_train = padding(X_train, pad)\n",
    "X_test = padding(X_test, pad)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "X_train = format_input(X_train, pad)\n",
    "X_test = format_input(X_test, pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\gabri\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\gabri\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\gabri\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\gabri\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\gabri\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dropout, Dense,CuDNNLSTM, Flatten, Embedding, TimeDistributed, Concatenate, LSTM, BatchNormalization, Lambda, Reshape\n",
    "from keras.regularizers import l2\n",
    "\n",
    "pad_size = pad\n",
    "\n",
    "num = Input(shape = (pad_size, 46))\n",
    "pred_title = Input(shape = (pad_size, 1))\n",
    "title = Input(shape = (pad_size, 1))\n",
    "type1 = Input(shape = (pad_size, 1))\n",
    "world = Input(shape = (pad_size, 1))\n",
    "acc_grp = Input(shape = (pad_size, 1))\n",
    "\n",
    "inputs = [pred_title, title, type1, world,acc_grp, num]\n",
    "\n",
    "num_emb = TimeDistributed(Dense(16))(num)\n",
    "\n",
    "pred_title_emb = TimeDistributed(Embedding( 5 ,5))(pred_title)\n",
    "pred_title_emb = Lambda(lambda x: x, output_shape = lambda s:s)(pred_title_emb)\n",
    "pred_title_emb = Reshape((-1,5))(pred_title_emb)\n",
    "\n",
    "title_emb = TimeDistributed(Embedding( 44 ,10))(title)\n",
    "title_emb = Lambda(lambda x: x, output_shape = lambda s:s)(title_emb)\n",
    "title_emb = Reshape((-1,10))(title_emb)\n",
    "\n",
    "type_emb = TimeDistributed(Embedding( 4 ,4))(type1)\n",
    "type_emb = Lambda(lambda x: x, output_shape = lambda s:s)(type_emb)\n",
    "type_emb = Reshape((-1,4))(type_emb)\n",
    "\n",
    "world_emb = TimeDistributed(Embedding( 4 ,2))(world)\n",
    "world_emb = Lambda(lambda x: x, output_shape = lambda s:s)(world_emb)\n",
    "world_emb = Reshape((-1,2))(world_emb)\n",
    "\n",
    "acc_grp_emb = TimeDistributed(Embedding( 5 ,3))(acc_grp)\n",
    "acc_grp_emb = Lambda(lambda x: x, output_shape = lambda s:s)(acc_grp_emb)\n",
    "acc_grp_emb = Reshape((-1,3))(acc_grp_emb)\n",
    "\n",
    "embedded = [pred_title_emb, title_emb, type_emb, world_emb, acc_grp_emb]\n",
    "\n",
    "merged = Concatenate()(embedded + [num_emb])\n",
    "\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "merged = TimeDistributed(Dense(16))(merged)\n",
    "merged = TimeDistributed(Dense(5))(merged)\n",
    "\n",
    "lstm_out = CuDNNLSTM(64, return_sequences = True)(merged)\n",
    "lstm_out = CuDNNLSTM(64, return_sequences = True)(lstm_out)\n",
    "lstm_out = CuDNNLSTM(64, return_sequences = False)(lstm_out)\n",
    "\n",
    "# lstm_out = Flatten()(lstm_out)\n",
    "\n",
    "td = Dropout(0.2)(lstm_out)\n",
    "td = Dense(32, activation = 'relu')(td)\n",
    "td = Dropout(0.2)(td)\n",
    "\n",
    "td = Dense(1, activation = 'linear')(td)\n",
    "\n",
    "model = Model(inputs , td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 100, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 100, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 100, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 100, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 100, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 100, 1, 5)    25          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 100, 1, 10)   440         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 100, 1, 4)    16          input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, 100, 1, 2)    8           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 100, 1, 3)    15          input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 100, 1, 5)    0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 100, 1, 10)   0           time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 100, 1, 4)    0           time_distributed_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 100, 1, 2)    0           time_distributed_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 100, 1, 3)    0           time_distributed_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 100, 46)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 100, 5)       0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 100, 10)      0           lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 100, 4)       0           lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 100, 2)       0           lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 100, 3)       0           lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 100, 16)      752         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 100, 40)      0           reshape_1[0][0]                  \n",
      "                                                                 reshape_2[0][0]                  \n",
      "                                                                 reshape_3[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "                                                                 reshape_5[0][0]                  \n",
      "                                                                 time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 100, 40)      160         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, 100, 16)      656         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistrib (None, 100, 5)       85          time_distributed_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)        (None, 100, 64)      18176       time_distributed_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)        (None, 100, 64)      33280       cu_dnnlstm_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_3 (CuDNNLSTM)        (None, 64)           33280       cu_dnnlstm_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           cu_dnnlstm_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 32)           2080        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            33          dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 89,006\n",
      "Trainable params: 88,926\n",
      "Non-trainable params: 80\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\gabri\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\gabri\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 14152 samples, validate on 3538 samples\n",
      "Epoch 1/50\n",
      "14152/14152 [==============================] - 9s 650us/step - loss: 1.7461 - mean_squared_error: 1.7461 - val_loss: 1.5394 - val_mean_squared_error: 1.5394\n",
      "Epoch 2/50\n",
      "14152/14152 [==============================] - 5s 378us/step - loss: 1.5622 - mean_squared_error: 1.5622 - val_loss: 1.4877 - val_mean_squared_error: 1.4877\n",
      "Epoch 3/50\n",
      "14152/14152 [==============================] - 5s 378us/step - loss: 1.4924 - mean_squared_error: 1.4924 - val_loss: 1.4044 - val_mean_squared_error: 1.4044\n",
      "Epoch 4/50\n",
      "14152/14152 [==============================] - 5s 383us/step - loss: 1.4474 - mean_squared_error: 1.4474 - val_loss: 1.3604 - val_mean_squared_error: 1.3604\n",
      "Epoch 5/50\n",
      "14152/14152 [==============================] - 5s 378us/step - loss: 1.4030 - mean_squared_error: 1.4030 - val_loss: 1.3927 - val_mean_squared_error: 1.3927\n",
      "Epoch 6/50\n",
      "14152/14152 [==============================] - 5s 380us/step - loss: 1.3738 - mean_squared_error: 1.3738 - val_loss: 1.4713 - val_mean_squared_error: 1.4713\n",
      "Epoch 7/50\n",
      "14152/14152 [==============================] - 5s 378us/step - loss: 1.3500 - mean_squared_error: 1.3500 - val_loss: 1.4009 - val_mean_squared_error: 1.4009\n",
      "Epoch 8/50\n",
      "14152/14152 [==============================] - 6s 389us/step - loss: 1.3275 - mean_squared_error: 1.3275 - val_loss: 1.2717 - val_mean_squared_error: 1.2717\n",
      "Epoch 9/50\n",
      "14152/14152 [==============================] - 5s 380us/step - loss: 1.3130 - mean_squared_error: 1.3130 - val_loss: 1.2369 - val_mean_squared_error: 1.2369\n",
      "Epoch 10/50\n",
      "14152/14152 [==============================] - 5s 387us/step - loss: 1.2973 - mean_squared_error: 1.2973 - val_loss: 1.2959 - val_mean_squared_error: 1.2959\n",
      "Epoch 11/50\n",
      "14152/14152 [==============================] - 5s 386us/step - loss: 1.2798 - mean_squared_error: 1.2798 - val_loss: 1.3607 - val_mean_squared_error: 1.3607\n",
      "Epoch 12/50\n",
      "14152/14152 [==============================] - 5s 389us/step - loss: 1.2720 - mean_squared_error: 1.2720 - val_loss: 1.2222 - val_mean_squared_error: 1.2222\n",
      "Epoch 13/50\n",
      "14152/14152 [==============================] - 6s 393us/step - loss: 1.2631 - mean_squared_error: 1.2631 - val_loss: 1.1950 - val_mean_squared_error: 1.1950\n",
      "Epoch 14/50\n",
      "14152/14152 [==============================] - 5s 381us/step - loss: 1.2477 - mean_squared_error: 1.2477 - val_loss: 1.1846 - val_mean_squared_error: 1.1846\n",
      "Epoch 15/50\n",
      "14152/14152 [==============================] - 5s 380us/step - loss: 1.2402 - mean_squared_error: 1.2402 - val_loss: 2.1938 - val_mean_squared_error: 2.1938\n",
      "Epoch 16/50\n",
      "14152/14152 [==============================] - 5s 384us/step - loss: 1.2365 - mean_squared_error: 1.2365 - val_loss: 1.2009 - val_mean_squared_error: 1.2009\n",
      "Epoch 17/50\n",
      "14152/14152 [==============================] - 5s 381us/step - loss: 1.2274 - mean_squared_error: 1.2274 - val_loss: 1.2029 - val_mean_squared_error: 1.2029\n",
      "Epoch 18/50\n",
      "14152/14152 [==============================] - 5s 388us/step - loss: 1.2174 - mean_squared_error: 1.2174 - val_loss: 1.2566 - val_mean_squared_error: 1.2566\n",
      "Epoch 19/50\n",
      "14152/14152 [==============================] - 5s 382us/step - loss: 1.2175 - mean_squared_error: 1.2175 - val_loss: 1.1799 - val_mean_squared_error: 1.1799\n",
      "Epoch 20/50\n",
      "14152/14152 [==============================] - 5s 382us/step - loss: 1.2067 - mean_squared_error: 1.2067 - val_loss: 1.2373 - val_mean_squared_error: 1.2373\n",
      "Epoch 21/50\n",
      "14152/14152 [==============================] - 6s 393us/step - loss: 1.2063 - mean_squared_error: 1.2063 - val_loss: 1.2406 - val_mean_squared_error: 1.2406\n",
      "Epoch 22/50\n",
      "14152/14152 [==============================] - 5s 386us/step - loss: 1.2058 - mean_squared_error: 1.2058 - val_loss: 1.1924 - val_mean_squared_error: 1.1924\n",
      "Epoch 23/50\n",
      "14152/14152 [==============================] - 6s 398us/step - loss: 1.1952 - mean_squared_error: 1.1952 - val_loss: 1.1451 - val_mean_squared_error: 1.1451\n",
      "Epoch 24/50\n",
      "14152/14152 [==============================] - 6s 409us/step - loss: 1.1898 - mean_squared_error: 1.1898 - val_loss: 1.3451 - val_mean_squared_error: 1.3451\n",
      "Epoch 25/50\n",
      "14152/14152 [==============================] - 6s 408us/step - loss: 1.1840 - mean_squared_error: 1.1840 - val_loss: 1.1415 - val_mean_squared_error: 1.1415\n",
      "Epoch 26/50\n",
      "14152/14152 [==============================] - 6s 402us/step - loss: 1.1851 - mean_squared_error: 1.1851 - val_loss: 1.2877 - val_mean_squared_error: 1.2877\n",
      "Epoch 27/50\n",
      "14152/14152 [==============================] - 6s 401us/step - loss: 1.1804 - mean_squared_error: 1.1804 - val_loss: 1.1603 - val_mean_squared_error: 1.1603\n",
      "Epoch 28/50\n",
      "14152/14152 [==============================] - 6s 401us/step - loss: 1.1795 - mean_squared_error: 1.1795 - val_loss: 1.1592 - val_mean_squared_error: 1.1592\n",
      "Epoch 29/50\n",
      "14152/14152 [==============================] - 6s 390us/step - loss: 1.1741 - mean_squared_error: 1.1741 - val_loss: 1.1607 - val_mean_squared_error: 1.1607\n",
      "Epoch 30/50\n",
      "14152/14152 [==============================] - 6s 398us/step - loss: 1.1732 - mean_squared_error: 1.1732 - val_loss: 1.1605 - val_mean_squared_error: 1.1605\n",
      "Epoch 31/50\n",
      "14152/14152 [==============================] - 5s 385us/step - loss: 1.1730 - mean_squared_error: 1.1730 - val_loss: 1.1384 - val_mean_squared_error: 1.1384\n",
      "Epoch 32/50\n",
      "14152/14152 [==============================] - 5s 383us/step - loss: 1.1638 - mean_squared_error: 1.1638 - val_loss: 1.1375 - val_mean_squared_error: 1.1375\n",
      "Epoch 33/50\n",
      "  832/14152 [>.............................] - ETA: 4s - loss: 1.1368 - mean_squared_error: 1.1368"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import keras.backend as K\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "optimizer = SGD(0.01)\n",
    "model.compile(loss='mse', #'categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['mse'])\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, validation_data=(X_test, y_test))#, class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test).reshape(len(X_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "def tres(pred, t):\n",
    "    pred1 = deepcopy(pred)\n",
    "    pred1[pred1 < t[0]] = 0\n",
    "    pred1[(pred1>= t[0])&(pred1 < t[1])] = 1\n",
    "    pred1[(pred1>= t[1])&(pred1 < t[2])] = 2\n",
    "    pred1[pred1 >= t[2]] = 3\n",
    "    \n",
    "    return pred1\n",
    "\n",
    "p = model.predict(X_train).reshape(len(X_train[0]))\n",
    "\n",
    "def objective(trial, pred =p,  y_test = y_train):\n",
    "    x0 = trial.suggest_uniform('x0', pred.min(), pred.max())\n",
    "    x1 = trial.suggest_uniform('x1', x0,  pred.max())\n",
    "    x2 = trial.suggest_uniform('x2', x1,  pred.max())\n",
    "\n",
    "    t = [x0, x1, x2]\n",
    "    pred1 = tres(pred, t)\n",
    "    a = cohen_kappa_score(y_test, pred1, weights = 'quadratic')\n",
    "    print(a)\n",
    "    return 1-a\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = list(study.best_params.values())\n",
    "\n",
    "pred = model.predict(X_test).reshape(len(X_test[0]))\n",
    "pred1 = tres(pred, t)\n",
    "\n",
    "print(cohen_kappa_score(y_test, pred1, weights = 'quadratic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
